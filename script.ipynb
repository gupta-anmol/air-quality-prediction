{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "RKP = \"DL031\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Bidirectional\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "\n",
    "sns.set_theme(style=\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\confusement\\miniconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3338: DtypeWarning: Columns (15) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['StationId', 'Datetime', 'PM2.5', 'PM10', 'NO', 'NO2', 'NOx', 'NH3',\n",
      "       'CO', 'SO2', 'O3', 'Benzene', 'Toluene', 'Xylene', 'AQI', 'AQI_Bucket'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Load datasets and rename columns, load all aqi data but specify metro data name\n",
    "def loadcsv(city=\"./data/rkpuram.csv\"):\n",
    "    met = pd.read_csv(city,delimiter=';',skiprows=24)\n",
    "    aqi = pd.read_csv('./data/station_hour.csv')\n",
    "    print(aqi.columns)\n",
    "    met.rename(columns={'# Date': 'Date',}, inplace=True)\n",
    "    met.rename(columns={'UT time': 'Time',}, inplace=True)\n",
    "    aqi['Time'] = aqi['Datetime'].str[-8:-3]\n",
    "    aqi['Date'] = aqi['Datetime'].str[0:10]\n",
    "    stations = [\"DL\"+str(x).zfill(3) for x in range(1,39)]\n",
    "    split_aqi = {}\n",
    "    for i in range(len(stations)):\n",
    "        split_aqi[stations[i]] = (aqi[aqi['StationId'] == stations[i]])\n",
    "    return met,aqi,split_aqi\n",
    "met,aqi,split_aqi = loadcsv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre - processing and loading data\n",
    "class dataset:\n",
    "    def __init__(self,met,aqi,split_aqi):\n",
    "            self.metro_data = met\n",
    "            self.aqi_data = aqi\n",
    "            self.split_aqi = split_aqi\n",
    "    def mergedData(self,station,rlist=['PM2.5','PM10','NO','NO2','NOx','NH3','CO','SO2','O3','AQI'],roll=48,shift=72):\n",
    "        df_aqi = self.getdf(station)\n",
    "        df = pd.merge(df_aqi, self.metro_data, how='inner', on=['Date', 'Time'])\n",
    "        print(\"Merged Dataset Size\",len(df))\n",
    "        \n",
    "        #Pre Processing merged Data\n",
    "        df['Year'] = df['Date'].str[0:4]\n",
    "        df['Month'] = df['Date'].str[5:7].astype(np.float64)\n",
    "        df['Day'] = df['Date'].str[8:10].astype(np.float64)\n",
    "        df['Hour'] = df['Time'].str[0:2]\n",
    "        \n",
    "        # TRIG TRANSFORMATIONS\n",
    "        df['windX'] = np.cos(np.deg2rad(df['Wind direction'])) * df['Wind speed']\n",
    "        df['windY'] = np.sin(np.deg2rad(df['Wind direction'])) * df['Wind speed']\n",
    "        df['hourX'] = np.cos((df['Hour'].astype(np.float64)-1)*np.pi/24)\n",
    "        df['hourY'] = np.sin((df['Hour'].astype(np.float64)-1)*np.pi/24)\n",
    "        df['MonthX'] = np.cos((df['Month'].astype(np.float64)-1)*np.pi/12)\n",
    "        df['MonthY'] = np.sin((df['Month'].astype(np.float64)-1)*np.pi/12)\n",
    "        \n",
    "        import datetime\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "        df['isWeekend'] =  (df['Date'].dt.dayofweek>=5).astype(int)\n",
    "        \n",
    "        df.interpolate(method='linear', limit=5,inplace=True)\n",
    "        \n",
    "        # Drop Additional columns\n",
    "        df.drop('Benzene', axis=1, inplace=True)\n",
    "        df.drop('Toluene',axis=1, inplace=True)\n",
    "        df.drop('Xylene', axis=1,inplace=True)\n",
    "        df.drop('AQI_Bucket',axis=1,inplace=True)\n",
    "        df.drop('Datetime',axis=1,inplace=True)\n",
    "        df.drop('StationId',axis=1,inplace=True)\n",
    "        df.drop('Short-wave irradiation',axis=1,inplace=True)\n",
    "        df.drop('Date',axis=1,inplace=True)\n",
    "        df.drop('Time',axis=1,inplace=True)\n",
    "        \n",
    "        # Rolling and shifting \n",
    "        print(\"Size before roll\",len(df))\n",
    "        for i in rlist:\n",
    "            df[i+'_lag1'] = df[i].shift(24)\n",
    "            df[i+'_lag2'] = df[i].shift(48)\n",
    "            df[i+'_lag3'] = df[i].shift(72)\n",
    "        for i in rlist:\n",
    "            df[i+\"_pred1\"] = df[i].shift(-24)\n",
    "            df[i+\"_pred2\"] = df[i].shift(-48)\n",
    "            df[i+\"_pred3\"] = df[i].shift(-72)\n",
    "        newlist = rlist + ['Temperature','Relative Humidity','windX','windY']\n",
    "        for i in newlist:\n",
    "            for j in range(24):\n",
    "                df[i+\"_t-\"+str(j)] = df[i].shift(j)\n",
    "                df[i+\"_t+\"+str(j)] = df[i].shift(-j-shift)\n",
    "        futurelist = ['Year','MonthX','MonthY','hourX','hourY','isWeekend']\n",
    "        for i in futurelist:\n",
    "            for j in range(24):\n",
    "                df[i+\"_t-\"+str(j)] = df[i].shift(-(shift+23-j))\n",
    "        df.dropna(inplace=True)\n",
    "        print(\"Size after roll\",len(df))\n",
    "        \n",
    "        return df.copy()\n",
    "    def getdf(self,station):\n",
    "        return self.split_aqi[station]\n",
    "    def plot(self,station):\n",
    "        df = self.getdf(station)\n",
    "    def stats(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Model Testing as well\n",
    "def getSplitFeatures(df,TIME_SERIES_LENGTH = 24):\n",
    "    features = []\n",
    "    rlist=['PM2.5','PM10','NO','NO2','CO','AQI']\n",
    "    for it in rlist:\n",
    "        print(it,np.mean(df[it]),np.std(df[it]))\n",
    "    newlist = rlist + ['Temperature','Relative Humidity','windX','windY','Year','MonthX','MonthY','hourX','hourY','isWeekend']\n",
    "    for j in range(24):\n",
    "        for i in newlist:\n",
    "            features.append(i+'_t-'+str(j))\n",
    "    predVector = []\n",
    "    for j in range(24):\n",
    "        predVector.append('PM2.5_t+'+str(j))\n",
    "    X = df[features]\n",
    "    y = df[predVector]\n",
    "    X = np.array(X).reshape(X.shape[0],TIME_SERIES_LENGTH,len(newlist))\n",
    "    scaler = StandardScaler()\n",
    "    Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "    Xtrain = scaler.fit_transform(Xtrain.reshape(Xtrain.shape[0],TIME_SERIES_LENGTH*len(newlist)))\n",
    "    Xtrain = Xtrain.reshape(Xtrain.shape[0],TIME_SERIES_LENGTH,len(newlist))\n",
    "    Xtest = scaler.transform(Xtest.reshape(Xtest.shape[0],TIME_SERIES_LENGTH*len(newlist)))\n",
    "    Xtest = Xtest.reshape(Xtest.shape[0],TIME_SERIES_LENGTH,len(newlist))\n",
    "    return Xtrain,ytrain,Xtest,ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def trainModel1(Xtrain,ytrain,Xtest,ytest,TIME_SERIES_LENGTH=24):\n",
    "    model = Sequential()\n",
    "    # model.add(LSTM(50, activation='relu', return_sequences=True, input_shape=(TIME_SERIES_LENGTH,len(newlist))))\n",
    "    model.add(Conv1D(128, 3,activation='relu',input_shape=(TIME_SERIES_LENGTH,len(newlist))))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv1D(128, 6,activation='relu',input_shape=(TIME_SERIES_LENGTH,len(newlist))))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv1D(128, 6,activation='relu',input_shape=(TIME_SERIES_LENGTH,len(newlist))))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # model.add(LSTM(200,activation='relu',input_shape=(TIME_SERIES_LENGTH,len(newlist))))\n",
    "    # model.add(Bidirectional(LSTM(50, activation='relu'), input_shape=(TIME_SERIES_LENGTH,len(newlist))))\n",
    "    # model.add(Dense(200, activation='relu'))\n",
    "    \n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.summary()\n",
    "    #Fit\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['mse','mae'])\n",
    "    history = model.fit(Xtrain, ytrain, epochs=200, batch_size=256,  verbose=1, validation_split=0.2)\n",
    "    return model,history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictStats(model,Xtrain,ytrain,Xtest,ytest):\n",
    "    testPred = model.predict(Xtest)\n",
    "    trainPred = model.predict(Xtrain)\n",
    "    print(mean_squared_error(testPred, ytest,squared=False))\n",
    "    print(mean_squared_error(trainPred, ytrain,squared=False))\n",
    "    print(mean_absolute_error(testPred, ytest))\n",
    "    print(mean_absolute_error(trainPred, ytrain))\n",
    "def plothistory(history):\n",
    "    # list all data in history\n",
    "    print(history.history.keys())\n",
    "    # summarize history for loss\n",
    "    plt.plot(np.sqrt(history.history['loss']))\n",
    "    plt.plot(np.sqrt(history.history['val_loss']))\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('RMSE loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged Dataset Size 44035\n",
      "Size before roll 44035\n",
      "Size after roll 11596\n",
      "PM2.5 102.14626081956295 82.29759489131531\n",
      "PM10 216.3002180308481 141.8134423731434\n",
      "NO 43.17497931401046 81.89773244893385\n",
      "NO2 58.702180613527936 41.01196908713821\n",
      "CO 1.5506782982223413 2.153596798375222\n",
      "AQI 224.7247650051742 111.16981620228673\n",
      "Model: \"sequential_72\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_117 (Conv1D)          (None, 22, 128)           6272      \n",
      "_________________________________________________________________\n",
      "batch_normalization_99 (Batc (None, 22, 128)           512       \n",
      "_________________________________________________________________\n",
      "conv1d_118 (Conv1D)          (None, 17, 128)           98432     \n",
      "_________________________________________________________________\n",
      "batch_normalization_100 (Bat (None, 17, 128)           512       \n",
      "_________________________________________________________________\n",
      "conv1d_119 (Conv1D)          (None, 12, 128)           98432     \n",
      "_________________________________________________________________\n",
      "batch_normalization_101 (Bat (None, 12, 128)           512       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 6, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_43 (Flatten)         (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_96 (Dense)             (None, 24)                18456     \n",
      "=================================================================\n",
      "Total params: 223,128\n",
      "Trainable params: 222,360\n",
      "Non-trainable params: 768\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "25/25 [==============================] - 4s 149ms/step - loss: 15361.6426 - mse: 15361.6426 - mae: 96.8259 - val_loss: 15502.7080 - val_mse: 15502.7080 - val_mae: 98.1748\n",
      "Epoch 2/200\n",
      "25/25 [==============================] - 4s 140ms/step - loss: 12190.2588 - mse: 12190.2588 - mae: 85.4989 - val_loss: 14943.1406 - val_mse: 14943.1406 - val_mae: 96.5682\n",
      "Epoch 3/200\n",
      "25/25 [==============================] - 3s 130ms/step - loss: 8883.1689 - mse: 8883.1689 - mae: 71.0332 - val_loss: 12049.0811 - val_mse: 12049.0811 - val_mae: 85.1381\n",
      "Epoch 4/200\n",
      "25/25 [==============================] - 3s 135ms/step - loss: 6329.6250 - mse: 6329.6250 - mae: 58.2075 - val_loss: 10651.5879 - val_mse: 10651.5879 - val_mae: 77.9098\n",
      "Epoch 5/200\n",
      "25/25 [==============================] - 4s 152ms/step - loss: 4651.6270 - mse: 4651.6270 - mae: 49.8244 - val_loss: 8302.7012 - val_mse: 8302.7012 - val_mae: 66.2719\n",
      "Epoch 6/200\n",
      "25/25 [==============================] - 4s 142ms/step - loss: 3748.7437 - mse: 3748.7437 - mae: 44.9603 - val_loss: 6567.6499 - val_mse: 6567.6499 - val_mae: 55.9146\n",
      "Epoch 7/200\n",
      "25/25 [==============================] - 3s 137ms/step - loss: 3313.2288 - mse: 3313.2288 - mae: 42.2500 - val_loss: 6524.3770 - val_mse: 6524.3770 - val_mae: 55.9880\n",
      "Epoch 8/200\n",
      "25/25 [==============================] - 4s 157ms/step - loss: 3148.8975 - mse: 3148.8975 - mae: 41.2386 - val_loss: 6273.6177 - val_mse: 6273.6177 - val_mae: 53.3005\n",
      "Epoch 9/200\n",
      "25/25 [==============================] - 5s 193ms/step - loss: 3021.4275 - mse: 3021.4275 - mae: 39.8236 - val_loss: 5022.8477 - val_mse: 5022.8477 - val_mae: 47.5815\n",
      "Epoch 10/200\n",
      "25/25 [==============================] - 4s 168ms/step - loss: 2902.5474 - mse: 2902.5474 - mae: 39.0294 - val_loss: 4721.2295 - val_mse: 4721.2295 - val_mae: 46.0157\n",
      "Epoch 11/200\n",
      "25/25 [==============================] - 4s 170ms/step - loss: 2782.8889 - mse: 2782.8889 - mae: 37.9415 - val_loss: 4208.9199 - val_mse: 4208.9199 - val_mae: 43.7964\n",
      "Epoch 12/200\n",
      "25/25 [==============================] - 4s 162ms/step - loss: 2668.1465 - mse: 2668.1465 - mae: 36.7947 - val_loss: 3788.6829 - val_mse: 3788.6829 - val_mae: 41.4479\n",
      "Epoch 13/200\n",
      "25/25 [==============================] - 3s 121ms/step - loss: 2566.2720 - mse: 2566.2720 - mae: 35.9570 - val_loss: 3904.5505 - val_mse: 3904.5505 - val_mae: 41.8123\n",
      "Epoch 14/200\n",
      "25/25 [==============================] - 4s 172ms/step - loss: 2493.9626 - mse: 2493.9626 - mae: 35.2946 - val_loss: 3319.7896 - val_mse: 3319.7896 - val_mae: 39.1052\n",
      "Epoch 15/200\n",
      "25/25 [==============================] - 4s 154ms/step - loss: 2411.9724 - mse: 2411.9724 - mae: 34.5066 - val_loss: 3225.0728 - val_mse: 3225.0728 - val_mae: 38.3581\n",
      "Epoch 16/200\n",
      "25/25 [==============================] - 4s 167ms/step - loss: 2305.5491 - mse: 2305.5491 - mae: 33.6893 - val_loss: 2936.1570 - val_mse: 2936.1570 - val_mae: 36.5134\n",
      "Epoch 17/200\n",
      "25/25 [==============================] - 4s 140ms/step - loss: 2182.4409 - mse: 2182.4409 - mae: 32.4826 - val_loss: 2528.4224 - val_mse: 2528.4224 - val_mae: 33.6980\n",
      "Epoch 18/200\n",
      "25/25 [==============================] - 4s 159ms/step - loss: 2083.8372 - mse: 2083.8372 - mae: 31.5765 - val_loss: 2189.0322 - val_mse: 2189.0322 - val_mae: 31.5594\n",
      "Epoch 19/200\n",
      "25/25 [==============================] - 4s 153ms/step - loss: 1947.9802 - mse: 1947.9802 - mae: 30.3218 - val_loss: 2387.5374 - val_mse: 2387.5374 - val_mae: 33.0780\n",
      "Epoch 20/200\n",
      "25/25 [==============================] - 4s 155ms/step - loss: 1876.2482 - mse: 1876.2482 - mae: 29.5630 - val_loss: 1975.9874 - val_mse: 1975.9874 - val_mae: 30.3911\n",
      "Epoch 21/200\n",
      "25/25 [==============================] - 3s 140ms/step - loss: 1791.9602 - mse: 1791.9602 - mae: 29.0176 - val_loss: 1899.4924 - val_mse: 1899.4924 - val_mae: 29.6389\n",
      "Epoch 22/200\n",
      "25/25 [==============================] - 4s 148ms/step - loss: 1740.1058 - mse: 1740.1058 - mae: 28.7871 - val_loss: 2030.3385 - val_mse: 2030.3385 - val_mae: 30.9719\n",
      "Epoch 23/200\n",
      "25/25 [==============================] - 3s 135ms/step - loss: 1648.5918 - mse: 1648.5918 - mae: 27.9602 - val_loss: 1914.6599 - val_mse: 1914.6599 - val_mae: 29.6832\n",
      "Epoch 24/200\n",
      "25/25 [==============================] - 3s 137ms/step - loss: 1581.2096 - mse: 1581.2096 - mae: 27.4024 - val_loss: 1758.3733 - val_mse: 1758.3733 - val_mae: 28.5688\n",
      "Epoch 25/200\n",
      "25/25 [==============================] - 3s 122ms/step - loss: 1519.7515 - mse: 1519.7515 - mae: 26.8906 - val_loss: 1739.7961 - val_mse: 1739.7961 - val_mae: 28.3458\n",
      "Epoch 26/200\n",
      "25/25 [==============================] - 3s 119ms/step - loss: 1430.3217 - mse: 1430.3217 - mae: 26.0260 - val_loss: 1646.3523 - val_mse: 1646.3523 - val_mae: 27.3731\n",
      "Epoch 27/200\n",
      "25/25 [==============================] - 3s 135ms/step - loss: 1396.1486 - mse: 1396.1486 - mae: 25.8242 - val_loss: 1572.8640 - val_mse: 1572.8640 - val_mae: 27.0456\n",
      "Epoch 28/200\n",
      "25/25 [==============================] - 3s 140ms/step - loss: 1347.9706 - mse: 1347.9706 - mae: 25.5515 - val_loss: 1500.7881 - val_mse: 1500.7881 - val_mae: 26.2757\n",
      "Epoch 29/200\n",
      "25/25 [==============================] - 3s 138ms/step - loss: 1246.6172 - mse: 1246.6172 - mae: 24.4192 - val_loss: 1548.1113 - val_mse: 1548.1113 - val_mae: 26.5894\n",
      "Epoch 30/200\n",
      "25/25 [==============================] - 4s 178ms/step - loss: 1204.7195 - mse: 1204.7195 - mae: 24.0537 - val_loss: 1328.5204 - val_mse: 1328.5204 - val_mae: 25.3556\n",
      "Epoch 31/200\n",
      "25/25 [==============================] - 5s 188ms/step - loss: 1156.4109 - mse: 1156.4109 - mae: 23.5302 - val_loss: 1323.3063 - val_mse: 1323.3063 - val_mae: 25.0225\n",
      "Epoch 32/200\n",
      "25/25 [==============================] - 4s 159ms/step - loss: 1145.8973 - mse: 1145.8973 - mae: 23.3365 - val_loss: 1267.6049 - val_mse: 1267.6049 - val_mae: 24.5460\n",
      "Epoch 33/200\n",
      "25/25 [==============================] - 4s 173ms/step - loss: 1111.3936 - mse: 1111.3936 - mae: 23.1614 - val_loss: 1156.6400 - val_mse: 1156.6400 - val_mae: 23.5447\n",
      "Epoch 34/200\n",
      "25/25 [==============================] - 4s 152ms/step - loss: 1050.9335 - mse: 1050.9335 - mae: 22.4966 - val_loss: 1158.4974 - val_mse: 1158.4974 - val_mae: 23.5668\n",
      "Epoch 35/200\n",
      "25/25 [==============================] - 4s 169ms/step - loss: 1033.0865 - mse: 1033.0865 - mae: 22.3350 - val_loss: 1147.1893 - val_mse: 1147.1893 - val_mae: 23.2015\n",
      "Epoch 36/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 5s 194ms/step - loss: 975.2178 - mse: 975.2178 - mae: 21.5572 - val_loss: 1098.7650 - val_mse: 1098.7650 - val_mae: 22.9867\n",
      "Epoch 37/200\n",
      "25/25 [==============================] - 4s 172ms/step - loss: 1000.9492 - mse: 1000.9492 - mae: 22.0924 - val_loss: 1055.7306 - val_mse: 1055.7306 - val_mae: 23.0541\n",
      "Epoch 38/200\n",
      "25/25 [==============================] - 4s 174ms/step - loss: 927.8609 - mse: 927.8609 - mae: 21.1456 - val_loss: 962.1454 - val_mse: 962.1454 - val_mae: 21.4949\n",
      "Epoch 39/200\n",
      "25/25 [==============================] - 4s 168ms/step - loss: 897.3201 - mse: 897.3201 - mae: 20.6924 - val_loss: 964.2725 - val_mse: 964.2725 - val_mae: 21.9074\n",
      "Epoch 40/200\n",
      "25/25 [==============================] - 4s 144ms/step - loss: 865.9007 - mse: 865.9007 - mae: 20.3176 - val_loss: 948.8651 - val_mse: 948.8651 - val_mae: 21.4235\n",
      "Epoch 41/200\n",
      "25/25 [==============================] - 4s 147ms/step - loss: 856.9461 - mse: 856.9461 - mae: 20.3261 - val_loss: 994.4719 - val_mse: 994.4719 - val_mae: 22.0953\n",
      "Epoch 42/200\n",
      "25/25 [==============================] - 4s 141ms/step - loss: 833.6544 - mse: 833.6544 - mae: 20.0289 - val_loss: 950.6169 - val_mse: 950.6169 - val_mae: 22.1267\n",
      "Epoch 43/200\n",
      "25/25 [==============================] - 4s 157ms/step - loss: 841.0836 - mse: 841.0836 - mae: 20.2797 - val_loss: 869.3644 - val_mse: 869.3644 - val_mae: 20.7099\n",
      "Epoch 44/200\n",
      "25/25 [==============================] - 4s 159ms/step - loss: 812.3610 - mse: 812.3610 - mae: 19.9130 - val_loss: 834.6273 - val_mse: 834.6273 - val_mae: 20.0685\n",
      "Epoch 45/200\n",
      "25/25 [==============================] - 4s 153ms/step - loss: 771.6583 - mse: 771.6583 - mae: 19.1512 - val_loss: 830.3724 - val_mse: 830.3724 - val_mae: 20.5271\n",
      "Epoch 46/200\n",
      "25/25 [==============================] - 4s 158ms/step - loss: 733.1927 - mse: 733.1927 - mae: 18.6493 - val_loss: 777.1512 - val_mse: 777.1512 - val_mae: 19.5465\n",
      "Epoch 47/200\n",
      "25/25 [==============================] - 4s 166ms/step - loss: 699.8812 - mse: 699.8812 - mae: 18.2038 - val_loss: 743.5876 - val_mse: 743.5876 - val_mae: 18.7320\n",
      "Epoch 48/200\n",
      "25/25 [==============================] - 4s 153ms/step - loss: 688.4070 - mse: 688.4069 - mae: 18.1413 - val_loss: 769.4493 - val_mse: 769.4493 - val_mae: 19.1445\n",
      "Epoch 49/200\n",
      "25/25 [==============================] - 4s 156ms/step - loss: 685.3830 - mse: 685.3830 - mae: 18.1294 - val_loss: 733.6934 - val_mse: 733.6934 - val_mae: 18.8078\n",
      "Epoch 50/200\n",
      "25/25 [==============================] - 4s 159ms/step - loss: 681.0991 - mse: 681.0991 - mae: 18.1090 - val_loss: 688.7470 - val_mse: 688.7470 - val_mae: 18.1420\n",
      "Epoch 51/200\n",
      "25/25 [==============================] - 4s 147ms/step - loss: 640.2861 - mse: 640.2861 - mae: 17.3346 - val_loss: 720.5627 - val_mse: 720.5627 - val_mae: 18.6639\n",
      "Epoch 52/200\n",
      "25/25 [==============================] - 3s 135ms/step - loss: 627.4186 - mse: 627.4186 - mae: 17.2251 - val_loss: 724.9789 - val_mse: 724.9789 - val_mae: 18.4652\n",
      "Epoch 53/200\n",
      "25/25 [==============================] - 4s 143ms/step - loss: 622.3355 - mse: 622.3355 - mae: 17.1159 - val_loss: 648.3165 - val_mse: 648.3165 - val_mae: 17.4904\n",
      "Epoch 54/200\n",
      "25/25 [==============================] - 4s 167ms/step - loss: 616.5703 - mse: 616.5703 - mae: 17.0907 - val_loss: 703.1037 - val_mse: 703.1037 - val_mae: 18.4710\n",
      "Epoch 55/200\n",
      "25/25 [==============================] - 4s 178ms/step - loss: 606.8484 - mse: 606.8484 - mae: 17.0142 - val_loss: 663.3854 - val_mse: 663.3854 - val_mae: 17.7695\n",
      "Epoch 56/200\n",
      "25/25 [==============================] - 4s 166ms/step - loss: 587.7048 - mse: 587.7048 - mae: 16.7151 - val_loss: 632.1068 - val_mse: 632.1068 - val_mae: 17.4182\n",
      "Epoch 57/200\n",
      "25/25 [==============================] - 4s 176ms/step - loss: 557.5400 - mse: 557.5400 - mae: 16.2077 - val_loss: 620.4120 - val_mse: 620.4120 - val_mae: 17.2542\n",
      "Epoch 58/200\n",
      "25/25 [==============================] - 5s 201ms/step - loss: 554.0064 - mse: 554.0064 - mae: 16.2686 - val_loss: 621.2744 - val_mse: 621.2744 - val_mae: 17.1713\n",
      "Epoch 59/200\n",
      "25/25 [==============================] - 4s 170ms/step - loss: 547.3713 - mse: 547.3713 - mae: 16.1425 - val_loss: 648.7762 - val_mse: 648.7762 - val_mae: 17.7623\n",
      "Epoch 60/200\n",
      "25/25 [==============================] - 4s 156ms/step - loss: 535.5050 - mse: 535.5050 - mae: 15.9008 - val_loss: 647.7410 - val_mse: 647.7410 - val_mae: 17.9668\n",
      "Epoch 61/200\n",
      "25/25 [==============================] - 4s 161ms/step - loss: 524.9409 - mse: 524.9409 - mae: 15.7168 - val_loss: 578.1277 - val_mse: 578.1277 - val_mae: 16.6791\n",
      "Epoch 62/200\n",
      "25/25 [==============================] - 4s 161ms/step - loss: 504.6892 - mse: 504.6892 - mae: 15.4255 - val_loss: 567.3614 - val_mse: 567.3614 - val_mae: 16.3485\n",
      "Epoch 63/200\n",
      "25/25 [==============================] - 5s 180ms/step - loss: 499.8128 - mse: 499.8128 - mae: 15.4083 - val_loss: 559.4894 - val_mse: 559.4894 - val_mae: 16.2188\n",
      "Epoch 64/200\n",
      "25/25 [==============================] - 5s 194ms/step - loss: 511.0975 - mse: 511.0975 - mae: 15.5652 - val_loss: 564.5638 - val_mse: 564.5638 - val_mae: 16.4789\n",
      "Epoch 65/200\n",
      "25/25 [==============================] - 4s 159ms/step - loss: 474.5294 - mse: 474.5294 - mae: 14.9632 - val_loss: 536.7608 - val_mse: 536.7608 - val_mae: 15.9458\n",
      "Epoch 66/200\n",
      "25/25 [==============================] - 4s 151ms/step - loss: 479.1573 - mse: 479.1573 - mae: 14.9782 - val_loss: 529.4827 - val_mse: 529.4827 - val_mae: 15.7921\n",
      "Epoch 67/200\n",
      "25/25 [==============================] - 3s 130ms/step - loss: 470.4416 - mse: 470.4416 - mae: 14.9430 - val_loss: 544.2444 - val_mse: 544.2444 - val_mae: 15.9592\n",
      "Epoch 68/200\n",
      "25/25 [==============================] - 4s 144ms/step - loss: 466.3016 - mse: 466.3016 - mae: 14.9181 - val_loss: 537.8121 - val_mse: 537.8121 - val_mae: 15.8007\n",
      "Epoch 69/200\n",
      "25/25 [==============================] - 4s 161ms/step - loss: 474.7747 - mse: 474.7747 - mae: 15.0738 - val_loss: 514.9600 - val_mse: 514.9600 - val_mae: 15.6272\n",
      "Epoch 70/200\n",
      "25/25 [==============================] - 3s 133ms/step - loss: 459.3263 - mse: 459.3263 - mae: 14.7928 - val_loss: 512.3223 - val_mse: 512.3223 - val_mae: 15.7220\n",
      "Epoch 71/200\n",
      "25/25 [==============================] - 4s 155ms/step - loss: 445.8442 - mse: 445.8442 - mae: 14.5131 - val_loss: 499.0914 - val_mse: 499.0914 - val_mae: 15.2883\n",
      "Epoch 72/200\n",
      "25/25 [==============================] - 4s 150ms/step - loss: 464.8692 - mse: 464.8692 - mae: 14.9266 - val_loss: 520.7308 - val_mse: 520.7308 - val_mae: 15.7202\n",
      "Epoch 73/200\n",
      "25/25 [==============================] - 4s 151ms/step - loss: 447.8028 - mse: 447.8028 - mae: 14.6679 - val_loss: 488.7882 - val_mse: 488.7882 - val_mae: 15.1089\n",
      "Epoch 74/200\n",
      "25/25 [==============================] - 4s 178ms/step - loss: 428.1448 - mse: 428.1448 - mae: 14.2286 - val_loss: 498.2864 - val_mse: 498.2864 - val_mae: 15.2966\n",
      "Epoch 75/200\n",
      "25/25 [==============================] - 4s 170ms/step - loss: 424.0739 - mse: 424.0739 - mae: 14.2036 - val_loss: 528.8030 - val_mse: 528.8030 - val_mae: 16.4303\n",
      "Epoch 76/200\n",
      "25/25 [==============================] - 4s 150ms/step - loss: 437.4013 - mse: 437.4013 - mae: 14.5094 - val_loss: 487.6015 - val_mse: 487.6015 - val_mae: 15.3178\n",
      "Epoch 77/200\n",
      "25/25 [==============================] - 3s 131ms/step - loss: 431.7274 - mse: 431.7274 - mae: 14.4647 - val_loss: 467.7789 - val_mse: 467.7789 - val_mae: 14.8877\n",
      "Epoch 78/200\n",
      "25/25 [==============================] - 4s 147ms/step - loss: 418.4904 - mse: 418.4904 - mae: 14.1606 - val_loss: 480.9123 - val_mse: 480.9123 - val_mae: 14.8649\n",
      "Epoch 79/200\n",
      "25/25 [==============================] - 4s 158ms/step - loss: 407.5963 - mse: 407.5963 - mae: 13.9779 - val_loss: 453.2667 - val_mse: 453.2667 - val_mae: 14.4591\n",
      "Epoch 80/200\n",
      "25/25 [==============================] - 4s 160ms/step - loss: 403.3068 - mse: 403.3068 - mae: 13.8676 - val_loss: 481.3427 - val_mse: 481.3427 - val_mae: 15.1084\n",
      "Epoch 81/200\n",
      "25/25 [==============================] - 4s 143ms/step - loss: 409.3704 - mse: 409.3704 - mae: 13.9779 - val_loss: 460.9469 - val_mse: 460.9469 - val_mae: 14.7556\n",
      "Epoch 82/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 4s 162ms/step - loss: 408.0902 - mse: 408.0902 - mae: 13.9309 - val_loss: 447.2578 - val_mse: 447.2578 - val_mae: 14.5231\n",
      "Epoch 83/200\n",
      "25/25 [==============================] - 4s 156ms/step - loss: 400.6190 - mse: 400.6190 - mae: 13.8358 - val_loss: 480.9281 - val_mse: 480.9281 - val_mae: 14.9707\n",
      "Epoch 84/200\n",
      "25/25 [==============================] - 4s 157ms/step - loss: 373.6512 - mse: 373.6512 - mae: 13.3700 - val_loss: 443.9196 - val_mse: 443.9196 - val_mae: 14.3092\n",
      "Epoch 85/200\n",
      "25/25 [==============================] - 3s 138ms/step - loss: 368.2013 - mse: 368.2013 - mae: 13.2112 - val_loss: 435.8628 - val_mse: 435.8628 - val_mae: 14.2411\n",
      "Epoch 86/200\n",
      "25/25 [==============================] - 5s 195ms/step - loss: 363.4749 - mse: 363.4749 - mae: 13.1356 - val_loss: 420.0450 - val_mse: 420.0450 - val_mae: 13.9581\n",
      "Epoch 87/200\n",
      "25/25 [==============================] - 4s 160ms/step - loss: 367.3010 - mse: 367.3010 - mae: 13.2342 - val_loss: 420.3189 - val_mse: 420.3189 - val_mae: 13.9847\n",
      "Epoch 88/200\n",
      "25/25 [==============================] - 4s 155ms/step - loss: 361.4899 - mse: 361.4899 - mae: 13.1720 - val_loss: 433.5689 - val_mse: 433.5689 - val_mae: 14.1888\n",
      "Epoch 89/200\n",
      "25/25 [==============================] - 4s 145ms/step - loss: 368.0341 - mse: 368.0341 - mae: 13.3544 - val_loss: 417.5242 - val_mse: 417.5242 - val_mae: 14.0486\n",
      "Epoch 90/200\n",
      "25/25 [==============================] - 5s 189ms/step - loss: 360.1202 - mse: 360.1202 - mae: 13.0981 - val_loss: 445.3580 - val_mse: 445.3580 - val_mae: 14.2879\n",
      "Epoch 91/200\n",
      "25/25 [==============================] - 4s 162ms/step - loss: 365.1097 - mse: 365.1097 - mae: 13.2723 - val_loss: 415.0117 - val_mse: 415.0117 - val_mae: 13.9295\n",
      "Epoch 92/200\n",
      "25/25 [==============================] - 5s 185ms/step - loss: 363.7764 - mse: 363.7764 - mae: 13.3039 - val_loss: 428.4998 - val_mse: 428.4998 - val_mae: 14.6148\n",
      "Epoch 93/200\n",
      "25/25 [==============================] - 4s 152ms/step - loss: 336.5612 - mse: 336.5612 - mae: 12.7150 - val_loss: 405.6527 - val_mse: 405.6527 - val_mae: 13.6549\n",
      "Epoch 94/200\n",
      "25/25 [==============================] - 4s 146ms/step - loss: 338.8207 - mse: 338.8207 - mae: 12.7411 - val_loss: 425.3383 - val_mse: 425.3383 - val_mae: 14.1026\n",
      "Epoch 95/200\n",
      "25/25 [==============================] - 4s 177ms/step - loss: 341.2867 - mse: 341.2867 - mae: 12.8030 - val_loss: 401.1565 - val_mse: 401.1565 - val_mae: 13.5942\n",
      "Epoch 96/200\n",
      "25/25 [==============================] - 4s 169ms/step - loss: 335.9424 - mse: 335.9424 - mae: 12.6558 - val_loss: 431.7142 - val_mse: 431.7142 - val_mae: 14.3631\n",
      "Epoch 97/200\n",
      "25/25 [==============================] - 4s 158ms/step - loss: 389.3882 - mse: 389.3882 - mae: 13.6687 - val_loss: 425.0619 - val_mse: 425.0619 - val_mae: 14.1116\n",
      "Epoch 98/200\n",
      "25/25 [==============================] - 4s 158ms/step - loss: 334.6018 - mse: 334.6018 - mae: 12.7117 - val_loss: 399.4522 - val_mse: 399.4522 - val_mae: 13.6872\n",
      "Epoch 99/200\n",
      "25/25 [==============================] - 4s 147ms/step - loss: 331.7396 - mse: 331.7396 - mae: 12.5969 - val_loss: 460.9109 - val_mse: 460.9109 - val_mae: 15.1243\n",
      "Epoch 100/200\n",
      "25/25 [==============================] - 4s 148ms/step - loss: 338.9357 - mse: 338.9357 - mae: 12.9567 - val_loss: 402.0235 - val_mse: 402.0235 - val_mae: 13.7218\n",
      "Epoch 101/200\n",
      "25/25 [==============================] - 4s 161ms/step - loss: 329.5687 - mse: 329.5687 - mae: 12.6577 - val_loss: 396.8037 - val_mse: 396.8037 - val_mae: 13.7574\n",
      "Epoch 102/200\n",
      "25/25 [==============================] - 4s 158ms/step - loss: 316.1654 - mse: 316.1654 - mae: 12.3590 - val_loss: 414.0199 - val_mse: 414.0199 - val_mae: 14.0310\n",
      "Epoch 103/200\n",
      "25/25 [==============================] - 4s 157ms/step - loss: 307.5122 - mse: 307.5122 - mae: 12.1925 - val_loss: 385.5800 - val_mse: 385.5800 - val_mae: 13.4936\n",
      "Epoch 104/200\n",
      "25/25 [==============================] - 3s 129ms/step - loss: 320.4788 - mse: 320.4788 - mae: 12.4778 - val_loss: 435.4843 - val_mse: 435.4843 - val_mae: 14.5286\n",
      "Epoch 105/200\n",
      "25/25 [==============================] - 4s 156ms/step - loss: 337.5972 - mse: 337.5972 - mae: 12.8903 - val_loss: 391.9952 - val_mse: 391.9952 - val_mae: 13.5427\n",
      "Epoch 106/200\n",
      "25/25 [==============================] - 4s 156ms/step - loss: 322.9999 - mse: 322.9999 - mae: 12.5636 - val_loss: 400.5432 - val_mse: 400.5432 - val_mae: 13.7069\n",
      "Epoch 107/200\n",
      "25/25 [==============================] - 4s 157ms/step - loss: 320.6254 - mse: 320.6254 - mae: 12.5508 - val_loss: 386.2260 - val_mse: 386.2260 - val_mae: 13.4582\n",
      "Epoch 108/200\n",
      "25/25 [==============================] - 4s 158ms/step - loss: 310.0524 - mse: 310.0524 - mae: 12.3209 - val_loss: 387.5131 - val_mse: 387.5131 - val_mae: 13.3925\n",
      "Epoch 109/200\n",
      "25/25 [==============================] - 4s 144ms/step - loss: 296.5026 - mse: 296.5026 - mae: 12.0529 - val_loss: 379.0928 - val_mse: 379.0928 - val_mae: 13.3592\n",
      "Epoch 110/200\n",
      "25/25 [==============================] - 4s 176ms/step - loss: 293.5245 - mse: 293.5245 - mae: 11.9330 - val_loss: 381.3674 - val_mse: 381.3674 - val_mae: 13.4862\n",
      "Epoch 111/200\n",
      "25/25 [==============================] - 5s 202ms/step - loss: 296.9891 - mse: 296.9891 - mae: 12.0846 - val_loss: 359.2244 - val_mse: 359.2244 - val_mae: 12.8905\n",
      "Epoch 112/200\n",
      "25/25 [==============================] - 5s 187ms/step - loss: 279.8451 - mse: 279.8451 - mae: 11.6549 - val_loss: 357.6879 - val_mse: 357.6879 - val_mae: 12.8444\n",
      "Epoch 113/200\n",
      "25/25 [==============================] - 4s 143ms/step - loss: 294.1183 - mse: 294.1183 - mae: 12.0600 - val_loss: 385.0463 - val_mse: 385.0463 - val_mae: 13.3779\n",
      "Epoch 114/200\n",
      "25/25 [==============================] - 4s 166ms/step - loss: 292.0287 - mse: 292.0287 - mae: 11.9013 - val_loss: 408.1286 - val_mse: 408.1286 - val_mae: 14.4153\n",
      "Epoch 115/200\n",
      "25/25 [==============================] - 5s 191ms/step - loss: 287.1104 - mse: 287.1104 - mae: 11.9316 - val_loss: 362.2529 - val_mse: 362.2529 - val_mae: 13.0197\n",
      "Epoch 116/200\n",
      "25/25 [==============================] - 5s 195ms/step - loss: 276.8552 - mse: 276.8552 - mae: 11.6478 - val_loss: 349.0628 - val_mse: 349.0628 - val_mae: 12.7082\n",
      "Epoch 117/200\n",
      "25/25 [==============================] - 4s 164ms/step - loss: 277.7053 - mse: 277.7053 - mae: 11.6605 - val_loss: 350.1692 - val_mse: 350.1692 - val_mae: 12.6884\n",
      "Epoch 118/200\n",
      "25/25 [==============================] - 5s 186ms/step - loss: 293.5217 - mse: 293.5217 - mae: 12.0425 - val_loss: 395.6639 - val_mse: 395.6639 - val_mae: 13.5505\n",
      "Epoch 119/200\n",
      "25/25 [==============================] - 4s 154ms/step - loss: 290.1198 - mse: 290.1198 - mae: 11.8921 - val_loss: 392.6546 - val_mse: 392.6546 - val_mae: 13.6448\n",
      "Epoch 120/200\n",
      "25/25 [==============================] - 4s 162ms/step - loss: 292.1737 - mse: 292.1737 - mae: 12.1098 - val_loss: 358.8746 - val_mse: 358.8746 - val_mae: 12.9434\n",
      "Epoch 121/200\n",
      "25/25 [==============================] - 4s 169ms/step - loss: 260.1385 - mse: 260.1385 - mae: 11.3300 - val_loss: 351.5430 - val_mse: 351.5430 - val_mae: 12.7479\n",
      "Epoch 122/200\n",
      "25/25 [==============================] - 5s 183ms/step - loss: 265.3006 - mse: 265.3006 - mae: 11.4531 - val_loss: 347.7034 - val_mse: 347.7034 - val_mae: 12.7266\n",
      "Epoch 123/200\n",
      "25/25 [==============================] - 4s 146ms/step - loss: 250.8485 - mse: 250.8485 - mae: 11.1277 - val_loss: 346.6185 - val_mse: 346.6185 - val_mae: 12.6387\n",
      "Epoch 124/200\n",
      "25/25 [==============================] - 4s 173ms/step - loss: 256.0360 - mse: 256.0360 - mae: 11.2641 - val_loss: 339.7337 - val_mse: 339.7337 - val_mae: 12.4493\n",
      "Epoch 125/200\n",
      "25/25 [==============================] - 4s 165ms/step - loss: 260.6787 - mse: 260.6787 - mae: 11.3558 - val_loss: 371.7368 - val_mse: 371.7368 - val_mae: 13.1395\n",
      "Epoch 126/200\n",
      "25/25 [==============================] - 5s 181ms/step - loss: 253.2947 - mse: 253.2947 - mae: 11.2034 - val_loss: 379.1125 - val_mse: 379.1125 - val_mae: 13.6640\n",
      "Epoch 127/200\n",
      "25/25 [==============================] - 4s 161ms/step - loss: 260.7390 - mse: 260.7390 - mae: 11.4356 - val_loss: 346.7894 - val_mse: 346.7894 - val_mae: 12.7012\n",
      "Epoch 128/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 4s 144ms/step - loss: 242.6524 - mse: 242.6524 - mae: 10.9940 - val_loss: 346.7868 - val_mse: 346.7868 - val_mae: 12.5905\n",
      "Epoch 129/200\n",
      "25/25 [==============================] - 4s 152ms/step - loss: 251.0874 - mse: 251.0874 - mae: 11.1600 - val_loss: 353.6856 - val_mse: 353.6856 - val_mae: 12.8329\n",
      "Epoch 130/200\n",
      "25/25 [==============================] - 4s 174ms/step - loss: 249.5507 - mse: 249.5507 - mae: 11.1630 - val_loss: 334.8743 - val_mse: 334.8743 - val_mae: 12.4928\n",
      "Epoch 131/200\n",
      "25/25 [==============================] - 4s 153ms/step - loss: 238.7658 - mse: 238.7658 - mae: 10.9107 - val_loss: 339.5143 - val_mse: 339.5143 - val_mae: 12.5742\n",
      "Epoch 132/200\n",
      "25/25 [==============================] - 4s 155ms/step - loss: 250.2217 - mse: 250.2217 - mae: 11.2043 - val_loss: 337.3073 - val_mse: 337.3073 - val_mae: 12.4484\n",
      "Epoch 133/200\n",
      "25/25 [==============================] - 4s 168ms/step - loss: 250.6822 - mse: 250.6822 - mae: 11.2585 - val_loss: 336.6669 - val_mse: 336.6669 - val_mae: 12.5309\n",
      "Epoch 134/200\n",
      "25/25 [==============================] - 4s 179ms/step - loss: 240.8768 - mse: 240.8768 - mae: 11.0091 - val_loss: 344.6201 - val_mse: 344.6201 - val_mae: 12.5647\n",
      "Epoch 135/200\n",
      "25/25 [==============================] - 4s 179ms/step - loss: 234.5609 - mse: 234.5609 - mae: 10.8430 - val_loss: 361.2390 - val_mse: 361.2390 - val_mae: 13.0284\n",
      "Epoch 136/200\n",
      "25/25 [==============================] - 5s 182ms/step - loss: 235.0978 - mse: 235.0978 - mae: 10.8045 - val_loss: 327.3969 - val_mse: 327.3969 - val_mae: 12.3099\n",
      "Epoch 137/200\n",
      "25/25 [==============================] - 4s 174ms/step - loss: 230.6534 - mse: 230.6534 - mae: 10.7553 - val_loss: 323.2342 - val_mse: 323.2342 - val_mae: 12.1553\n",
      "Epoch 138/200\n",
      "25/25 [==============================] - 5s 183ms/step - loss: 223.8229 - mse: 223.8229 - mae: 10.6366 - val_loss: 328.1110 - val_mse: 328.1110 - val_mae: 12.2249\n",
      "Epoch 139/200\n",
      "25/25 [==============================] - 5s 185ms/step - loss: 229.9131 - mse: 229.9131 - mae: 10.7624 - val_loss: 328.0148 - val_mse: 328.0148 - val_mae: 12.2974\n",
      "Epoch 140/200\n",
      "25/25 [==============================] - 5s 195ms/step - loss: 224.6234 - mse: 224.6234 - mae: 10.6368 - val_loss: 324.9598 - val_mse: 324.9598 - val_mae: 12.2973\n",
      "Epoch 141/200\n",
      "25/25 [==============================] - 5s 193ms/step - loss: 236.5435 - mse: 236.5435 - mae: 11.0143 - val_loss: 364.3330 - val_mse: 364.3330 - val_mae: 13.4031\n",
      "Epoch 142/200\n",
      "25/25 [==============================] - 4s 177ms/step - loss: 227.0150 - mse: 227.0150 - mae: 10.7424 - val_loss: 326.5111 - val_mse: 326.5111 - val_mae: 12.2478\n",
      "Epoch 143/200\n",
      "25/25 [==============================] - 4s 156ms/step - loss: 223.5285 - mse: 223.5285 - mae: 10.6350 - val_loss: 337.5872 - val_mse: 337.5872 - val_mae: 12.4962\n",
      "Epoch 144/200\n",
      "25/25 [==============================] - 4s 170ms/step - loss: 228.8432 - mse: 228.8432 - mae: 10.7570 - val_loss: 321.0182 - val_mse: 321.0182 - val_mae: 12.1321\n",
      "Epoch 145/200\n",
      "25/25 [==============================] - 4s 177ms/step - loss: 212.7913 - mse: 212.7913 - mae: 10.3024 - val_loss: 328.9620 - val_mse: 328.9620 - val_mae: 12.5454\n",
      "Epoch 146/200\n",
      "25/25 [==============================] - 5s 196ms/step - loss: 220.9537 - mse: 220.9537 - mae: 10.5940 - val_loss: 318.9167 - val_mse: 318.9167 - val_mae: 12.0794\n",
      "Epoch 147/200\n",
      "25/25 [==============================] - 5s 188ms/step - loss: 213.4060 - mse: 213.4060 - mae: 10.4533 - val_loss: 326.3934 - val_mse: 326.3934 - val_mae: 12.2644\n",
      "Epoch 148/200\n",
      "25/25 [==============================] - 4s 164ms/step - loss: 210.7785 - mse: 210.7785 - mae: 10.3576 - val_loss: 312.1074 - val_mse: 312.1074 - val_mae: 12.0374\n",
      "Epoch 149/200\n",
      "25/25 [==============================] - 4s 179ms/step - loss: 205.3770 - mse: 205.3770 - mae: 10.1849 - val_loss: 340.6773 - val_mse: 340.6773 - val_mae: 12.8206\n",
      "Epoch 150/200\n",
      "25/25 [==============================] - 4s 155ms/step - loss: 202.0609 - mse: 202.0609 - mae: 10.1460 - val_loss: 313.0651 - val_mse: 313.0651 - val_mae: 11.9332\n",
      "Epoch 151/200\n",
      "25/25 [==============================] - 4s 167ms/step - loss: 203.0493 - mse: 203.0493 - mae: 10.1650 - val_loss: 302.1802 - val_mse: 302.1802 - val_mae: 11.6766\n",
      "Epoch 152/200\n",
      "25/25 [==============================] - 4s 180ms/step - loss: 203.7404 - mse: 203.7404 - mae: 10.2018 - val_loss: 331.9104 - val_mse: 331.9104 - val_mae: 12.3978\n",
      "Epoch 153/200\n",
      "25/25 [==============================] - 5s 181ms/step - loss: 206.8837 - mse: 206.8837 - mae: 10.2880 - val_loss: 344.3722 - val_mse: 344.3722 - val_mae: 12.9232\n",
      "Epoch 154/200\n",
      "25/25 [==============================] - 4s 164ms/step - loss: 223.7874 - mse: 223.7874 - mae: 10.7475 - val_loss: 333.0315 - val_mse: 333.0315 - val_mae: 12.5576\n",
      "Epoch 155/200\n",
      "25/25 [==============================] - 4s 158ms/step - loss: 217.7411 - mse: 217.7411 - mae: 10.5440 - val_loss: 314.5222 - val_mse: 314.5222 - val_mae: 12.0332\n",
      "Epoch 156/200\n",
      "25/25 [==============================] - 4s 162ms/step - loss: 226.9905 - mse: 226.9905 - mae: 10.7739 - val_loss: 321.1727 - val_mse: 321.1727 - val_mae: 12.1627\n",
      "Epoch 157/200\n",
      "25/25 [==============================] - 4s 160ms/step - loss: 223.0063 - mse: 223.0063 - mae: 10.7422 - val_loss: 324.2977 - val_mse: 324.2977 - val_mae: 12.2577\n",
      "Epoch 158/200\n",
      "25/25 [==============================] - 4s 162ms/step - loss: 211.6121 - mse: 211.6121 - mae: 10.4450 - val_loss: 323.9509 - val_mse: 323.9509 - val_mae: 12.2442\n",
      "Epoch 159/200\n",
      "25/25 [==============================] - 3s 139ms/step - loss: 207.0761 - mse: 207.0761 - mae: 10.3926 - val_loss: 319.6476 - val_mse: 319.6476 - val_mae: 12.1370\n",
      "Epoch 160/200\n",
      "25/25 [==============================] - 4s 140ms/step - loss: 203.7672 - mse: 203.7672 - mae: 10.2678 - val_loss: 310.2253 - val_mse: 310.2253 - val_mae: 11.9566\n",
      "Epoch 161/200\n",
      "25/25 [==============================] - 4s 157ms/step - loss: 215.7969 - mse: 215.7969 - mae: 10.5733 - val_loss: 308.9997 - val_mse: 308.9997 - val_mae: 11.9313\n",
      "Epoch 162/200\n",
      "25/25 [==============================] - 4s 150ms/step - loss: 199.0217 - mse: 199.0217 - mae: 10.1596 - val_loss: 321.2689 - val_mse: 321.2689 - val_mae: 12.2877\n",
      "Epoch 163/200\n",
      "25/25 [==============================] - 4s 155ms/step - loss: 196.9104 - mse: 196.9104 - mae: 10.1077 - val_loss: 327.1948 - val_mse: 327.1948 - val_mae: 12.5071\n",
      "Epoch 164/200\n",
      "25/25 [==============================] - 4s 155ms/step - loss: 207.6376 - mse: 207.6376 - mae: 10.4222 - val_loss: 320.3142 - val_mse: 320.3142 - val_mae: 12.0837\n",
      "Epoch 165/200\n",
      "25/25 [==============================] - 4s 157ms/step - loss: 206.0265 - mse: 206.0265 - mae: 10.3845 - val_loss: 309.0421 - val_mse: 309.0421 - val_mae: 12.0021\n",
      "Epoch 166/200\n",
      "25/25 [==============================] - 4s 159ms/step - loss: 186.5966 - mse: 186.5966 - mae: 9.8864 - val_loss: 298.5283 - val_mse: 298.5283 - val_mae: 11.7065\n",
      "Epoch 167/200\n",
      "25/25 [==============================] - 4s 144ms/step - loss: 188.1030 - mse: 188.1030 - mae: 9.8888 - val_loss: 371.5761 - val_mse: 371.5761 - val_mae: 13.6877\n",
      "Epoch 168/200\n",
      "25/25 [==============================] - 3s 133ms/step - loss: 180.3194 - mse: 180.3194 - mae: 9.6996 - val_loss: 297.4138 - val_mse: 297.4138 - val_mae: 11.6454\n",
      "Epoch 169/200\n",
      "25/25 [==============================] - 4s 168ms/step - loss: 177.1927 - mse: 177.1927 - mae: 9.5902 - val_loss: 333.8442 - val_mse: 333.8442 - val_mae: 12.4977\n",
      "Epoch 170/200\n",
      "25/25 [==============================] - 4s 158ms/step - loss: 182.4614 - mse: 182.4614 - mae: 9.7859 - val_loss: 312.8981 - val_mse: 312.8981 - val_mae: 12.2536\n",
      "Epoch 171/200\n",
      "25/25 [==============================] - 4s 160ms/step - loss: 198.2523 - mse: 198.2523 - mae: 10.2057 - val_loss: 309.2553 - val_mse: 309.2553 - val_mae: 11.8663\n",
      "Epoch 172/200\n",
      "25/25 [==============================] - 4s 160ms/step - loss: 190.4346 - mse: 190.4346 - mae: 9.9856 - val_loss: 326.6878 - val_mse: 326.6878 - val_mae: 12.4515\n",
      "Epoch 173/200\n",
      "25/25 [==============================] - 4s 158ms/step - loss: 184.8281 - mse: 184.8281 - mae: 9.9137 - val_loss: 300.0120 - val_mse: 300.0120 - val_mae: 11.7237\n",
      "Epoch 174/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 4s 160ms/step - loss: 178.8717 - mse: 178.8717 - mae: 9.7021 - val_loss: 299.3088 - val_mse: 299.3088 - val_mae: 11.6488\n",
      "Epoch 175/200\n",
      "25/25 [==============================] - 4s 162ms/step - loss: 173.2027 - mse: 173.2027 - mae: 9.5133 - val_loss: 294.8911 - val_mse: 294.8911 - val_mae: 11.5861\n",
      "Epoch 176/200\n",
      "25/25 [==============================] - 4s 155ms/step - loss: 187.7842 - mse: 187.7842 - mae: 9.9842 - val_loss: 308.9007 - val_mse: 308.9007 - val_mae: 11.8139\n",
      "Epoch 177/200\n",
      "25/25 [==============================] - 3s 138ms/step - loss: 189.9111 - mse: 189.9111 - mae: 10.0011 - val_loss: 308.9485 - val_mse: 308.9485 - val_mae: 11.8246\n",
      "Epoch 178/200\n",
      "25/25 [==============================] - 4s 160ms/step - loss: 190.0991 - mse: 190.0991 - mae: 10.0090 - val_loss: 318.9729 - val_mse: 318.9729 - val_mae: 12.4299\n",
      "Epoch 179/200\n",
      "25/25 [==============================] - 4s 155ms/step - loss: 190.7363 - mse: 190.7363 - mae: 10.1009 - val_loss: 303.9808 - val_mse: 303.9808 - val_mae: 11.8571\n",
      "Epoch 180/200\n",
      "25/25 [==============================] - 4s 162ms/step - loss: 195.9860 - mse: 195.9860 - mae: 10.2169 - val_loss: 352.6444 - val_mse: 352.6444 - val_mae: 12.9382\n",
      "Epoch 181/200\n",
      "25/25 [==============================] - 4s 156ms/step - loss: 188.6068 - mse: 188.6068 - mae: 10.0003 - val_loss: 323.5290 - val_mse: 323.5290 - val_mae: 12.1860\n",
      "Epoch 182/200\n",
      "25/25 [==============================] - 4s 147ms/step - loss: 187.9719 - mse: 187.9719 - mae: 9.9894 - val_loss: 304.7217 - val_mse: 304.7217 - val_mae: 11.8780\n",
      "Epoch 183/200\n",
      "25/25 [==============================] - 3s 136ms/step - loss: 195.1549 - mse: 195.1549 - mae: 10.1854 - val_loss: 299.9836 - val_mse: 299.9836 - val_mae: 11.6564\n",
      "Epoch 184/200\n",
      "25/25 [==============================] - 4s 163ms/step - loss: 177.8138 - mse: 177.8138 - mae: 9.7376 - val_loss: 292.7481 - val_mse: 292.7481 - val_mae: 11.4970\n",
      "Epoch 185/200\n",
      "25/25 [==============================] - 4s 155ms/step - loss: 176.0325 - mse: 176.0325 - mae: 9.7014 - val_loss: 304.6665 - val_mse: 304.6665 - val_mae: 11.9396\n",
      "Epoch 186/200\n",
      "25/25 [==============================] - 4s 162ms/step - loss: 178.0310 - mse: 178.0310 - mae: 9.6965 - val_loss: 306.8822 - val_mse: 306.8822 - val_mae: 11.9159\n",
      "Epoch 187/200\n",
      "25/25 [==============================] - 4s 156ms/step - loss: 177.0014 - mse: 177.0014 - mae: 9.6655 - val_loss: 290.2563 - val_mse: 290.2563 - val_mae: 11.5188\n",
      "Epoch 188/200\n",
      "25/25 [==============================] - 3s 131ms/step - loss: 172.1341 - mse: 172.1341 - mae: 9.5268 - val_loss: 304.1636 - val_mse: 304.1636 - val_mae: 11.8536\n",
      "Epoch 189/200\n",
      "25/25 [==============================] - 4s 158ms/step - loss: 188.0052 - mse: 188.0052 - mae: 9.9970 - val_loss: 323.4878 - val_mse: 323.4878 - val_mae: 12.4411\n",
      "Epoch 190/200\n",
      "25/25 [==============================] - 4s 164ms/step - loss: 168.9108 - mse: 168.9108 - mae: 9.4596 - val_loss: 318.9771 - val_mse: 318.9771 - val_mae: 12.3238\n",
      "Epoch 191/200\n",
      "25/25 [==============================] - 4s 164ms/step - loss: 167.0744 - mse: 167.0744 - mae: 9.4209 - val_loss: 292.5600 - val_mse: 292.5600 - val_mae: 11.4999\n",
      "Epoch 192/200\n",
      "25/25 [==============================] - 4s 158ms/step - loss: 168.2946 - mse: 168.2946 - mae: 9.4712 - val_loss: 302.8465 - val_mse: 302.8465 - val_mae: 11.9417\n",
      "Epoch 193/200\n",
      "25/25 [==============================] - 4s 154ms/step - loss: 165.4037 - mse: 165.4037 - mae: 9.4113 - val_loss: 282.8938 - val_mse: 282.8938 - val_mae: 11.2699\n",
      "Epoch 194/200\n",
      "25/25 [==============================] - 3s 133ms/step - loss: 157.9366 - mse: 157.9366 - mae: 9.1780 - val_loss: 319.6730 - val_mse: 319.6730 - val_mae: 12.4402\n",
      "Epoch 195/200\n",
      "25/25 [==============================] - 5s 199ms/step - loss: 160.3063 - mse: 160.3063 - mae: 9.2593 - val_loss: 291.9463 - val_mse: 291.9463 - val_mae: 11.5574\n",
      "Epoch 196/200\n",
      "25/25 [==============================] - 5s 183ms/step - loss: 157.1228 - mse: 157.1228 - mae: 9.1889 - val_loss: 294.8276 - val_mse: 294.8276 - val_mae: 11.6161\n",
      "Epoch 197/200\n",
      "25/25 [==============================] - 4s 167ms/step - loss: 167.3108 - mse: 167.3108 - mae: 9.4809 - val_loss: 294.2764 - val_mse: 294.2764 - val_mae: 11.6134\n",
      "Epoch 198/200\n",
      "25/25 [==============================] - 4s 159ms/step - loss: 169.0325 - mse: 169.0325 - mae: 9.5173 - val_loss: 306.9915 - val_mse: 306.9915 - val_mae: 11.8115\n",
      "Epoch 199/200\n",
      "25/25 [==============================] - 4s 156ms/step - loss: 156.9918 - mse: 156.9918 - mae: 9.1471 - val_loss: 285.2738 - val_mse: 285.2738 - val_mae: 11.2912\n",
      "Epoch 200/200\n",
      "25/25 [==============================] - 3s 138ms/step - loss: 158.8106 - mse: 158.8106 - mae: 9.2256 - val_loss: 280.7741 - val_mse: 280.7741 - val_mae: 11.2631\n",
      "WARNING:tensorflow:From c:\\users\\confusement\\miniconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From c:\\users\\confusement\\miniconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: 3daypm1_366\\assets\n",
      "17.269357391503988\n",
      "12.521252762564883\n",
      "11.446782968018203\n",
      "8.823588283538102\n",
      "dict_keys(['loss', 'mse', 'mae', 'val_loss', 'val_mse', 'val_mae'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dat = dataset(met,aqi,split_aqi)\n",
    "df = dat.mergedData('DL031',roll=48,shift=72)\n",
    "Xtrain,ytrain,Xtest,ytest = getSplitFeatures(df,TIME_SERIES_LENGTH = 24)\n",
    "model,history = trainModel1(Xtrain,ytrain,Xtest,ytest,TIME_SERIES_LENGTH=24)\n",
    "model.save('3daypm1_366')\n",
    "predictStats(model,Xtrain,ytrain,Xtest,ytest)\n",
    "plothistory(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif, f_regression , mutual_info_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.svm import SVR\n",
    "dfTrain = df[:]\n",
    "features = ['Temperature','Relative Humidity','windX','windY','Year','MonthX','MonthY','hourX','hourY','isWeekend']\n",
    "features.append(\"PM2.5_lag1\")\n",
    "features.append(\"PM2.5_lag2\")\n",
    "\n",
    "features.append(\"PM10_lag1\")\n",
    "features.append(\"PM10_lag2\")\n",
    "\n",
    "features.append(\"NO2_lag1\")\n",
    "features.append(\"NO2_lag2\")\n",
    "\n",
    "features.append(\"SO2_lag1\")\n",
    "features.append(\"SO2_lag2\")\n",
    "\n",
    "features.append(\"NO_lag1\")\n",
    "features.append(\"NO_lag2\")\n",
    "\n",
    "features.append(\"NOx_lag1\")\n",
    "features.append(\"NOx_lag2\")\n",
    "\n",
    "# features.append(\"CO_lag1\")\n",
    "# features.append(\"CO_lag2\")\n",
    "\n",
    "# features.append(\"O3_lag1\")\n",
    "# features.append(\"O3_lag2\")\n",
    "\n",
    "# features.append(\"NH3_lag1\")\n",
    "# features.append(\"NH3_lag2\")\n",
    "\n",
    "# features = []\n",
    "# rlist=['PM2.5','PM10','NO','NO2','CO']\n",
    "# newlist = rlist + ['Temperature','Relative Humidity','windX','windY','Year','MonthX','MonthY','hourX','hourY','isWeekend']\n",
    "# for i in newlist:\n",
    "#     for j in range(24):\n",
    "#         features.append(i+'_t-'+str(j))\n",
    "        \n",
    "X = df[features]\n",
    "y = df['PM2.5_pred3']\n",
    "scaler = StandardScaler()\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "scaler.fit(Xtrain)\n",
    "\n",
    "reg = LinearRegression().fit(scaler.transform(Xtrain), ytrain)\n",
    "\n",
    "testPred = reg.predict(scaler.transform(Xtest))\n",
    "trainPred = reg.predict(scaler.transform(Xtrain))\n",
    "mse = np.mean((testPred - np.array(ytest))*(testPred - np.array(ytest)))\n",
    "print(reg.score(scaler.transform(Xtest), ytest))\n",
    "print(mean_squared_error(testPred, ytest,squared=False))\n",
    "print(mean_squared_error(trainPred, ytrain,squared=False))\n",
    "print(mean_absolute_error(testPred, ytest))\n",
    "print(mean_absolute_error(trainPred, ytrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif, f_regression , mutual_info_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "dfTrain = df[:]\n",
    "features = ['Temperature','Relative Humidity','windX','windY','Year','MonthX','MonthY','hourX','hourY','isWeekend']\n",
    "features.append(\"PM2.5_lag1\")\n",
    "features.append(\"PM2.5_lag2\")\n",
    "\n",
    "features.append(\"PM10_lag1\")\n",
    "features.append(\"PM10_lag2\")\n",
    "\n",
    "features.append(\"NO2_lag1\")\n",
    "features.append(\"NO2_lag2\")\n",
    "\n",
    "features.append(\"SO2_lag1\")\n",
    "features.append(\"SO2_lag2\")\n",
    "\n",
    "features.append(\"NO_lag1\")\n",
    "features.append(\"NO_lag2\")\n",
    "\n",
    "features.append(\"NOx_lag1\")\n",
    "features.append(\"NOx_lag2\")\n",
    "\n",
    "features.append(\"CO_lag1\")\n",
    "features.append(\"CO_lag2\")\n",
    "\n",
    "features.append(\"O3_lag1\")\n",
    "features.append(\"O3_lag2\")\n",
    "\n",
    "features.append(\"NH3_lag1\")\n",
    "features.append(\"NH3_lag2\")\n",
    "\n",
    "features = []\n",
    "rlist=['PM2.5','PM10','NO','NO2','CO']\n",
    "newlist = rlist + ['Temperature','Relative Humidity','windX','windY','Year','MonthX','MonthY','hourX','hourY','isWeekend']\n",
    "for i in newlist:\n",
    "    for j in range(24):\n",
    "        features.append(i+'_t-'+str(j))\n",
    "        \n",
    "X = df[features]\n",
    "y = df['PM2.5_pred1']\n",
    "scaler = StandardScaler()\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "scaler.fit(Xtrain)\n",
    "\n",
    "reg = MLPRegressor(random_state=1, max_iter=100).fit(scaler.transform(Xtrain), ytrain)\n",
    "\n",
    "testPred = reg.predict(scaler.transform(Xtest))\n",
    "trainPred = reg.predict(scaler.transform(Xtrain))\n",
    "mse = np.mean((testPred - np.array(ytest))*(testPred - np.array(ytest)))\n",
    "print(reg.score(scaler.transform(Xtest), ytest))\n",
    "print(mean_squared_error(testPred, ytest,squared=False))\n",
    "print(mean_squared_error(trainPred, ytrain,squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7769, 360) (7769,)\n",
      "0.3379685723175532\n",
      "65.78093011534429\n",
      "67.0801815314519\n",
      "43.41983467544137\n",
      "43.25189223855267\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif, f_regression , mutual_info_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.svm import SVR\n",
    "dfTrain = df[:]\n",
    "features = ['Temperature','Relative Humidity','windX','windY','Year','MonthX','MonthY','hourX','hourY','isWeekend']\n",
    "features.append(\"PM2.5_lag1\")\n",
    "features.append(\"PM2.5_lag2\")\n",
    "\n",
    "features.append(\"PM10_lag1\")\n",
    "features.append(\"PM10_lag2\")\n",
    "\n",
    "features.append(\"NO2_lag1\")\n",
    "features.append(\"NO2_lag2\")\n",
    "\n",
    "features.append(\"SO2_lag1\")\n",
    "features.append(\"SO2_lag2\")\n",
    "\n",
    "features.append(\"NO_lag1\")\n",
    "features.append(\"NO_lag2\")\n",
    "\n",
    "features.append(\"NOx_lag1\")\n",
    "features.append(\"NOx_lag2\")\n",
    "\n",
    "features.append(\"CO_lag1\")\n",
    "features.append(\"CO_lag2\")\n",
    "\n",
    "features.append(\"O3_lag1\")\n",
    "features.append(\"O3_lag2\")\n",
    "\n",
    "features.append(\"NH3_lag1\")\n",
    "features.append(\"NH3_lag2\")\n",
    "\n",
    "features = []\n",
    "rlist=['PM2.5','PM10','NO','NO2','CO']\n",
    "newlist = rlist + ['Temperature','Relative Humidity','windX','windY','Year','MonthX','MonthY','hourX','hourY','isWeekend']\n",
    "for i in newlist:\n",
    "    for j in range(24):\n",
    "        features.append(i+'_t-'+str(j))\n",
    "\n",
    "X = df[features]\n",
    "y = df['PM2.5_pred1']\n",
    "scaler = StandardScaler()\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "print(Xtrain.shape,ytrain.shape)\n",
    "# scaler.fit(Xtrain)\n",
    "\n",
    "reg = SVR(C=3.0, epsilon=0.2).fit(Xtrain, ytrain)\n",
    "\n",
    "testPred = reg.predict(Xtest)\n",
    "trainPred = reg.predict(Xtrain)\n",
    "mse = np.mean((testPred - np.array(ytest))*(testPred - np.array(ytest)))\n",
    "print(reg.score(Xtest, ytest))\n",
    "print(mean_squared_error(testPred, ytest,squared=False))\n",
    "print(mean_squared_error(trainPred, ytrain,squared=False))\n",
    "print(mean_absolute_error(testPred, ytest))\n",
    "print(mean_absolute_error(trainPred, ytrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf  \n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "features = ['Temperature','Relative Humidity','windX','windY','Year','MonthX','MonthY','hourX','hourY','isWeekend']\n",
    "features = []\n",
    "rlist=['PM2.5','PM10','NO','NO2','CO']\n",
    "newlist = rlist + ['Temperature','Relative Humidity','windX','windY','Year','MonthX','MonthY','hourX','hourY','isWeekend']\n",
    "for i in newlist:\n",
    "    for j in range(24):\n",
    "        features.append(i+'_t-'+str(j))\n",
    "predVector = []\n",
    "for j in range(24):\n",
    "    predVector.append('PM2.5_t+'+str(j))\n",
    "X = df[features]\n",
    "y = df[predVector]\n",
    "scaler = StandardScaler()\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "scaler.fit(Xtrain)\n",
    "print(Xtrain.shape)\n",
    "model = Sequential()\n",
    "model.add(Dense(200, input_dim=360, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(24, activation='linear'))\n",
    "model.summary()\n",
    "#Fit\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['mse','mae'])\n",
    "history = model.fit(scaler.transform(Xtrain), ytrain, epochs=100, batch_size=50,  verbose=1, validation_split=0.2)\n",
    "#Print Accuracy\n",
    "testPred = model.predict(scaler.transform(Xtest))\n",
    "trainPred = model.predict(scaler.transform(Xtrain))\n",
    "print(mean_squared_error(testPred, ytest,squared=False))\n",
    "print(mean_squared_error(trainPred, ytrain,squared=False))\n",
    "print(mean_absolute_error(testPred, ytest))\n",
    "print(mean_absolute_error(trainPred, ytrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for loss\n",
    "plt.plot(np.sqrt(history.history['loss']))\n",
    "plt.plot(np.sqrt(history.history['val_loss']))\n",
    "plt.title('model loss')\n",
    "plt.ylabel('RMSE loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
