{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "RKP = \"DL031\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "sns.set_theme(style=\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\confusement\\miniconda3\\envs\\mlc\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3263: DtypeWarning: Columns (15) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['StationId', 'Datetime', 'PM2.5', 'PM10', 'NO', 'NO2', 'NOx', 'NH3',\n",
      "       'CO', 'SO2', 'O3', 'Benzene', 'Toluene', 'Xylene', 'AQI', 'AQI_Bucket'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Load datasets and rename columns, load all aqi data but specify metro data name\n",
    "def loadcsv(city=\"./data/rkpuram.csv\"):\n",
    "    met = pd.read_csv(city,delimiter=';',skiprows=24)\n",
    "    aqi = pd.read_csv('./data/station_hour.csv')\n",
    "    print(aqi.columns)\n",
    "    met.rename(columns={'# Date': 'Date',}, inplace=True)\n",
    "    met.rename(columns={'UT time': 'Time',}, inplace=True)\n",
    "    aqi['Time'] = aqi['Datetime'].str[-8:-3]\n",
    "    aqi['Date'] = aqi['Datetime'].str[0:10]\n",
    "    stations = [\"DL\"+str(x).zfill(3) for x in range(1,39)]\n",
    "    split_aqi = {}\n",
    "    for i in range(len(stations)):\n",
    "        split_aqi[stations[i]] = (aqi[aqi['StationId'] == stations[i]])\n",
    "    return met,aqi,split_aqi\n",
    "met,aqi,split_aqi = loadcsv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged Dataset Size 44035\n",
      "Size before roll 44035\n",
      "Size after roll 14861\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PM2.5</th>\n",
       "      <th>PM10</th>\n",
       "      <th>NO</th>\n",
       "      <th>NO2</th>\n",
       "      <th>NOx</th>\n",
       "      <th>NH3</th>\n",
       "      <th>CO</th>\n",
       "      <th>SO2</th>\n",
       "      <th>O3</th>\n",
       "      <th>AQI</th>\n",
       "      <th>...</th>\n",
       "      <th>O3_t-19</th>\n",
       "      <th>O3_t+19</th>\n",
       "      <th>O3_t-20</th>\n",
       "      <th>O3_t+20</th>\n",
       "      <th>O3_t-21</th>\n",
       "      <th>O3_t+21</th>\n",
       "      <th>O3_t-22</th>\n",
       "      <th>O3_t+22</th>\n",
       "      <th>O3_t-23</th>\n",
       "      <th>O3_t+23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>33.00</td>\n",
       "      <td>102.17</td>\n",
       "      <td>6.33</td>\n",
       "      <td>14.45</td>\n",
       "      <td>22.22</td>\n",
       "      <td>23.65</td>\n",
       "      <td>0.17</td>\n",
       "      <td>10.27</td>\n",
       "      <td>18.08</td>\n",
       "      <td>429.0</td>\n",
       "      <td>...</td>\n",
       "      <td>26.83</td>\n",
       "      <td>39.17</td>\n",
       "      <td>19.83</td>\n",
       "      <td>34.33</td>\n",
       "      <td>18.75</td>\n",
       "      <td>32.58</td>\n",
       "      <td>19.58</td>\n",
       "      <td>32.58</td>\n",
       "      <td>21.33</td>\n",
       "      <td>34.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>95.17</td>\n",
       "      <td>184.83</td>\n",
       "      <td>6.72</td>\n",
       "      <td>15.63</td>\n",
       "      <td>23.86</td>\n",
       "      <td>22.06</td>\n",
       "      <td>20.00</td>\n",
       "      <td>11.97</td>\n",
       "      <td>18.58</td>\n",
       "      <td>429.0</td>\n",
       "      <td>...</td>\n",
       "      <td>35.33</td>\n",
       "      <td>34.33</td>\n",
       "      <td>26.83</td>\n",
       "      <td>32.58</td>\n",
       "      <td>19.83</td>\n",
       "      <td>32.58</td>\n",
       "      <td>18.75</td>\n",
       "      <td>34.42</td>\n",
       "      <td>19.58</td>\n",
       "      <td>34.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>77.50</td>\n",
       "      <td>164.67</td>\n",
       "      <td>8.15</td>\n",
       "      <td>32.28</td>\n",
       "      <td>41.18</td>\n",
       "      <td>26.91</td>\n",
       "      <td>8.00</td>\n",
       "      <td>17.28</td>\n",
       "      <td>20.33</td>\n",
       "      <td>318.0</td>\n",
       "      <td>...</td>\n",
       "      <td>43.58</td>\n",
       "      <td>32.58</td>\n",
       "      <td>35.33</td>\n",
       "      <td>32.58</td>\n",
       "      <td>26.83</td>\n",
       "      <td>34.42</td>\n",
       "      <td>19.83</td>\n",
       "      <td>34.75</td>\n",
       "      <td>18.75</td>\n",
       "      <td>38.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>65.50</td>\n",
       "      <td>154.33</td>\n",
       "      <td>7.64</td>\n",
       "      <td>90.45</td>\n",
       "      <td>94.00</td>\n",
       "      <td>30.74</td>\n",
       "      <td>17.00</td>\n",
       "      <td>18.81</td>\n",
       "      <td>29.83</td>\n",
       "      <td>318.0</td>\n",
       "      <td>...</td>\n",
       "      <td>52.17</td>\n",
       "      <td>32.58</td>\n",
       "      <td>43.58</td>\n",
       "      <td>34.42</td>\n",
       "      <td>35.33</td>\n",
       "      <td>34.75</td>\n",
       "      <td>26.83</td>\n",
       "      <td>38.33</td>\n",
       "      <td>19.83</td>\n",
       "      <td>46.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>60.50</td>\n",
       "      <td>310.17</td>\n",
       "      <td>14.19</td>\n",
       "      <td>116.38</td>\n",
       "      <td>121.56</td>\n",
       "      <td>28.73</td>\n",
       "      <td>15.00</td>\n",
       "      <td>15.16</td>\n",
       "      <td>33.33</td>\n",
       "      <td>318.0</td>\n",
       "      <td>...</td>\n",
       "      <td>59.83</td>\n",
       "      <td>34.75</td>\n",
       "      <td>56.42</td>\n",
       "      <td>38.33</td>\n",
       "      <td>52.17</td>\n",
       "      <td>46.50</td>\n",
       "      <td>43.58</td>\n",
       "      <td>55.92</td>\n",
       "      <td>35.33</td>\n",
       "      <td>56.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43959</th>\n",
       "      <td>45.00</td>\n",
       "      <td>173.00</td>\n",
       "      <td>1.97</td>\n",
       "      <td>10.47</td>\n",
       "      <td>7.20</td>\n",
       "      <td>26.23</td>\n",
       "      <td>0.60</td>\n",
       "      <td>14.25</td>\n",
       "      <td>8.98</td>\n",
       "      <td>96.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.30</td>\n",
       "      <td>8.80</td>\n",
       "      <td>6.05</td>\n",
       "      <td>11.10</td>\n",
       "      <td>11.80</td>\n",
       "      <td>12.57</td>\n",
       "      <td>10.88</td>\n",
       "      <td>12.40</td>\n",
       "      <td>10.25</td>\n",
       "      <td>14.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43960</th>\n",
       "      <td>66.00</td>\n",
       "      <td>250.00</td>\n",
       "      <td>2.55</td>\n",
       "      <td>19.05</td>\n",
       "      <td>12.20</td>\n",
       "      <td>23.60</td>\n",
       "      <td>0.65</td>\n",
       "      <td>14.43</td>\n",
       "      <td>7.52</td>\n",
       "      <td>102.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.30</td>\n",
       "      <td>11.10</td>\n",
       "      <td>7.30</td>\n",
       "      <td>12.57</td>\n",
       "      <td>6.05</td>\n",
       "      <td>12.40</td>\n",
       "      <td>11.80</td>\n",
       "      <td>14.65</td>\n",
       "      <td>10.88</td>\n",
       "      <td>11.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43961</th>\n",
       "      <td>56.00</td>\n",
       "      <td>183.00</td>\n",
       "      <td>3.25</td>\n",
       "      <td>28.25</td>\n",
       "      <td>17.70</td>\n",
       "      <td>28.57</td>\n",
       "      <td>0.80</td>\n",
       "      <td>18.93</td>\n",
       "      <td>10.67</td>\n",
       "      <td>105.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.10</td>\n",
       "      <td>12.57</td>\n",
       "      <td>7.30</td>\n",
       "      <td>12.40</td>\n",
       "      <td>7.30</td>\n",
       "      <td>14.65</td>\n",
       "      <td>6.05</td>\n",
       "      <td>11.53</td>\n",
       "      <td>11.80</td>\n",
       "      <td>10.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43962</th>\n",
       "      <td>63.00</td>\n",
       "      <td>190.00</td>\n",
       "      <td>4.03</td>\n",
       "      <td>44.83</td>\n",
       "      <td>27.15</td>\n",
       "      <td>38.50</td>\n",
       "      <td>0.82</td>\n",
       "      <td>18.10</td>\n",
       "      <td>7.38</td>\n",
       "      <td>108.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.43</td>\n",
       "      <td>12.40</td>\n",
       "      <td>8.10</td>\n",
       "      <td>14.65</td>\n",
       "      <td>7.30</td>\n",
       "      <td>11.53</td>\n",
       "      <td>7.30</td>\n",
       "      <td>10.25</td>\n",
       "      <td>6.05</td>\n",
       "      <td>10.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43963</th>\n",
       "      <td>55.00</td>\n",
       "      <td>177.00</td>\n",
       "      <td>4.90</td>\n",
       "      <td>52.40</td>\n",
       "      <td>31.88</td>\n",
       "      <td>57.32</td>\n",
       "      <td>1.17</td>\n",
       "      <td>20.85</td>\n",
       "      <td>8.20</td>\n",
       "      <td>110.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.85</td>\n",
       "      <td>14.65</td>\n",
       "      <td>9.43</td>\n",
       "      <td>11.53</td>\n",
       "      <td>8.10</td>\n",
       "      <td>10.25</td>\n",
       "      <td>7.30</td>\n",
       "      <td>10.05</td>\n",
       "      <td>7.30</td>\n",
       "      <td>10.57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14861 rows Ã— 497 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       PM2.5    PM10     NO     NO2     NOx    NH3     CO    SO2     O3  \\\n",
       "165    33.00  102.17   6.33   14.45   22.22  23.65   0.17  10.27  18.08   \n",
       "166    95.17  184.83   6.72   15.63   23.86  22.06  20.00  11.97  18.58   \n",
       "167    77.50  164.67   8.15   32.28   41.18  26.91   8.00  17.28  20.33   \n",
       "168    65.50  154.33   7.64   90.45   94.00  30.74  17.00  18.81  29.83   \n",
       "170    60.50  310.17  14.19  116.38  121.56  28.73  15.00  15.16  33.33   \n",
       "...      ...     ...    ...     ...     ...    ...    ...    ...    ...   \n",
       "43959  45.00  173.00   1.97   10.47    7.20  26.23   0.60  14.25   8.98   \n",
       "43960  66.00  250.00   2.55   19.05   12.20  23.60   0.65  14.43   7.52   \n",
       "43961  56.00  183.00   3.25   28.25   17.70  28.57   0.80  18.93  10.67   \n",
       "43962  63.00  190.00   4.03   44.83   27.15  38.50   0.82  18.10   7.38   \n",
       "43963  55.00  177.00   4.90   52.40   31.88  57.32   1.17  20.85   8.20   \n",
       "\n",
       "         AQI  ...  O3_t-19  O3_t+19  O3_t-20  O3_t+20  O3_t-21  O3_t+21  \\\n",
       "165    429.0  ...    26.83    39.17    19.83    34.33    18.75    32.58   \n",
       "166    429.0  ...    35.33    34.33    26.83    32.58    19.83    32.58   \n",
       "167    318.0  ...    43.58    32.58    35.33    32.58    26.83    34.42   \n",
       "168    318.0  ...    52.17    32.58    43.58    34.42    35.33    34.75   \n",
       "170    318.0  ...    59.83    34.75    56.42    38.33    52.17    46.50   \n",
       "...      ...  ...      ...      ...      ...      ...      ...      ...   \n",
       "43959   96.0  ...     7.30     8.80     6.05    11.10    11.80    12.57   \n",
       "43960  102.0  ...     7.30    11.10     7.30    12.57     6.05    12.40   \n",
       "43961  105.0  ...     8.10    12.57     7.30    12.40     7.30    14.65   \n",
       "43962  108.0  ...     9.43    12.40     8.10    14.65     7.30    11.53   \n",
       "43963  110.0  ...     7.85    14.65     9.43    11.53     8.10    10.25   \n",
       "\n",
       "       O3_t-22  O3_t+22 O3_t-23  O3_t+23  \n",
       "165      19.58    32.58   21.33    34.42  \n",
       "166      18.75    34.42   19.58    34.75  \n",
       "167      19.83    34.75   18.75    38.33  \n",
       "168      26.83    38.33   19.83    46.50  \n",
       "170      43.58    55.92   35.33    56.92  \n",
       "...        ...      ...     ...      ...  \n",
       "43959    10.88    12.40   10.25    14.65  \n",
       "43960    11.80    14.65   10.88    11.53  \n",
       "43961     6.05    11.53   11.80    10.25  \n",
       "43962     7.30    10.25    6.05    10.05  \n",
       "43963     7.30    10.05    7.30    10.57  \n",
       "\n",
       "[14861 rows x 497 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pre - processing and loading data\n",
    "class dataset:\n",
    "    def __init__(self,met,aqi,split_aqi):\n",
    "            self.metro_data = met\n",
    "            self.aqi_data = aqi\n",
    "            self.split_aqi = split_aqi\n",
    "    def mergedData(self,station,rlist=['PM2.5','PM10','NO','NO2','NOx','NH3','CO','SO2','O3'],roll=48,shift=48):\n",
    "        df_aqi = self.getdf(station)\n",
    "        df = pd.merge(df_aqi, self.metro_data, how='inner', on=['Date', 'Time'])\n",
    "        print(\"Merged Dataset Size\",len(df))\n",
    "        \n",
    "        #Pre Processing merged Data\n",
    "        df['Year'] = df['Date'].str[0:4]\n",
    "        df['Month'] = df['Date'].str[5:7].astype(np.float64)\n",
    "        df['Day'] = df['Date'].str[8:10].astype(np.float64)\n",
    "        df['Hour'] = df['Time'].str[0:2]\n",
    "        \n",
    "        # TRIG TRANSFORMATIONS\n",
    "        df['windX'] = np.cos(np.deg2rad(df['Wind direction'])) * df['Wind speed']\n",
    "        df['windY'] = np.sin(np.deg2rad(df['Wind direction'])) * df['Wind speed']\n",
    "        df['hourX'] = np.cos((df['Hour'].astype(np.float64)-1)*np.pi/24)\n",
    "        df['hourY'] = np.sin((df['Hour'].astype(np.float64)-1)*np.pi/24)\n",
    "        df['MonthX'] = np.cos((df['Month'].astype(np.float64)-1)*np.pi/12)\n",
    "        df['MonthY'] = np.sin((df['Month'].astype(np.float64)-1)*np.pi/12)\n",
    "        \n",
    "        import datetime\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "        df['isWeekend'] =  (df['Date'].dt.dayofweek>=5).astype(int)\n",
    "        \n",
    "        df.interpolate(method='linear', limit=5,inplace=True)\n",
    "        \n",
    "        # Drop Additional columns\n",
    "        df.drop('Benzene', axis=1, inplace=True)\n",
    "        df.drop('Toluene',axis=1, inplace=True)\n",
    "        df.drop('Xylene', axis=1,inplace=True)\n",
    "        df.drop('AQI_Bucket',axis=1,inplace=True)\n",
    "        df.drop('Datetime',axis=1,inplace=True)\n",
    "        df.drop('StationId',axis=1,inplace=True)\n",
    "        df.drop('Short-wave irradiation',axis=1,inplace=True)\n",
    "        df.drop('Date',axis=1,inplace=True)\n",
    "        df.drop('Time',axis=1,inplace=True)\n",
    "        \n",
    "        # Rolling and shifting \n",
    "        print(\"Size before roll\",len(df))\n",
    "        for i in rlist:\n",
    "            df[i+'_lag1'] = df[i].shift(24)\n",
    "            df[i+'_lag2'] = df[i].shift(48)\n",
    "        for i in rlist:\n",
    "            df[i+\"_pred1\"] = df[i].shift(-24)\n",
    "            df[i+\"_pred2\"] = df[i].shift(-48)\n",
    "        for i in rlist:\n",
    "            for j in range(24):\n",
    "                df[i+\"_t-\"+str(j)] = df[i].shift(j)\n",
    "                df[i+\"_t+\"+str(j)] = df[i].shift(-j-shift)\n",
    "        df.dropna(inplace=True)\n",
    "        print(\"Size after roll\",len(df))\n",
    "        \n",
    "        return df.copy()\n",
    "    def getdf(self,station):\n",
    "        return self.split_aqi[station]\n",
    "    def plot(self,station):\n",
    "        df = self.getdf(station)\n",
    "    def stats(self):\n",
    "        pass\n",
    "dat = dataset(met,aqi,split_aqi)\n",
    "df = dat.mergedData('DL031')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4613403329888863\n",
      "60.96974796032171\n",
      "63.05849255749911\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif, f_regression , mutual_info_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "dfTrain = df[:]\n",
    "features = ['Temperature','Relative Humidity','windX','windY','Year','MonthX','MonthY','hourX','hourY','isWeekend']\n",
    "features.append(\"PM2.5_lag1\")\n",
    "features.append(\"PM2.5_lag2\")\n",
    "\n",
    "features.append(\"PM10_lag1\")\n",
    "features.append(\"PM10_lag2\")\n",
    "\n",
    "features.append(\"NO2_lag1\")\n",
    "features.append(\"NO2_lag2\")\n",
    "\n",
    "features.append(\"SO2_lag1\")\n",
    "features.append(\"SO2_lag2\")\n",
    "\n",
    "features.append(\"CO_lag1\")\n",
    "features.append(\"CO_lag2\")\n",
    "X = df[features]\n",
    "y = df['PM2.5_pred1']\n",
    "scaler = StandardScaler()\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "scaler.fit(Xtrain)\n",
    "\n",
    "reg = LinearRegression().fit(scaler.transform(Xtrain), ytrain)\n",
    "\n",
    "testPred = reg.predict(scaler.transform(Xtest))\n",
    "trainPred = reg.predict(scaler.transform(Xtrain))\n",
    "mse = np.mean((testPred - np.array(ytest))*(testPred - np.array(ytest)))\n",
    "print(reg.score(scaler.transform(Xtest), ytest))\n",
    "print(mean_squared_error(testPred, ytest,squared=False))\n",
    "print(mean_squared_error(trainPred, ytrain,squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5366752515515851\n",
      "56.54574363812652\n",
      "57.906774033167665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\confusement\\miniconda3\\envs\\mlc\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif, f_regression , mutual_info_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "dfTrain = df[:]\n",
    "features = ['Temperature','Relative Humidity','windX','windY','Year','MonthX','MonthY','hourX','hourY','isWeekend']\n",
    "features.append(\"PM2.5_lag1\")\n",
    "features.append(\"PM2.5_lag2\")\n",
    "\n",
    "features.append(\"PM10_lag1\")\n",
    "features.append(\"PM10_lag2\")\n",
    "\n",
    "features.append(\"NO2_lag1\")\n",
    "features.append(\"NO2_lag2\")\n",
    "\n",
    "features.append(\"SO2_lag1\")\n",
    "features.append(\"SO2_lag2\")\n",
    "\n",
    "features.append(\"CO_lag1\")\n",
    "features.append(\"CO_lag2\")\n",
    "X = df[features]\n",
    "y = df['PM2.5_pred1']\n",
    "scaler = StandardScaler()\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "scaler.fit(Xtrain)\n",
    "\n",
    "reg = MLPRegressor(random_state=1, max_iter=100).fit(scaler.transform(Xtrain), ytrain)\n",
    "\n",
    "testPred = reg.predict(scaler.transform(Xtest))\n",
    "trainPred = reg.predict(scaler.transform(Xtrain))\n",
    "mse = np.mean((testPred - np.array(ytest))*(testPred - np.array(ytest)))\n",
    "print(reg.score(scaler.transform(Xtest), ytest))\n",
    "print(mean_squared_error(testPred, ytest,squared=False))\n",
    "print(mean_squared_error(trainPred, ytrain,squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3778398185894405\n",
      "65.52518818123579\n",
      "66.79772066299263\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif, f_regression , mutual_info_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "dfTrain = df[:]\n",
    "features = ['Temperature','Relative Humidity','windX','windY','Year','MonthX','MonthY','hourX','hourY','isWeekend']\n",
    "features.append(\"PM2.5_lag1\")\n",
    "features.append(\"PM2.5_lag2\")\n",
    "\n",
    "features.append(\"PM10_lag1\")\n",
    "features.append(\"PM10_lag2\")\n",
    "\n",
    "features.append(\"NO2_lag1\")\n",
    "features.append(\"NO2_lag2\")\n",
    "\n",
    "features.append(\"SO2_lag1\")\n",
    "features.append(\"SO2_lag2\")\n",
    "\n",
    "features.append(\"CO_lag1\")\n",
    "features.append(\"CO_lag2\")\n",
    "X = df[features]\n",
    "y = df['PM2.5_pred1']\n",
    "scaler = StandardScaler()\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "scaler.fit(Xtrain)\n",
    "\n",
    "reg = SVR(C=1.0, epsilon=0.2).fit(scaler.transform(Xtrain), ytrain)\n",
    "\n",
    "testPred = reg.predict(scaler.transform(Xtest))\n",
    "trainPred = reg.predict(scaler.transform(Xtrain))\n",
    "mse = np.mean((testPred - np.array(ytest))*(testPred - np.array(ytest)))\n",
    "print(reg.score(scaler.transform(Xtest), ytest))\n",
    "print(mean_squared_error(testPred, ytest,squared=False))\n",
    "print(mean_squared_error(trainPred, ytrain,squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9956, 34)\n",
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_75 (Dense)             (None, 50)                1750      \n",
      "_________________________________________________________________\n",
      "dense_76 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_77 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_78 (Dense)             (None, 24)                1224      \n",
      "=================================================================\n",
      "Total params: 8,074\n",
      "Trainable params: 8,074\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 7964 samples, validate on 1992 samples\n",
      "Epoch 1/250\n",
      "7964/7964 [==============================] - 0s 55us/step - loss: 11689.5299 - mse: 11689.5254 - mae: 78.0468 - val_loss: 6017.2228 - val_mse: 6017.2217 - val_mae: 53.6609\n",
      "Epoch 2/250\n",
      "7964/7964 [==============================] - 0s 33us/step - loss: 5150.1894 - mse: 5150.1890 - mae: 49.5203 - val_loss: 4187.9051 - val_mse: 4187.9048 - val_mae: 45.5909\n",
      "Epoch 3/250\n",
      "7964/7964 [==============================] - 0s 33us/step - loss: 4213.2308 - mse: 4213.2310 - mae: 44.9976 - val_loss: 3867.9165 - val_mse: 3867.9165 - val_mae: 43.5446\n",
      "Epoch 4/250\n",
      "7964/7964 [==============================] - 0s 40us/step - loss: 4097.9524 - mse: 4097.9526 - mae: 44.3653 - val_loss: 3812.1343 - val_mse: 3812.1348 - val_mae: 43.0206\n",
      "Epoch 5/250\n",
      "7964/7964 [==============================] - 0s 40us/step - loss: 4053.2609 - mse: 4053.2615 - mae: 44.0705 - val_loss: 3781.5056 - val_mse: 3781.5054 - val_mae: 42.6249\n",
      "Epoch 6/250\n",
      "7964/7964 [==============================] - 0s 44us/step - loss: 4007.9895 - mse: 4007.9890 - mae: 43.7102 - val_loss: 3883.0429 - val_mse: 3883.0437 - val_mae: 43.9144\n",
      "Epoch 7/250\n",
      "7964/7964 [==============================] - 0s 42us/step - loss: 3966.8865 - mse: 3966.8875 - mae: 43.5179 - val_loss: 3724.9402 - val_mse: 3724.9407 - val_mae: 42.3424\n",
      "Epoch 8/250\n",
      "7964/7964 [==============================] - 0s 39us/step - loss: 3897.0304 - mse: 3897.0310 - mae: 42.9793 - val_loss: 3720.8028 - val_mse: 3720.8027 - val_mae: 43.0817\n",
      "Epoch 9/250\n",
      "7964/7964 [==============================] - 0s 35us/step - loss: 3821.4500 - mse: 3821.4509 - mae: 42.5073 - val_loss: 3694.6885 - val_mse: 3694.6880 - val_mae: 42.7578\n",
      "Epoch 10/250\n",
      "7964/7964 [==============================] - 0s 34us/step - loss: 3747.8391 - mse: 3747.8389 - mae: 41.9411 - val_loss: 3492.7709 - val_mse: 3492.7708 - val_mae: 40.8071\n",
      "Epoch 11/250\n",
      "7964/7964 [==============================] - 0s 34us/step - loss: 3704.8971 - mse: 3704.8975 - mae: 41.5261 - val_loss: 3485.9165 - val_mse: 3485.9167 - val_mae: 40.4858\n",
      "Epoch 12/250\n",
      "7964/7964 [==============================] - 0s 34us/step - loss: 3685.3404 - mse: 3685.3384 - mae: 41.3526 - val_loss: 3480.6337 - val_mse: 3480.6335 - val_mae: 40.3134\n",
      "Epoch 13/250\n",
      "7964/7964 [==============================] - 0s 34us/step - loss: 3650.2726 - mse: 3650.2727 - mae: 41.1273 - val_loss: 3453.7822 - val_mse: 3453.7822 - val_mae: 40.2093\n",
      "Epoch 14/250\n",
      "7964/7964 [==============================] - 0s 34us/step - loss: 3629.9585 - mse: 3629.9585 - mae: 40.9963 - val_loss: 3430.4636 - val_mse: 3430.4636 - val_mae: 40.2979\n",
      "Epoch 15/250\n",
      "7964/7964 [==============================] - 0s 34us/step - loss: 3610.0472 - mse: 3610.0479 - mae: 40.9426 - val_loss: 3444.5542 - val_mse: 3444.5540 - val_mae: 40.5533\n",
      "Epoch 16/250\n",
      "7964/7964 [==============================] - 0s 34us/step - loss: 3588.8991 - mse: 3588.8987 - mae: 40.7301 - val_loss: 3406.7327 - val_mse: 3406.7324 - val_mae: 39.7247\n",
      "Epoch 17/250\n",
      "7964/7964 [==============================] - 0s 33us/step - loss: 3574.8333 - mse: 3574.8330 - mae: 40.6457 - val_loss: 3427.5118 - val_mse: 3427.5112 - val_mae: 40.8220\n",
      "Epoch 18/250\n",
      "7964/7964 [==============================] - 0s 35us/step - loss: 3557.6745 - mse: 3557.6750 - mae: 40.5418 - val_loss: 3419.1284 - val_mse: 3419.1279 - val_mae: 40.4998\n",
      "Epoch 19/250\n",
      "7964/7964 [==============================] - 0s 34us/step - loss: 3524.7908 - mse: 3524.7908 - mae: 40.3809 - val_loss: 3367.8567 - val_mse: 3367.8564 - val_mae: 39.8908\n",
      "Epoch 20/250\n",
      "7964/7964 [==============================] - 0s 33us/step - loss: 3506.9813 - mse: 3506.9807 - mae: 40.2442 - val_loss: 3368.7224 - val_mse: 3368.7224 - val_mae: 40.1569\n",
      "Epoch 21/250\n",
      "7964/7964 [==============================] - 0s 41us/step - loss: 3484.0142 - mse: 3484.0149 - mae: 40.1859 - val_loss: 3348.0221 - val_mse: 3348.0220 - val_mae: 39.7669\n",
      "Epoch 22/250\n",
      "7964/7964 [==============================] - 0s 37us/step - loss: 3467.7142 - mse: 3467.7141 - mae: 40.0682 - val_loss: 3447.8179 - val_mse: 3447.8179 - val_mae: 40.9886\n",
      "Epoch 23/250\n",
      "7964/7964 [==============================] - 0s 36us/step - loss: 3448.0997 - mse: 3448.0999 - mae: 39.9708 - val_loss: 3335.4522 - val_mse: 3335.4526 - val_mae: 39.4581\n",
      "Epoch 24/250\n",
      "7964/7964 [==============================] - 0s 34us/step - loss: 3447.8805 - mse: 3447.8811 - mae: 39.9469 - val_loss: 3310.5777 - val_mse: 3310.5781 - val_mae: 39.0960\n",
      "Epoch 25/250\n",
      "7964/7964 [==============================] - 0s 36us/step - loss: 3443.1319 - mse: 3443.1323 - mae: 39.9112 - val_loss: 3387.3693 - val_mse: 3387.3696 - val_mae: 40.1583\n",
      "Epoch 26/250\n",
      "7964/7964 [==============================] - 0s 38us/step - loss: 3406.0896 - mse: 3406.0889 - mae: 39.6653 - val_loss: 3320.5581 - val_mse: 3320.5576 - val_mae: 39.7268\n",
      "Epoch 27/250\n",
      "7964/7964 [==============================] - 0s 36us/step - loss: 3402.3402 - mse: 3402.3401 - mae: 39.7523 - val_loss: 3289.9574 - val_mse: 3289.9573 - val_mae: 39.1921\n",
      "Epoch 28/250\n",
      "7964/7964 [==============================] - 0s 40us/step - loss: 3378.8265 - mse: 3378.8254 - mae: 39.5357 - val_loss: 3311.7360 - val_mse: 3311.7361 - val_mae: 40.1165\n",
      "Epoch 29/250\n",
      "7964/7964 [==============================] - 0s 41us/step - loss: 3358.4165 - mse: 3358.4165 - mae: 39.4847 - val_loss: 3322.8938 - val_mse: 3322.8938 - val_mae: 39.6885\n",
      "Epoch 30/250\n",
      "7964/7964 [==============================] - 0s 35us/step - loss: 3373.2925 - mse: 3373.2942 - mae: 39.5107 - val_loss: 3257.5967 - val_mse: 3257.5967 - val_mae: 38.9918\n",
      "Epoch 31/250\n",
      "7964/7964 [==============================] - 0s 35us/step - loss: 3349.7813 - mse: 3349.7808 - mae: 39.4204 - val_loss: 3296.3040 - val_mse: 3296.3035 - val_mae: 39.5501\n",
      "Epoch 32/250\n",
      "7964/7964 [==============================] - 0s 38us/step - loss: 3317.6516 - mse: 3317.6531 - mae: 39.1853 - val_loss: 3228.2693 - val_mse: 3228.2695 - val_mae: 39.0060\n",
      "Epoch 33/250\n",
      "7964/7964 [==============================] - 0s 39us/step - loss: 3309.9827 - mse: 3309.9822 - mae: 39.1616 - val_loss: 3260.6033 - val_mse: 3260.6035 - val_mae: 39.3480\n",
      "Epoch 34/250\n",
      "7964/7964 [==============================] - 0s 38us/step - loss: 3300.2541 - mse: 3300.2539 - mae: 39.1446 - val_loss: 3256.8036 - val_mse: 3256.8037 - val_mae: 39.1593\n",
      "Epoch 35/250\n",
      "7964/7964 [==============================] - 0s 35us/step - loss: 3284.8808 - mse: 3284.8809 - mae: 39.0064 - val_loss: 3206.6224 - val_mse: 3206.6226 - val_mae: 38.9448\n",
      "Epoch 36/250\n",
      "7964/7964 [==============================] - 0s 39us/step - loss: 3257.9949 - mse: 3257.9944 - mae: 38.8921 - val_loss: 3214.1266 - val_mse: 3214.1267 - val_mae: 38.9853\n",
      "Epoch 37/250\n",
      "7964/7964 [==============================] - 0s 35us/step - loss: 3251.2638 - mse: 3251.2642 - mae: 38.8567 - val_loss: 3191.9186 - val_mse: 3191.9185 - val_mae: 38.7954\n",
      "Epoch 38/250\n",
      "7964/7964 [==============================] - 0s 37us/step - loss: 3240.9877 - mse: 3240.9883 - mae: 38.8124 - val_loss: 3190.1043 - val_mse: 3190.1047 - val_mae: 38.5064\n",
      "Epoch 39/250\n",
      "7964/7964 [==============================] - 0s 53us/step - loss: 3222.5099 - mse: 3222.5105 - mae: 38.7158 - val_loss: 3273.7502 - val_mse: 3273.7507 - val_mae: 38.3236\n",
      "Epoch 40/250\n",
      "7964/7964 [==============================] - 0s 34us/step - loss: 3234.1150 - mse: 3234.1138 - mae: 38.7422 - val_loss: 3235.4264 - val_mse: 3235.4263 - val_mae: 39.2669\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/250\n",
      "7964/7964 [==============================] - 0s 35us/step - loss: 3193.0123 - mse: 3193.0122 - mae: 38.5410 - val_loss: 3183.5403 - val_mse: 3183.5408 - val_mae: 38.8331\n",
      "Epoch 42/250\n",
      "7964/7964 [==============================] - 0s 33us/step - loss: 3204.2456 - mse: 3204.2451 - mae: 38.6335 - val_loss: 3206.7920 - val_mse: 3206.7920 - val_mae: 39.1455\n",
      "Epoch 43/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 3202.4999 - mse: 3202.5000 - mae: 38.5341 - val_loss: 3207.9672 - val_mse: 3207.9670 - val_mae: 39.1569\n",
      "Epoch 44/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 3160.6704 - mse: 3160.6709 - mae: 38.3299 - val_loss: 3205.5682 - val_mse: 3205.5684 - val_mae: 39.2420\n",
      "Epoch 45/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 3151.5604 - mse: 3151.5598 - mae: 38.3125 - val_loss: 3167.6525 - val_mse: 3167.6526 - val_mae: 38.9445\n",
      "Epoch 46/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 3143.5979 - mse: 3143.5979 - mae: 38.2371 - val_loss: 3152.1140 - val_mse: 3152.1140 - val_mae: 38.4176\n",
      "Epoch 47/250\n",
      "7964/7964 [==============================] - 0s 31us/step - loss: 3149.6623 - mse: 3149.6616 - mae: 38.2811 - val_loss: 3241.7891 - val_mse: 3241.7888 - val_mae: 39.7268\n",
      "Epoch 48/250\n",
      "7964/7964 [==============================] - 0s 41us/step - loss: 3143.3025 - mse: 3143.3030 - mae: 38.2322 - val_loss: 3171.5111 - val_mse: 3171.5105 - val_mae: 38.9731\n",
      "Epoch 49/250\n",
      "7964/7964 [==============================] - 0s 40us/step - loss: 3112.5117 - mse: 3112.5115 - mae: 38.0999 - val_loss: 3129.7453 - val_mse: 3129.7451 - val_mae: 38.4538\n",
      "Epoch 50/250\n",
      "7964/7964 [==============================] - 0s 47us/step - loss: 3118.5230 - mse: 3118.5227 - mae: 38.1558 - val_loss: 3147.8093 - val_mse: 3147.8096 - val_mae: 39.0074\n",
      "Epoch 51/250\n",
      "7964/7964 [==============================] - 0s 43us/step - loss: 3100.3746 - mse: 3100.3752 - mae: 38.0257 - val_loss: 3204.7577 - val_mse: 3204.7581 - val_mae: 39.3578\n",
      "Epoch 52/250\n",
      "7964/7964 [==============================] - 0s 42us/step - loss: 3083.8660 - mse: 3083.8660 - mae: 38.0054 - val_loss: 3124.4688 - val_mse: 3124.4685 - val_mae: 37.8680\n",
      "Epoch 53/250\n",
      "7964/7964 [==============================] - 0s 36us/step - loss: 3083.2135 - mse: 3083.2146 - mae: 37.9309 - val_loss: 3148.3485 - val_mse: 3148.3484 - val_mae: 38.6034\n",
      "Epoch 54/250\n",
      "7964/7964 [==============================] - 0s 41us/step - loss: 3079.8709 - mse: 3079.8706 - mae: 37.8845 - val_loss: 3123.7770 - val_mse: 3123.7766 - val_mae: 38.6414\n",
      "Epoch 55/250\n",
      "7964/7964 [==============================] - 0s 42us/step - loss: 3063.0247 - mse: 3063.0259 - mae: 37.8507 - val_loss: 3204.1265 - val_mse: 3204.1262 - val_mae: 39.6814\n",
      "Epoch 56/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 3046.8565 - mse: 3046.8567 - mae: 37.7324 - val_loss: 3117.2431 - val_mse: 3117.2429 - val_mae: 38.5613\n",
      "Epoch 57/250\n",
      "7964/7964 [==============================] - 0s 31us/step - loss: 3041.4757 - mse: 3041.4761 - mae: 37.6779 - val_loss: 3106.8900 - val_mse: 3106.8901 - val_mae: 38.5330\n",
      "Epoch 58/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 3028.2330 - mse: 3028.2329 - mae: 37.6224 - val_loss: 3093.8329 - val_mse: 3093.8325 - val_mae: 38.0767\n",
      "Epoch 59/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 3021.0465 - mse: 3021.0461 - mae: 37.5707 - val_loss: 3115.2001 - val_mse: 3115.1997 - val_mae: 38.2702\n",
      "Epoch 60/250\n",
      "7964/7964 [==============================] - 0s 31us/step - loss: 3010.8823 - mse: 3010.8811 - mae: 37.5696 - val_loss: 3102.8899 - val_mse: 3102.8901 - val_mae: 38.2017\n",
      "Epoch 61/250\n",
      "7964/7964 [==============================] - 0s 34us/step - loss: 3000.3121 - mse: 3000.3125 - mae: 37.4605 - val_loss: 3108.5382 - val_mse: 3108.5381 - val_mae: 38.0560\n",
      "Epoch 62/250\n",
      "7964/7964 [==============================] - 0s 31us/step - loss: 2996.7902 - mse: 2996.7905 - mae: 37.4468 - val_loss: 3099.5905 - val_mse: 3099.5901 - val_mae: 38.0694\n",
      "Epoch 63/250\n",
      "7964/7964 [==============================] - 0s 33us/step - loss: 2984.9096 - mse: 2984.9104 - mae: 37.4089 - val_loss: 3096.3496 - val_mse: 3096.3499 - val_mae: 38.3801\n",
      "Epoch 64/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2992.7267 - mse: 2992.7266 - mae: 37.4355 - val_loss: 3089.0647 - val_mse: 3089.0647 - val_mae: 38.1910\n",
      "Epoch 65/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2974.2817 - mse: 2974.2817 - mae: 37.2889 - val_loss: 3087.0547 - val_mse: 3087.0552 - val_mae: 37.8157\n",
      "Epoch 66/250\n",
      "7964/7964 [==============================] - 0s 33us/step - loss: 2965.0599 - mse: 2965.0598 - mae: 37.2604 - val_loss: 3090.3431 - val_mse: 3090.3433 - val_mae: 38.2257\n",
      "Epoch 67/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2951.6564 - mse: 2951.6575 - mae: 37.1838 - val_loss: 3069.2242 - val_mse: 3069.2241 - val_mae: 37.6375\n",
      "Epoch 68/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2924.0342 - mse: 2924.0342 - mae: 37.0269 - val_loss: 3074.0960 - val_mse: 3074.0964 - val_mae: 38.0994\n",
      "Epoch 69/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2927.2236 - mse: 2927.2239 - mae: 37.0263 - val_loss: 3052.7354 - val_mse: 3052.7354 - val_mae: 37.8898\n",
      "Epoch 70/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2908.1083 - mse: 2908.1077 - mae: 36.9443 - val_loss: 3061.5561 - val_mse: 3061.5557 - val_mae: 38.0395\n",
      "Epoch 71/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2906.6846 - mse: 2906.6851 - mae: 36.9331 - val_loss: 3065.8851 - val_mse: 3065.8855 - val_mae: 37.9024\n",
      "Epoch 72/250\n",
      "7964/7964 [==============================] - 0s 31us/step - loss: 2887.0373 - mse: 2887.0374 - mae: 36.8289 - val_loss: 3079.5908 - val_mse: 3079.5908 - val_mae: 38.3602\n",
      "Epoch 73/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2890.4660 - mse: 2890.4663 - mae: 36.7808 - val_loss: 3126.6970 - val_mse: 3126.6970 - val_mae: 38.9053\n",
      "Epoch 74/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2885.5178 - mse: 2885.5181 - mae: 36.7993 - val_loss: 3162.1036 - val_mse: 3162.1038 - val_mae: 39.0971\n",
      "Epoch 75/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2886.6994 - mse: 2886.6997 - mae: 36.8254 - val_loss: 3056.5733 - val_mse: 3056.5730 - val_mae: 38.1833\n",
      "Epoch 76/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2869.8284 - mse: 2869.8281 - mae: 36.7110 - val_loss: 3105.9644 - val_mse: 3105.9641 - val_mae: 38.7269\n",
      "Epoch 77/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2857.9741 - mse: 2857.9734 - mae: 36.6353 - val_loss: 3027.6889 - val_mse: 3027.6890 - val_mae: 37.5366\n",
      "Epoch 78/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2849.1487 - mse: 2849.1479 - mae: 36.6055 - val_loss: 3092.4594 - val_mse: 3092.4595 - val_mae: 38.3323\n",
      "Epoch 79/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2847.4683 - mse: 2847.4683 - mae: 36.5638 - val_loss: 3061.3246 - val_mse: 3061.3245 - val_mae: 38.2317\n",
      "Epoch 80/250\n",
      "7964/7964 [==============================] - 0s 31us/step - loss: 2842.7612 - mse: 2842.7612 - mae: 36.5783 - val_loss: 3036.0041 - val_mse: 3036.0042 - val_mae: 37.5588\n",
      "Epoch 81/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2843.3711 - mse: 2843.3711 - mae: 36.5624 - val_loss: 3054.8973 - val_mse: 3054.8977 - val_mae: 37.7853\n",
      "Epoch 82/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2819.2841 - mse: 2819.2827 - mae: 36.3591 - val_loss: 3098.9898 - val_mse: 3098.9900 - val_mae: 38.6496\n",
      "Epoch 83/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2830.8685 - mse: 2830.8684 - mae: 36.4570 - val_loss: 3013.9409 - val_mse: 3013.9412 - val_mae: 37.9370\n",
      "Epoch 84/250\n",
      "7964/7964 [==============================] - 0s 31us/step - loss: 2817.4236 - mse: 2817.4238 - mae: 36.4300 - val_loss: 3100.9153 - val_mse: 3100.9148 - val_mae: 38.5290\n",
      "Epoch 85/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2811.0842 - mse: 2811.0837 - mae: 36.3440 - val_loss: 3038.4155 - val_mse: 3038.4158 - val_mae: 37.7330\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2802.1024 - mse: 2802.1018 - mae: 36.2788 - val_loss: 3023.6330 - val_mse: 3023.6328 - val_mae: 37.7649\n",
      "Epoch 87/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2804.6874 - mse: 2804.6875 - mae: 36.3140 - val_loss: 3053.6952 - val_mse: 3053.6951 - val_mae: 38.0602\n",
      "Epoch 88/250\n",
      "7964/7964 [==============================] - 0s 31us/step - loss: 2780.0640 - mse: 2780.0642 - mae: 36.1844 - val_loss: 3074.8422 - val_mse: 3074.8418 - val_mae: 38.2929\n",
      "Epoch 89/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2781.2297 - mse: 2781.2297 - mae: 36.1706 - val_loss: 3019.6268 - val_mse: 3019.6270 - val_mae: 37.6843\n",
      "Epoch 90/250\n",
      "7964/7964 [==============================] - 0s 31us/step - loss: 2776.5254 - mse: 2776.5254 - mae: 36.0943 - val_loss: 3002.3605 - val_mse: 3002.3604 - val_mae: 37.5959\n",
      "Epoch 91/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2771.0495 - mse: 2771.0496 - mae: 36.1469 - val_loss: 3020.9437 - val_mse: 3020.9441 - val_mae: 37.6119\n",
      "Epoch 92/250\n",
      "7964/7964 [==============================] - 0s 31us/step - loss: 2765.2573 - mse: 2765.2576 - mae: 36.0532 - val_loss: 3025.3122 - val_mse: 3025.3120 - val_mae: 37.5926\n",
      "Epoch 93/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2752.3577 - mse: 2752.3584 - mae: 35.9815 - val_loss: 2988.0409 - val_mse: 2988.0410 - val_mae: 37.3174\n",
      "Epoch 94/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2752.4408 - mse: 2752.4404 - mae: 36.0024 - val_loss: 3014.6508 - val_mse: 3014.6509 - val_mae: 37.5083\n",
      "Epoch 95/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2757.4106 - mse: 2757.4109 - mae: 36.0016 - val_loss: 3012.1575 - val_mse: 3012.1575 - val_mae: 37.5143\n",
      "Epoch 96/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2732.9754 - mse: 2732.9761 - mae: 35.8825 - val_loss: 3038.7447 - val_mse: 3038.7446 - val_mae: 37.8566\n",
      "Epoch 97/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2758.3985 - mse: 2758.3979 - mae: 36.0237 - val_loss: 3008.2566 - val_mse: 3008.2563 - val_mae: 37.1463\n",
      "Epoch 98/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2751.3028 - mse: 2751.3025 - mae: 35.9522 - val_loss: 2996.6126 - val_mse: 2996.6125 - val_mae: 37.0015\n",
      "Epoch 99/250\n",
      "7964/7964 [==============================] - 0s 31us/step - loss: 2724.8101 - mse: 2724.8101 - mae: 35.7833 - val_loss: 3024.7749 - val_mse: 3024.7749 - val_mae: 37.7751\n",
      "Epoch 100/250\n",
      "7964/7964 [==============================] - 0s 33us/step - loss: 2721.3530 - mse: 2721.3535 - mae: 35.7736 - val_loss: 2986.1857 - val_mse: 2986.1855 - val_mae: 37.1139\n",
      "Epoch 101/250\n",
      "7964/7964 [==============================] - 0s 31us/step - loss: 2709.3236 - mse: 2709.3240 - mae: 35.7183 - val_loss: 2955.4493 - val_mse: 2955.4487 - val_mae: 37.1661\n",
      "Epoch 102/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2700.4308 - mse: 2700.4299 - mae: 35.6606 - val_loss: 3012.6915 - val_mse: 3012.6917 - val_mae: 37.2871\n",
      "Epoch 103/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2716.3629 - mse: 2716.3625 - mae: 35.7466 - val_loss: 2975.6124 - val_mse: 2975.6123 - val_mae: 36.9745\n",
      "Epoch 104/250\n",
      "7964/7964 [==============================] - 0s 31us/step - loss: 2702.7072 - mse: 2702.7080 - mae: 35.6846 - val_loss: 2995.2641 - val_mse: 2995.2639 - val_mae: 37.2118\n",
      "Epoch 105/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2697.6832 - mse: 2697.6829 - mae: 35.6288 - val_loss: 3051.8604 - val_mse: 3051.8601 - val_mae: 38.3262\n",
      "Epoch 106/250\n",
      "7964/7964 [==============================] - 0s 31us/step - loss: 2699.3391 - mse: 2699.3394 - mae: 35.6638 - val_loss: 2982.2614 - val_mse: 2982.2615 - val_mae: 37.0781\n",
      "Epoch 107/250\n",
      "7964/7964 [==============================] - 0s 31us/step - loss: 2674.3154 - mse: 2674.3154 - mae: 35.5216 - val_loss: 2967.3299 - val_mse: 2967.3303 - val_mae: 37.2452\n",
      "Epoch 108/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2697.3146 - mse: 2697.3145 - mae: 35.6035 - val_loss: 2968.6483 - val_mse: 2968.6487 - val_mae: 37.2438\n",
      "Epoch 109/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2672.8457 - mse: 2672.8462 - mae: 35.4679 - val_loss: 3002.9220 - val_mse: 3002.9219 - val_mae: 37.5813\n",
      "Epoch 110/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2676.9822 - mse: 2676.9817 - mae: 35.5158 - val_loss: 2991.3434 - val_mse: 2991.3433 - val_mae: 37.6088\n",
      "Epoch 111/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2677.9761 - mse: 2677.9756 - mae: 35.5133 - val_loss: 2959.0088 - val_mse: 2959.0083 - val_mae: 37.0954\n",
      "Epoch 112/250\n",
      "7964/7964 [==============================] - 0s 31us/step - loss: 2681.3153 - mse: 2681.3147 - mae: 35.5475 - val_loss: 2982.7364 - val_mse: 2982.7361 - val_mae: 37.3966\n",
      "Epoch 113/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2650.0076 - mse: 2650.0073 - mae: 35.4069 - val_loss: 3034.2525 - val_mse: 3034.2522 - val_mae: 37.7559\n",
      "Epoch 114/250\n",
      "7964/7964 [==============================] - 0s 33us/step - loss: 2652.9553 - mse: 2652.9558 - mae: 35.3433 - val_loss: 3088.9098 - val_mse: 3088.9097 - val_mae: 37.3243\n",
      "Epoch 115/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2675.9489 - mse: 2675.9485 - mae: 35.4429 - val_loss: 3035.3920 - val_mse: 3035.3926 - val_mae: 38.1228\n",
      "Epoch 116/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2649.7369 - mse: 2649.7363 - mae: 35.3228 - val_loss: 3023.2845 - val_mse: 3023.2847 - val_mae: 37.7860\n",
      "Epoch 117/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2644.8975 - mse: 2644.8967 - mae: 35.3155 - val_loss: 3063.9209 - val_mse: 3063.9209 - val_mae: 38.0205\n",
      "Epoch 118/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2662.7117 - mse: 2662.7124 - mae: 35.4283 - val_loss: 2964.5909 - val_mse: 2964.5913 - val_mae: 36.9500\n",
      "Epoch 119/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2625.3788 - mse: 2625.3789 - mae: 35.1825 - val_loss: 3001.6398 - val_mse: 3001.6399 - val_mae: 37.5947\n",
      "Epoch 120/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2626.4880 - mse: 2626.4873 - mae: 35.2035 - val_loss: 2977.9121 - val_mse: 2977.9119 - val_mae: 36.7800\n",
      "Epoch 121/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2646.3465 - mse: 2646.3462 - mae: 35.3380 - val_loss: 2969.8210 - val_mse: 2969.8213 - val_mae: 37.1126\n",
      "Epoch 122/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2625.7162 - mse: 2625.7166 - mae: 35.1990 - val_loss: 2943.5688 - val_mse: 2943.5693 - val_mae: 37.1228\n",
      "Epoch 123/250\n",
      "7964/7964 [==============================] - 0s 31us/step - loss: 2613.6000 - mse: 2613.6003 - mae: 35.0951 - val_loss: 3043.8266 - val_mse: 3043.8267 - val_mae: 37.8745\n",
      "Epoch 124/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2607.7575 - mse: 2607.7576 - mae: 35.1137 - val_loss: 2951.6382 - val_mse: 2951.6382 - val_mae: 37.3270\n",
      "Epoch 125/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2615.5999 - mse: 2615.6001 - mae: 35.1598 - val_loss: 2983.0336 - val_mse: 2983.0330 - val_mae: 36.9231\n",
      "Epoch 126/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2600.5607 - mse: 2600.5601 - mae: 35.0431 - val_loss: 3024.1083 - val_mse: 3024.1079 - val_mae: 37.9317\n",
      "Epoch 127/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2592.1033 - mse: 2592.1028 - mae: 34.9638 - val_loss: 2970.5293 - val_mse: 2970.5291 - val_mae: 37.0292\n",
      "Epoch 128/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2598.7335 - mse: 2598.7339 - mae: 34.9864 - val_loss: 2956.2654 - val_mse: 2956.2659 - val_mae: 37.0390\n",
      "Epoch 129/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2572.8500 - mse: 2572.8506 - mae: 34.8255 - val_loss: 2967.5089 - val_mse: 2967.5090 - val_mae: 37.3030\n",
      "Epoch 130/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7964/7964 [==============================] - 0s 31us/step - loss: 2599.2695 - mse: 2599.2690 - mae: 35.0322 - val_loss: 3048.0569 - val_mse: 3048.0566 - val_mae: 37.7864\n",
      "Epoch 131/250\n",
      "7964/7964 [==============================] - 0s 31us/step - loss: 2587.2496 - mse: 2587.2490 - mae: 34.9358 - val_loss: 3143.0749 - val_mse: 3143.0754 - val_mae: 38.4698\n",
      "Epoch 132/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2609.4466 - mse: 2609.4468 - mae: 35.0584 - val_loss: 2972.0599 - val_mse: 2972.0598 - val_mae: 36.9486\n",
      "Epoch 133/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2588.5128 - mse: 2588.5127 - mae: 34.8934 - val_loss: 3004.2690 - val_mse: 3004.2688 - val_mae: 37.4213\n",
      "Epoch 134/250\n",
      "7964/7964 [==============================] - 0s 31us/step - loss: 2568.6234 - mse: 2568.6235 - mae: 34.8127 - val_loss: 2996.6247 - val_mse: 2996.6245 - val_mae: 37.4674\n",
      "Epoch 135/250\n",
      "7964/7964 [==============================] - 0s 31us/step - loss: 2572.1593 - mse: 2572.1594 - mae: 34.8340 - val_loss: 2956.8012 - val_mse: 2956.8013 - val_mae: 37.2733\n",
      "Epoch 136/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2564.5891 - mse: 2564.5886 - mae: 34.8045 - val_loss: 2963.4791 - val_mse: 2963.4795 - val_mae: 36.9970\n",
      "Epoch 137/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2549.4549 - mse: 2549.4543 - mae: 34.6832 - val_loss: 2950.9316 - val_mse: 2950.9312 - val_mae: 36.7980\n",
      "Epoch 138/250\n",
      "7964/7964 [==============================] - 0s 31us/step - loss: 2549.9939 - mse: 2549.9939 - mae: 34.7197 - val_loss: 2982.0710 - val_mse: 2982.0713 - val_mae: 37.5471\n",
      "Epoch 139/250\n",
      "7964/7964 [==============================] - 0s 31us/step - loss: 2552.1652 - mse: 2552.1658 - mae: 34.7598 - val_loss: 2967.7979 - val_mse: 2967.7983 - val_mae: 36.6029\n",
      "Epoch 140/250\n",
      "7964/7964 [==============================] - 0s 31us/step - loss: 2556.5130 - mse: 2556.5129 - mae: 34.7032 - val_loss: 2929.4612 - val_mse: 2929.4617 - val_mae: 36.8810\n",
      "Epoch 141/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2537.8565 - mse: 2537.8564 - mae: 34.6103 - val_loss: 2981.7302 - val_mse: 2981.7302 - val_mae: 37.6279\n",
      "Epoch 142/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2546.5636 - mse: 2546.5640 - mae: 34.6689 - val_loss: 2990.6506 - val_mse: 2990.6506 - val_mae: 37.6313\n",
      "Epoch 143/250\n",
      "7964/7964 [==============================] - 0s 31us/step - loss: 2523.2835 - mse: 2523.2842 - mae: 34.5652 - val_loss: 2943.4937 - val_mse: 2943.4937 - val_mae: 37.3196\n",
      "Epoch 144/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2512.8568 - mse: 2512.8569 - mae: 34.5289 - val_loss: 2961.0682 - val_mse: 2961.0686 - val_mae: 37.0748\n",
      "Epoch 145/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2514.2234 - mse: 2514.2234 - mae: 34.4908 - val_loss: 2975.7548 - val_mse: 2975.7549 - val_mae: 37.3474\n",
      "Epoch 146/250\n",
      "7964/7964 [==============================] - 0s 31us/step - loss: 2515.5175 - mse: 2515.5171 - mae: 34.5081 - val_loss: 2968.6377 - val_mse: 2968.6377 - val_mae: 37.3834\n",
      "Epoch 147/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2502.4615 - mse: 2502.4612 - mae: 34.4567 - val_loss: 2965.1107 - val_mse: 2965.1108 - val_mae: 37.5087\n",
      "Epoch 148/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2512.7216 - mse: 2512.7209 - mae: 34.4733 - val_loss: 3017.7625 - val_mse: 3017.7625 - val_mae: 37.7512\n",
      "Epoch 149/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2517.5031 - mse: 2517.5024 - mae: 34.4891 - val_loss: 2958.9351 - val_mse: 2958.9351 - val_mae: 37.3079\n",
      "Epoch 150/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2509.9914 - mse: 2509.9912 - mae: 34.4679 - val_loss: 2901.9133 - val_mse: 2901.9128 - val_mae: 36.5772\n",
      "Epoch 151/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2489.4686 - mse: 2489.4683 - mae: 34.3323 - val_loss: 2952.4575 - val_mse: 2952.4573 - val_mae: 36.9357\n",
      "Epoch 152/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2496.7852 - mse: 2496.7849 - mae: 34.3968 - val_loss: 2893.7834 - val_mse: 2893.7839 - val_mae: 36.3872\n",
      "Epoch 153/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2496.4847 - mse: 2496.4849 - mae: 34.3604 - val_loss: 3051.0160 - val_mse: 3051.0156 - val_mae: 38.0031\n",
      "Epoch 154/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2485.2969 - mse: 2485.2964 - mae: 34.2942 - val_loss: 2976.0914 - val_mse: 2976.0911 - val_mae: 37.4114\n",
      "Epoch 155/250\n",
      "7964/7964 [==============================] - 0s 31us/step - loss: 2498.3154 - mse: 2498.3167 - mae: 34.3918 - val_loss: 2902.0293 - val_mse: 2902.0293 - val_mae: 36.7347\n",
      "Epoch 156/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2482.0120 - mse: 2482.0122 - mae: 34.2697 - val_loss: 2955.6997 - val_mse: 2955.6997 - val_mae: 37.3924\n",
      "Epoch 157/250\n",
      "7964/7964 [==============================] - 0s 33us/step - loss: 2482.6268 - mse: 2482.6262 - mae: 34.2544 - val_loss: 2932.6559 - val_mse: 2932.6558 - val_mae: 37.0770\n",
      "Epoch 158/250\n",
      "7964/7964 [==============================] - 0s 31us/step - loss: 2460.8937 - mse: 2460.8936 - mae: 34.1590 - val_loss: 2946.1436 - val_mse: 2946.1431 - val_mae: 36.5141\n",
      "Epoch 159/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2465.4691 - mse: 2465.4690 - mae: 34.2092 - val_loss: 2922.9832 - val_mse: 2922.9832 - val_mae: 36.4790\n",
      "Epoch 160/250\n",
      "7964/7964 [==============================] - 0s 33us/step - loss: 2460.2492 - mse: 2460.2488 - mae: 34.1647 - val_loss: 3159.0215 - val_mse: 3159.0210 - val_mae: 38.9638\n",
      "Epoch 161/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2463.9532 - mse: 2463.9536 - mae: 34.1731 - val_loss: 2897.1738 - val_mse: 2897.1736 - val_mae: 36.5013\n",
      "Epoch 162/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2452.0459 - mse: 2452.0461 - mae: 34.0980 - val_loss: 2940.2232 - val_mse: 2940.2227 - val_mae: 37.2695\n",
      "Epoch 163/250\n",
      "7964/7964 [==============================] - 0s 31us/step - loss: 2447.5701 - mse: 2447.5703 - mae: 34.0621 - val_loss: 2895.1437 - val_mse: 2895.1436 - val_mae: 36.7371\n",
      "Epoch 164/250\n",
      "7964/7964 [==============================] - 0s 33us/step - loss: 2443.5263 - mse: 2443.5276 - mae: 34.0607 - val_loss: 2941.2747 - val_mse: 2941.2747 - val_mae: 37.0857\n",
      "Epoch 165/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2452.4347 - mse: 2452.4353 - mae: 34.0919 - val_loss: 2977.3474 - val_mse: 2977.3477 - val_mae: 37.6324\n",
      "Epoch 166/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2465.4545 - mse: 2465.4546 - mae: 34.1729 - val_loss: 2916.8214 - val_mse: 2916.8213 - val_mae: 36.5356\n",
      "Epoch 167/250\n",
      "7964/7964 [==============================] - 0s 31us/step - loss: 2436.2245 - mse: 2436.2246 - mae: 34.0028 - val_loss: 2886.3841 - val_mse: 2886.3843 - val_mae: 36.5784\n",
      "Epoch 168/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2421.9412 - mse: 2421.9414 - mae: 33.9089 - val_loss: 2904.4869 - val_mse: 2904.4868 - val_mae: 36.7605\n",
      "Epoch 169/250\n",
      "7964/7964 [==============================] - 0s 33us/step - loss: 2422.9932 - mse: 2422.9924 - mae: 33.9068 - val_loss: 2916.9536 - val_mse: 2916.9541 - val_mae: 36.7526\n",
      "Epoch 170/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2416.9336 - mse: 2416.9336 - mae: 33.9139 - val_loss: 2898.9699 - val_mse: 2898.9697 - val_mae: 36.9331\n",
      "Epoch 171/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2425.7391 - mse: 2425.7393 - mae: 33.9561 - val_loss: 2911.4605 - val_mse: 2911.4607 - val_mae: 36.7574\n",
      "Epoch 172/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2415.2978 - mse: 2415.2979 - mae: 33.9129 - val_loss: 2931.8232 - val_mse: 2931.8230 - val_mae: 37.1761\n",
      "Epoch 173/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2446.9439 - mse: 2446.9443 - mae: 33.9641 - val_loss: 2983.5552 - val_mse: 2983.5554 - val_mae: 37.2857\n",
      "Epoch 174/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7964/7964 [==============================] - 0s 32us/step - loss: 2412.7283 - mse: 2412.7278 - mae: 33.8855 - val_loss: 2885.1508 - val_mse: 2885.1506 - val_mae: 36.3558\n",
      "Epoch 175/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2401.0165 - mse: 2401.0171 - mae: 33.7733 - val_loss: 2892.0411 - val_mse: 2892.0410 - val_mae: 36.8877\n",
      "Epoch 176/250\n",
      "7964/7964 [==============================] - 0s 31us/step - loss: 2401.3200 - mse: 2401.3198 - mae: 33.7878 - val_loss: 2934.7626 - val_mse: 2934.7625 - val_mae: 37.2459\n",
      "Epoch 177/250\n",
      "7964/7964 [==============================] - 0s 31us/step - loss: 2401.1046 - mse: 2401.1052 - mae: 33.7940 - val_loss: 2905.0127 - val_mse: 2905.0129 - val_mae: 36.8907\n",
      "Epoch 178/250\n",
      "7964/7964 [==============================] - 0s 31us/step - loss: 2400.3080 - mse: 2400.3076 - mae: 33.7581 - val_loss: 2899.6171 - val_mse: 2899.6174 - val_mae: 36.4336\n",
      "Epoch 179/250\n",
      "7964/7964 [==============================] - 0s 31us/step - loss: 2382.9309 - mse: 2382.9312 - mae: 33.6837 - val_loss: 2929.0598 - val_mse: 2929.0598 - val_mae: 37.0159\n",
      "Epoch 180/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2379.9783 - mse: 2379.9778 - mae: 33.6282 - val_loss: 2939.7963 - val_mse: 2939.7966 - val_mae: 37.2060\n",
      "Epoch 181/250\n",
      "7964/7964 [==============================] - 0s 34us/step - loss: 2397.5556 - mse: 2397.5549 - mae: 33.7247 - val_loss: 2866.0844 - val_mse: 2866.0842 - val_mae: 36.5954\n",
      "Epoch 182/250\n",
      "7964/7964 [==============================] - 0s 31us/step - loss: 2383.1439 - mse: 2383.1443 - mae: 33.6636 - val_loss: 3070.8150 - val_mse: 3070.8147 - val_mae: 38.3171\n",
      "Epoch 183/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2377.2485 - mse: 2377.2483 - mae: 33.6819 - val_loss: 2877.5246 - val_mse: 2877.5249 - val_mae: 36.4731\n",
      "Epoch 184/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2389.3631 - mse: 2389.3633 - mae: 33.6934 - val_loss: 2910.2744 - val_mse: 2910.2749 - val_mae: 37.0177\n",
      "Epoch 185/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2377.5725 - mse: 2377.5728 - mae: 33.6469 - val_loss: 2893.0293 - val_mse: 2893.0291 - val_mae: 36.7913\n",
      "Epoch 186/250\n",
      "7964/7964 [==============================] - 0s 31us/step - loss: 2377.1677 - mse: 2377.1677 - mae: 33.5824 - val_loss: 2898.8937 - val_mse: 2898.8938 - val_mae: 36.7490\n",
      "Epoch 187/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2367.2115 - mse: 2367.2119 - mae: 33.5576 - val_loss: 2885.0225 - val_mse: 2885.0227 - val_mae: 36.4191\n",
      "Epoch 188/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2375.4286 - mse: 2375.4275 - mae: 33.5576 - val_loss: 2874.6977 - val_mse: 2874.6978 - val_mae: 36.5531\n",
      "Epoch 189/250\n",
      "7964/7964 [==============================] - 0s 31us/step - loss: 2359.3516 - mse: 2359.3518 - mae: 33.5153 - val_loss: 2914.1622 - val_mse: 2914.1619 - val_mae: 36.8300\n",
      "Epoch 190/250\n",
      "7964/7964 [==============================] - 0s 31us/step - loss: 2339.2207 - mse: 2339.2200 - mae: 33.3769 - val_loss: 2877.8575 - val_mse: 2877.8574 - val_mae: 36.4414\n",
      "Epoch 191/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2347.6533 - mse: 2347.6526 - mae: 33.4656 - val_loss: 2906.2939 - val_mse: 2906.2939 - val_mae: 36.2962\n",
      "Epoch 192/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2353.7996 - mse: 2353.7996 - mae: 33.4729 - val_loss: 2907.4845 - val_mse: 2907.4849 - val_mae: 36.5727\n",
      "Epoch 193/250\n",
      "7964/7964 [==============================] - 0s 31us/step - loss: 2359.4466 - mse: 2359.4473 - mae: 33.5109 - val_loss: 2864.6047 - val_mse: 2864.6047 - val_mae: 36.2775\n",
      "Epoch 194/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2339.2791 - mse: 2339.2788 - mae: 33.3364 - val_loss: 3002.6141 - val_mse: 3002.6138 - val_mae: 37.9149\n",
      "Epoch 195/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2345.2720 - mse: 2345.2712 - mae: 33.4261 - val_loss: 2971.3866 - val_mse: 2971.3865 - val_mae: 37.7250\n",
      "Epoch 196/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2338.6216 - mse: 2338.6216 - mae: 33.3890 - val_loss: 2874.9182 - val_mse: 2874.9182 - val_mae: 36.5171\n",
      "Epoch 197/250\n",
      "7964/7964 [==============================] - 0s 31us/step - loss: 2324.3462 - mse: 2324.3462 - mae: 33.2968 - val_loss: 2863.4341 - val_mse: 2863.4346 - val_mae: 36.5373\n",
      "Epoch 198/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2319.0579 - mse: 2319.0576 - mae: 33.2766 - val_loss: 2891.1940 - val_mse: 2891.1941 - val_mae: 36.9208\n",
      "Epoch 199/250\n",
      "7964/7964 [==============================] - 0s 33us/step - loss: 2316.5104 - mse: 2316.5100 - mae: 33.2295 - val_loss: 2902.2459 - val_mse: 2902.2461 - val_mae: 36.3274\n",
      "Epoch 200/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2322.6446 - mse: 2322.6445 - mae: 33.2685 - val_loss: 2912.0079 - val_mse: 2912.0076 - val_mae: 36.3012\n",
      "Epoch 201/250\n",
      "7964/7964 [==============================] - 0s 31us/step - loss: 2317.0647 - mse: 2317.0647 - mae: 33.2257 - val_loss: 2914.4202 - val_mse: 2914.4197 - val_mae: 37.1606\n",
      "Epoch 202/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2324.0242 - mse: 2324.0239 - mae: 33.3010 - val_loss: 2918.8576 - val_mse: 2918.8579 - val_mae: 36.9169\n",
      "Epoch 203/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2300.1848 - mse: 2300.1841 - mae: 33.1278 - val_loss: 2971.6142 - val_mse: 2971.6138 - val_mae: 36.4759\n",
      "Epoch 204/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2316.3441 - mse: 2316.3440 - mae: 33.2401 - val_loss: 2892.2132 - val_mse: 2892.2129 - val_mae: 36.8341\n",
      "Epoch 205/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2304.3700 - mse: 2304.3694 - mae: 33.1859 - val_loss: 2866.4797 - val_mse: 2866.4797 - val_mae: 36.5707\n",
      "Epoch 206/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2299.3599 - mse: 2299.3601 - mae: 33.1306 - val_loss: 2855.3695 - val_mse: 2855.3696 - val_mae: 36.3308\n",
      "Epoch 207/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2287.2452 - mse: 2287.2451 - mae: 33.0613 - val_loss: 2890.5023 - val_mse: 2890.5022 - val_mae: 36.5245\n",
      "Epoch 208/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2315.8742 - mse: 2315.8738 - mae: 33.2362 - val_loss: 2841.8619 - val_mse: 2841.8618 - val_mae: 35.9816\n",
      "Epoch 209/250\n",
      "7964/7964 [==============================] - 0s 31us/step - loss: 2291.0622 - mse: 2291.0627 - mae: 33.0690 - val_loss: 2870.9042 - val_mse: 2870.9043 - val_mae: 36.6871\n",
      "Epoch 210/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2276.9451 - mse: 2276.9451 - mae: 32.9936 - val_loss: 2860.6400 - val_mse: 2860.6401 - val_mae: 36.5054\n",
      "Epoch 211/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2300.8606 - mse: 2300.8608 - mae: 33.1411 - val_loss: 2894.7310 - val_mse: 2894.7312 - val_mae: 36.7581\n",
      "Epoch 212/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2290.0644 - mse: 2290.0642 - mae: 33.1030 - val_loss: 2851.9201 - val_mse: 2851.9204 - val_mae: 36.5568\n",
      "Epoch 213/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2273.5729 - mse: 2273.5737 - mae: 32.9753 - val_loss: 2858.1166 - val_mse: 2858.1167 - val_mae: 36.6753\n",
      "Epoch 214/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2270.8178 - mse: 2270.8176 - mae: 32.9765 - val_loss: 2839.2992 - val_mse: 2839.2988 - val_mae: 36.2525\n",
      "Epoch 215/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2265.3855 - mse: 2265.3850 - mae: 32.9386 - val_loss: 2882.5674 - val_mse: 2882.5674 - val_mae: 36.1899\n",
      "Epoch 216/250\n",
      "7964/7964 [==============================] - 0s 31us/step - loss: 2269.5339 - mse: 2269.5344 - mae: 32.9073 - val_loss: 2922.6527 - val_mse: 2922.6526 - val_mae: 37.2504\n",
      "Epoch 217/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2277.2054 - mse: 2277.2053 - mae: 33.0427 - val_loss: 2859.0210 - val_mse: 2859.0208 - val_mae: 36.6377\n",
      "Epoch 218/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7964/7964 [==============================] - 0s 32us/step - loss: 2243.8815 - mse: 2243.8818 - mae: 32.8114 - val_loss: 2854.3310 - val_mse: 2854.3306 - val_mae: 36.1810\n",
      "Epoch 219/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2273.6635 - mse: 2273.6636 - mae: 32.9723 - val_loss: 2865.2337 - val_mse: 2865.2334 - val_mae: 36.2652\n",
      "Epoch 220/250\n",
      "7964/7964 [==============================] - 0s 31us/step - loss: 2251.3741 - mse: 2251.3745 - mae: 32.8174 - val_loss: 2837.9257 - val_mse: 2837.9258 - val_mae: 36.5174\n",
      "Epoch 221/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2267.3068 - mse: 2267.3069 - mae: 32.9324 - val_loss: 2840.2546 - val_mse: 2840.2539 - val_mae: 36.2332\n",
      "Epoch 222/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2260.5784 - mse: 2260.5784 - mae: 32.8513 - val_loss: 2842.4092 - val_mse: 2842.4089 - val_mae: 36.3952\n",
      "Epoch 223/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2244.3292 - mse: 2244.3291 - mae: 32.7741 - val_loss: 2833.6945 - val_mse: 2833.6943 - val_mae: 36.1953\n",
      "Epoch 224/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2237.1673 - mse: 2237.1670 - mae: 32.7480 - val_loss: 2854.9001 - val_mse: 2854.9006 - val_mae: 36.6633\n",
      "Epoch 225/250\n",
      "7964/7964 [==============================] - 0s 31us/step - loss: 2239.4771 - mse: 2239.4766 - mae: 32.7350 - val_loss: 2851.8416 - val_mse: 2851.8413 - val_mae: 36.4524\n",
      "Epoch 226/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2216.6912 - mse: 2216.6909 - mae: 32.6068 - val_loss: 2843.5750 - val_mse: 2843.5747 - val_mae: 36.4979\n",
      "Epoch 227/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2242.0178 - mse: 2242.0178 - mae: 32.7822 - val_loss: 2860.7999 - val_mse: 2860.7993 - val_mae: 36.0330\n",
      "Epoch 228/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2235.9010 - mse: 2235.9004 - mae: 32.7001 - val_loss: 2838.8337 - val_mse: 2838.8333 - val_mae: 36.0816\n",
      "Epoch 229/250\n",
      "7964/7964 [==============================] - 0s 31us/step - loss: 2223.8534 - mse: 2223.8533 - mae: 32.6416 - val_loss: 2847.2415 - val_mse: 2847.2417 - val_mae: 36.2192\n",
      "Epoch 230/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2222.6087 - mse: 2222.6094 - mae: 32.6215 - val_loss: 2826.2547 - val_mse: 2826.2546 - val_mae: 36.0997\n",
      "Epoch 231/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2232.8569 - mse: 2232.8574 - mae: 32.7015 - val_loss: 2887.8690 - val_mse: 2887.8687 - val_mae: 36.6976\n",
      "Epoch 232/250\n",
      "7964/7964 [==============================] - 0s 31us/step - loss: 2222.2609 - mse: 2222.2610 - mae: 32.6205 - val_loss: 2812.8193 - val_mse: 2812.8193 - val_mae: 35.9723\n",
      "Epoch 233/250\n",
      "7964/7964 [==============================] - 0s 31us/step - loss: 2199.6274 - mse: 2199.6277 - mae: 32.4735 - val_loss: 2827.7886 - val_mse: 2827.7883 - val_mae: 35.9835\n",
      "Epoch 234/250\n",
      "7964/7964 [==============================] - 0s 31us/step - loss: 2225.0360 - mse: 2225.0361 - mae: 32.6594 - val_loss: 2857.9009 - val_mse: 2857.9006 - val_mae: 36.3904\n",
      "Epoch 235/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2221.0467 - mse: 2221.0461 - mae: 32.6193 - val_loss: 2838.8143 - val_mse: 2838.8142 - val_mae: 36.4657\n",
      "Epoch 236/250\n",
      "7964/7964 [==============================] - 0s 31us/step - loss: 2213.1848 - mse: 2213.1846 - mae: 32.5721 - val_loss: 2835.7882 - val_mse: 2835.7881 - val_mae: 36.0818\n",
      "Epoch 237/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2193.9615 - mse: 2193.9617 - mae: 32.4321 - val_loss: 2948.2621 - val_mse: 2948.2625 - val_mae: 37.1009\n",
      "Epoch 238/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2195.7314 - mse: 2195.7305 - mae: 32.4886 - val_loss: 2856.2627 - val_mse: 2856.2627 - val_mae: 35.8041\n",
      "Epoch 239/250\n",
      "7964/7964 [==============================] - 0s 33us/step - loss: 2197.7014 - mse: 2197.7019 - mae: 32.4394 - val_loss: 2822.6037 - val_mse: 2822.6040 - val_mae: 36.1910\n",
      "Epoch 240/250\n",
      "7964/7964 [==============================] - 0s 31us/step - loss: 2200.5445 - mse: 2200.5437 - mae: 32.5146 - val_loss: 2797.3732 - val_mse: 2797.3728 - val_mae: 35.8200\n",
      "Epoch 241/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2179.6467 - mse: 2179.6462 - mae: 32.3720 - val_loss: 2815.6866 - val_mse: 2815.6868 - val_mae: 36.0472\n",
      "Epoch 242/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2177.6815 - mse: 2177.6819 - mae: 32.3294 - val_loss: 2814.0163 - val_mse: 2814.0168 - val_mae: 35.9034\n",
      "Epoch 243/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2179.2826 - mse: 2179.2825 - mae: 32.3652 - val_loss: 2830.0140 - val_mse: 2830.0142 - val_mae: 35.9959\n",
      "Epoch 244/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2168.2891 - mse: 2168.2893 - mae: 32.3094 - val_loss: 2791.7597 - val_mse: 2791.7600 - val_mae: 36.0657\n",
      "Epoch 245/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2177.6680 - mse: 2177.6682 - mae: 32.3207 - val_loss: 2893.3680 - val_mse: 2893.3684 - val_mae: 36.7893\n",
      "Epoch 246/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2174.8696 - mse: 2174.8701 - mae: 32.3050 - val_loss: 2830.0985 - val_mse: 2830.0984 - val_mae: 36.2445\n",
      "Epoch 247/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2189.7649 - mse: 2189.7649 - mae: 32.4045 - val_loss: 2867.1978 - val_mse: 2867.1978 - val_mae: 36.3634\n",
      "Epoch 248/250\n",
      "7964/7964 [==============================] - 0s 31us/step - loss: 2175.5344 - mse: 2175.5344 - mae: 32.3073 - val_loss: 2843.2868 - val_mse: 2843.2866 - val_mae: 36.4479\n",
      "Epoch 249/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2156.6131 - mse: 2156.6128 - mae: 32.2188 - val_loss: 2811.5943 - val_mse: 2811.5935 - val_mae: 35.9310\n",
      "Epoch 250/250\n",
      "7964/7964 [==============================] - 0s 32us/step - loss: 2163.0574 - mse: 2163.0569 - mae: 32.2541 - val_loss: 2800.4410 - val_mse: 2800.4407 - val_mae: 36.2524\n",
      "52.40233121552558\n",
      "47.341863965140625\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "features = ['Temperature','Relative Humidity','windX','windY','Year','MonthX','MonthY','hourX','hourY','isWeekend']\n",
    "for j in range(24):\n",
    "    features.append('PM2.5_t-'+str(j))\n",
    "predVector = []\n",
    "for j in range(24):\n",
    "    predVector.append('PM2.5_t+'+str(j))\n",
    "X = df[features]\n",
    "y = df[predVector]\n",
    "scaler = StandardScaler()\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "scaler.fit(Xtrain)\n",
    "print(Xtrain.shape)\n",
    "with tf.device('/CPU:0'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(50, input_dim=34, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(24, activation='linear'))\n",
    "    model.summary()\n",
    "    #Fit\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['mse','mae'])\n",
    "    history = model.fit(scaler.transform(Xtrain), ytrain, epochs=250, batch_size=50,  verbose=1, validation_split=0.2)\n",
    "    #Print Accuracy\n",
    "    testPred = model.predict(scaler.transform(Xtest))\n",
    "    trainPred = model.predict(scaler.transform(Xtrain))\n",
    "    print(mean_squared_error(testPred, ytest,squared=False))\n",
    "    print(mean_squared_error(trainPred, ytrain,squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'val_mse', 'val_mae', 'loss', 'mse', 'mae'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
