{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:CPU:0\n"
     ]
    }
   ],
   "source": [
    "RKP = \"DL031\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "sns.set_theme(style=\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\confusement\\miniconda3\\envs\\mlc\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3263: DtypeWarning: Columns (15) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['StationId', 'Datetime', 'PM2.5', 'PM10', 'NO', 'NO2', 'NOx', 'NH3',\n",
      "       'CO', 'SO2', 'O3', 'Benzene', 'Toluene', 'Xylene', 'AQI', 'AQI_Bucket'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Load datasets and rename columns, load all aqi data but specify metro data name\n",
    "def loadcsv(city=\"./data/rkpuram.csv\"):\n",
    "    met = pd.read_csv(city,delimiter=';',skiprows=24)\n",
    "    aqi = pd.read_csv('./data/station_hour.csv')\n",
    "    print(aqi.columns)\n",
    "    met.rename(columns={'# Date': 'Date',}, inplace=True)\n",
    "    met.rename(columns={'UT time': 'Time',}, inplace=True)\n",
    "    aqi['Time'] = aqi['Datetime'].str[-8:-3]\n",
    "    aqi['Date'] = aqi['Datetime'].str[0:10]\n",
    "    stations = [\"DL\"+str(x).zfill(3) for x in range(1,39)]\n",
    "    split_aqi = {}\n",
    "    for i in range(len(stations)):\n",
    "        split_aqi[stations[i]] = (aqi[aqi['StationId'] == stations[i]])\n",
    "    return met,aqi,split_aqi\n",
    "met,aqi,split_aqi = loadcsv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged Dataset Size 44035\n",
      "Size before roll 44035\n",
      "Size after roll 31275\n"
     ]
    }
   ],
   "source": [
    "# Pre - processing and loading data\n",
    "class dataset:\n",
    "    def __init__(self,met,aqi,split_aqi):\n",
    "            self.metro_data = met\n",
    "            self.aqi_data = aqi\n",
    "            self.split_aqi = split_aqi\n",
    "    def mergedData(self,station,rlist=['PM2.5','PM10','NO','NO2','NOx','NH3','CO','SO2','O3'],roll=48,shift=48):\n",
    "        df_aqi = self.getdf(station)\n",
    "        df = pd.merge(df_aqi, self.metro_data, how='inner', on=['Date', 'Time'])\n",
    "        print(\"Merged Dataset Size\",len(df))\n",
    "        \n",
    "        #Pre Processing merged Data\n",
    "        df['Year'] = df['Date'].str[0:4]\n",
    "        df['Month'] = df['Date'].str[5:7].astype(np.float64)\n",
    "        df['Day'] = df['Date'].str[8:10].astype(np.float64)\n",
    "        df['Hour'] = df['Time'].str[0:2]\n",
    "        \n",
    "        # TRIG TRANSFORMATIONS\n",
    "        df['windX'] = np.cos(np.deg2rad(df['Wind direction'])) * df['Wind speed']\n",
    "        df['windY'] = np.sin(np.deg2rad(df['Wind direction'])) * df['Wind speed']\n",
    "        df['hourX'] = np.cos((df['Hour'].astype(np.float64)-1)*np.pi/24)\n",
    "        df['hourY'] = np.sin((df['Hour'].astype(np.float64)-1)*np.pi/24)\n",
    "        df['MonthX'] = np.cos((df['Month'].astype(np.float64)-1)*np.pi/12)\n",
    "        df['MonthY'] = np.sin((df['Month'].astype(np.float64)-1)*np.pi/12)\n",
    "        \n",
    "        import datetime\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "        df['isWeekend'] =  (df['Date'].dt.dayofweek>=5).astype(int)\n",
    "        \n",
    "        df.interpolate(method='linear', limit=5,inplace=True)\n",
    "        \n",
    "        # Drop Additional columns\n",
    "        df.drop('Benzene', axis=1, inplace=True)\n",
    "        df.drop('Toluene',axis=1, inplace=True)\n",
    "        df.drop('Xylene', axis=1,inplace=True)\n",
    "        df.drop('AQI_Bucket',axis=1,inplace=True)\n",
    "        df.drop('Datetime',axis=1,inplace=True)\n",
    "        df.drop('StationId',axis=1,inplace=True)\n",
    "        df.drop('Short-wave irradiation',axis=1,inplace=True)\n",
    "        df.drop('Date',axis=1,inplace=True)\n",
    "        df.drop('Time',axis=1,inplace=True)\n",
    "        \n",
    "        # Rolling and shifting \n",
    "        print(\"Size before roll\",len(df))\n",
    "        for i in rlist:\n",
    "            df[i+'_mean'] = df[i].rolling(window=roll, min_periods=int(roll//4)).mean()\n",
    "            df[i+'_min'] = df[i].rolling(window=roll, min_periods=int(roll/4)).min()\n",
    "            df[i+'_max'] = df[i].rolling(window=roll, min_periods=int(roll/4)).max()\n",
    "        for i in rlist:\n",
    "            df[i] = df[i].shift(-shift)\n",
    "        df.dropna(inplace=True)\n",
    "        print(\"Size after roll\",len(df))\n",
    "        \n",
    "        return df.copy()\n",
    "    def getdf(self,station):\n",
    "        return self.split_aqi[station]\n",
    "    def plot(self,station):\n",
    "        df = self.getdf(station)\n",
    "    def stats(self):\n",
    "        pass\n",
    "dat = dataset(met,aqi,split_aqi)\n",
    "df = dat.mergedData('DL031')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StandardScaler()\n",
      "-6325.902946422826\n",
      "72.40145556005754\n",
      "73.25310699289916\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif, f_regression , mutual_info_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "dfTrain = df[:]\n",
    "X = df[['PM2.5_mean','PM2.5_min','PM2.5_max','Temperature','Relative Humidity','windX','windY','Year','MonthX','MonthY','hourX','hourY','isWeekend']]\n",
    "y = df['PM2.5']\n",
    "scaler = StandardScaler()\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "print(scaler.fit(Xtrain))\n",
    "\n",
    "reg = LinearRegression().fit(scaler.transform(Xtrain), ytrain)\n",
    "# reg = MLPRegressor(random_state=1, max_iter=300).fit(scaler.transform(Xtrain), ytrain)\n",
    "# reg = SVR(C=1.0, epsilon=0.2).fit(scaler.transform(Xtrain), ytrain)\n",
    "\n",
    "testPred = reg.predict(scaler.transform(Xtest))\n",
    "trainPred = reg.predict(scaler.transform(Xtrain))\n",
    "mse = np.mean((testPred - np.array(ytest))*(testPred - np.array(ytest)))\n",
    "print(reg.score(X, y))\n",
    "print(mean_squared_error(testPred, ytest,squared=False))\n",
    "print(mean_squared_error(trainPred, ytrain,squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[22. 28.]\n",
      " [49. 64.]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "# Place tensors on the CPU\n",
    "with tf.device('/CPU:0'):\n",
    "  a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "  b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "\n",
    "# Run on the GPU\n",
    "c = tf.matmul(a, b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op RandomStandardNormal in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_53 (Dense)             (None, 100)               1400      \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 11,601\n",
      "Trainable params: 11,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 16763 samples, validate on 4191 samples\n",
      "Epoch 1/1000\n",
      "Executing op __inference_keras_scratch_graph_1363728 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "15700/16763 [===========================>..] - ETA: 0s - loss: 10480.0923 - mse: 10480.0928 - mae: 67.2870Executing op __inference_keras_scratch_graph_1363796 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "16763/16763 [==============================] - 3s 192us/step - loss: 10133.7352 - mse: 10133.7354 - mae: 65.9274 - val_loss: 5127.6018 - val_mse: 5127.6016 - val_mae: 47.5918\n",
      "Epoch 2/1000\n",
      "16763/16763 [==============================] - 1s 43us/step - loss: 5396.7998 - mse: 5396.8022 - mae: 47.3462 - val_loss: 4904.9565 - val_mse: 4904.9570 - val_mae: 47.0625\n",
      "Epoch 3/1000\n",
      "16763/16763 [==============================] - 1s 40us/step - loss: 5243.2228 - mse: 5243.2207 - mae: 46.6275 - val_loss: 4833.8764 - val_mse: 4833.8770 - val_mae: 46.3830\n",
      "Epoch 4/1000\n",
      "16763/16763 [==============================] - 1s 39us/step - loss: 5176.0929 - mse: 5176.0952 - mae: 46.2457 - val_loss: 4766.0039 - val_mse: 4766.0039 - val_mae: 45.9997\n",
      "Epoch 5/1000\n",
      "16763/16763 [==============================] - 1s 43us/step - loss: 5078.0200 - mse: 5078.0186 - mae: 45.8159 - val_loss: 4712.5304 - val_mse: 4712.5303 - val_mae: 45.4416\n",
      "Epoch 6/1000\n",
      "16763/16763 [==============================] - 1s 39us/step - loss: 4991.9189 - mse: 4991.9194 - mae: 45.3966 - val_loss: 4650.9524 - val_mse: 4650.9521 - val_mae: 45.5618\n",
      "Epoch 7/1000\n",
      "16763/16763 [==============================] - 1s 39us/step - loss: 4917.4293 - mse: 4917.4287 - mae: 45.0526 - val_loss: 4578.8252 - val_mse: 4578.8257 - val_mae: 44.6439\n",
      "Epoch 8/1000\n",
      "16763/16763 [==============================] - 1s 39us/step - loss: 4845.9075 - mse: 4845.9097 - mae: 44.6958 - val_loss: 4625.7418 - val_mse: 4625.7417 - val_mae: 45.7446\n",
      "Epoch 9/1000\n",
      "16763/16763 [==============================] - 1s 42us/step - loss: 4787.7651 - mse: 4787.7661 - mae: 44.4987 - val_loss: 4505.8749 - val_mse: 4505.8755 - val_mae: 44.7654\n",
      "Epoch 10/1000\n",
      "16763/16763 [==============================] - 1s 38us/step - loss: 4734.9719 - mse: 4734.9731 - mae: 44.1606 - val_loss: 4438.0211 - val_mse: 4438.0215 - val_mae: 43.7608\n",
      "Epoch 11/1000\n",
      "16763/16763 [==============================] - 1s 39us/step - loss: 4670.0037 - mse: 4670.0044 - mae: 43.8790 - val_loss: 4419.7418 - val_mse: 4419.7427 - val_mae: 43.4086\n",
      "Epoch 12/1000\n",
      "16763/16763 [==============================] - 1s 39us/step - loss: 4637.7042 - mse: 4637.7046 - mae: 43.6594 - val_loss: 4391.2233 - val_mse: 4391.2231 - val_mae: 43.6472\n",
      "Epoch 13/1000\n",
      "16763/16763 [==============================] - 1s 41us/step - loss: 4590.2310 - mse: 4590.2300 - mae: 43.4509 - val_loss: 4360.3589 - val_mse: 4360.3579 - val_mae: 43.4151\n",
      "Epoch 14/1000\n",
      "16763/16763 [==============================] - 1s 44us/step - loss: 4555.7028 - mse: 4555.7026 - mae: 43.2581 - val_loss: 4332.5440 - val_mse: 4332.5435 - val_mae: 43.5314\n",
      "Epoch 15/1000\n",
      "16763/16763 [==============================] - 1s 38us/step - loss: 4524.8460 - mse: 4524.8457 - mae: 43.0833 - val_loss: 4320.6404 - val_mse: 4320.6411 - val_mae: 43.4526\n",
      "Epoch 16/1000\n",
      "16763/16763 [==============================] - 1s 47us/step - loss: 4489.4610 - mse: 4489.4629 - mae: 42.7685 - val_loss: 4286.8540 - val_mse: 4286.8540 - val_mae: 43.5519\n",
      "Epoch 17/1000\n",
      "16763/16763 [==============================] - 1s 50us/step - loss: 4474.5835 - mse: 4474.5850 - mae: 42.7974 - val_loss: 4270.0396 - val_mse: 4270.0400 - val_mae: 43.4006\n",
      "Epoch 18/1000\n",
      "16763/16763 [==============================] - 1s 49us/step - loss: 4434.8640 - mse: 4434.8623 - mae: 42.5895 - val_loss: 4292.5206 - val_mse: 4292.5210 - val_mae: 42.2128\n",
      "Epoch 19/1000\n",
      "16763/16763 [==============================] - 1s 50us/step - loss: 4398.1406 - mse: 4398.1421 - mae: 42.4265 - val_loss: 4233.3243 - val_mse: 4233.3242 - val_mae: 42.4536\n",
      "Epoch 20/1000\n",
      "16763/16763 [==============================] - 1s 43us/step - loss: 4374.8458 - mse: 4374.8452 - mae: 42.2522 - val_loss: 4264.8212 - val_mse: 4264.8218 - val_mae: 43.4402\n",
      "Epoch 21/1000\n",
      "16763/16763 [==============================] - 1s 46us/step - loss: 4348.9256 - mse: 4348.9287 - mae: 42.0988 - val_loss: 4182.8539 - val_mse: 4182.8540 - val_mae: 42.8089\n",
      "Epoch 22/1000\n",
      "16763/16763 [==============================] - 1s 44us/step - loss: 4313.1952 - mse: 4313.1948 - mae: 41.9568 - val_loss: 4300.2712 - val_mse: 4300.2710 - val_mae: 42.3315\n",
      "Epoch 23/1000\n",
      "16763/16763 [==============================] - 1s 49us/step - loss: 4302.5954 - mse: 4302.5947 - mae: 41.8501 - val_loss: 4156.8405 - val_mse: 4156.8403 - val_mae: 42.7701\n",
      "Epoch 24/1000\n",
      "16763/16763 [==============================] - 1s 63us/step - loss: 4276.1841 - mse: 4276.1855 - mae: 41.8351 - val_loss: 4116.9068 - val_mse: 4116.9067 - val_mae: 41.8432\n",
      "Epoch 25/1000\n",
      "16763/16763 [==============================] - 1s 60us/step - loss: 4251.0369 - mse: 4251.0361 - mae: 41.6427 - val_loss: 4125.5424 - val_mse: 4125.5425 - val_mae: 41.6128\n",
      "Epoch 26/1000\n",
      "16763/16763 [==============================] - 1s 56us/step - loss: 4233.9825 - mse: 4233.9819 - mae: 41.5159 - val_loss: 4098.0940 - val_mse: 4098.0938 - val_mae: 42.6368\n",
      "Epoch 27/1000\n",
      "16763/16763 [==============================] - 1s 49us/step - loss: 4217.2887 - mse: 4217.2900 - mae: 41.5358 - val_loss: 4189.4814 - val_mse: 4189.4810 - val_mae: 43.9135\n",
      "Epoch 28/1000\n",
      "16763/16763 [==============================] - 1s 45us/step - loss: 4198.0702 - mse: 4198.0698 - mae: 41.3974 - val_loss: 4053.2400 - val_mse: 4053.2383 - val_mae: 42.0655\n",
      "Epoch 29/1000\n",
      "16763/16763 [==============================] - 1s 53us/step - loss: 4161.9208 - mse: 4161.9189 - mae: 41.2634 - val_loss: 4024.2476 - val_mse: 4024.2476 - val_mae: 41.5087\n",
      "Epoch 30/1000\n",
      "16763/16763 [==============================] - 1s 46us/step - loss: 4140.3211 - mse: 4140.3203 - mae: 41.0085 - val_loss: 4050.6545 - val_mse: 4050.6548 - val_mae: 42.5147\n",
      "Epoch 31/1000\n",
      "16763/16763 [==============================] - 1s 40us/step - loss: 4136.7835 - mse: 4136.7827 - mae: 41.1460 - val_loss: 4005.7761 - val_mse: 4005.7747 - val_mae: 41.9331\n",
      "Epoch 32/1000\n",
      "16763/16763 [==============================] - 1s 49us/step - loss: 4109.1871 - mse: 4109.1880 - mae: 40.9175 - val_loss: 4006.5089 - val_mse: 4006.5078 - val_mae: 41.8157\n",
      "Epoch 33/1000\n",
      "16763/16763 [==============================] - 1s 55us/step - loss: 4079.7213 - mse: 4079.7217 - mae: 40.8084 - val_loss: 3974.3398 - val_mse: 3974.3408 - val_mae: 41.1176\n",
      "Epoch 34/1000\n",
      "16763/16763 [==============================] - 1s 53us/step - loss: 4048.0906 - mse: 4048.0911 - mae: 40.6272 - val_loss: 3965.4196 - val_mse: 3965.4197 - val_mae: 41.4297\n",
      "Epoch 35/1000\n",
      "16763/16763 [==============================] - 1s 61us/step - loss: 4035.0217 - mse: 4035.0220 - mae: 40.5908 - val_loss: 3980.4642 - val_mse: 3980.4644 - val_mae: 41.7600\n",
      "Epoch 36/1000\n",
      "16763/16763 [==============================] - 1s 56us/step - loss: 4023.2803 - mse: 4023.2798 - mae: 40.6105 - val_loss: 3973.3872 - val_mse: 3973.3872 - val_mae: 41.2972\n",
      "Epoch 37/1000\n",
      "16763/16763 [==============================] - 1s 49us/step - loss: 4013.2294 - mse: 4013.2288 - mae: 40.5897 - val_loss: 4001.3388 - val_mse: 4001.3389 - val_mae: 40.6876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/1000\n",
      "16763/16763 [==============================] - 1s 49us/step - loss: 3994.7483 - mse: 3994.7473 - mae: 40.4111 - val_loss: 3916.7457 - val_mse: 3916.7456 - val_mae: 41.1102\n",
      "Epoch 39/1000\n",
      "16763/16763 [==============================] - 1s 67us/step - loss: 3977.0177 - mse: 3977.0171 - mae: 40.4057 - val_loss: 3917.4462 - val_mse: 3917.4470 - val_mae: 40.8425\n",
      "Epoch 40/1000\n",
      "16763/16763 [==============================] - 1s 61us/step - loss: 3954.3292 - mse: 3954.3293 - mae: 40.2085 - val_loss: 3915.7191 - val_mse: 3915.7202 - val_mae: 41.3890\n",
      "Epoch 41/1000\n",
      "16763/16763 [==============================] - 1s 47us/step - loss: 3950.6255 - mse: 3950.6252 - mae: 40.2804 - val_loss: 3886.0537 - val_mse: 3886.0540 - val_mae: 40.9719\n",
      "Epoch 42/1000\n",
      "16763/16763 [==============================] - 1s 49us/step - loss: 3917.4142 - mse: 3917.4126 - mae: 40.1087 - val_loss: 3908.0191 - val_mse: 3908.0186 - val_mae: 41.8946\n",
      "Epoch 43/1000\n",
      "16763/16763 [==============================] - 1s 44us/step - loss: 3910.6576 - mse: 3910.6570 - mae: 40.0382 - val_loss: 3884.1887 - val_mse: 3884.1882 - val_mae: 40.9884\n",
      "Epoch 44/1000\n",
      "16763/16763 [==============================] - 1s 38us/step - loss: 3874.2402 - mse: 3874.2397 - mae: 39.8605 - val_loss: 3892.4820 - val_mse: 3892.4819 - val_mae: 41.1238\n",
      "Epoch 45/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 3883.8664 - mse: 3883.8665 - mae: 39.9605 - val_loss: 3853.1121 - val_mse: 3853.1116 - val_mae: 40.8212\n",
      "Epoch 46/1000\n",
      "16763/16763 [==============================] - 1s 37us/step - loss: 3860.1993 - mse: 3860.2000 - mae: 39.8043 - val_loss: 3853.0082 - val_mse: 3853.0083 - val_mae: 41.1657\n",
      "Epoch 47/1000\n",
      "16763/16763 [==============================] - 1s 37us/step - loss: 3860.4961 - mse: 3860.4954 - mae: 39.8450 - val_loss: 3910.1417 - val_mse: 3910.1421 - val_mae: 41.6376\n",
      "Epoch 48/1000\n",
      "16763/16763 [==============================] - 1s 40us/step - loss: 3831.2874 - mse: 3831.2861 - mae: 39.7821 - val_loss: 3855.1495 - val_mse: 3855.1497 - val_mae: 40.7815\n",
      "Epoch 49/1000\n",
      "16763/16763 [==============================] - 1s 37us/step - loss: 3830.9971 - mse: 3830.9976 - mae: 39.6886 - val_loss: 3865.3928 - val_mse: 3865.3926 - val_mae: 40.4893\n",
      "Epoch 50/1000\n",
      "16763/16763 [==============================] - 1s 44us/step - loss: 3821.1030 - mse: 3821.1033 - mae: 39.6368 - val_loss: 3836.2849 - val_mse: 3836.2849 - val_mae: 41.0718\n",
      "Epoch 51/1000\n",
      "16763/16763 [==============================] - 1s 44us/step - loss: 3803.3892 - mse: 3803.3889 - mae: 39.5597 - val_loss: 3823.1968 - val_mse: 3823.1968 - val_mae: 40.5736\n",
      "Epoch 52/1000\n",
      "16763/16763 [==============================] - 1s 45us/step - loss: 3787.3950 - mse: 3787.3945 - mae: 39.5080 - val_loss: 3842.0237 - val_mse: 3842.0239 - val_mae: 40.7898\n",
      "Epoch 53/1000\n",
      "16763/16763 [==============================] - 1s 45us/step - loss: 3777.9578 - mse: 3777.9563 - mae: 39.4687 - val_loss: 3849.8905 - val_mse: 3849.8909 - val_mae: 41.2110\n",
      "Epoch 54/1000\n",
      "16763/16763 [==============================] - 1s 41us/step - loss: 3756.9812 - mse: 3756.9812 - mae: 39.4452 - val_loss: 3838.3287 - val_mse: 3838.3293 - val_mae: 41.7888\n",
      "Epoch 55/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 3745.2877 - mse: 3745.2856 - mae: 39.4447 - val_loss: 3790.6410 - val_mse: 3790.6414 - val_mae: 40.4301\n",
      "Epoch 56/1000\n",
      "16763/16763 [==============================] - 1s 38us/step - loss: 3738.8849 - mse: 3738.8848 - mae: 39.4396 - val_loss: 3815.2290 - val_mse: 3815.2288 - val_mae: 40.6207\n",
      "Epoch 57/1000\n",
      "16763/16763 [==============================] - 1s 38us/step - loss: 3724.0977 - mse: 3724.0967 - mae: 39.2771 - val_loss: 3799.4045 - val_mse: 3799.4050 - val_mae: 40.6602\n",
      "Epoch 58/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 3717.0220 - mse: 3717.0237 - mae: 39.1790 - val_loss: 3824.7834 - val_mse: 3824.7839 - val_mae: 41.3007\n",
      "Epoch 59/1000\n",
      "16763/16763 [==============================] - 1s 38us/step - loss: 3697.1482 - mse: 3697.1475 - mae: 39.1365 - val_loss: 3753.7302 - val_mse: 3753.7297 - val_mae: 40.5844\n",
      "Epoch 60/1000\n",
      "16763/16763 [==============================] - 1s 46us/step - loss: 3684.5186 - mse: 3684.5188 - mae: 39.1727 - val_loss: 3768.3884 - val_mse: 3768.3877 - val_mae: 40.0521\n",
      "Epoch 61/1000\n",
      "16763/16763 [==============================] - 1s 53us/step - loss: 3676.9776 - mse: 3676.9773 - mae: 39.0232 - val_loss: 3805.1718 - val_mse: 3805.1724 - val_mae: 40.5377\n",
      "Epoch 62/1000\n",
      "16763/16763 [==============================] - 1s 47us/step - loss: 3682.4590 - mse: 3682.4585 - mae: 39.2013 - val_loss: 3819.7708 - val_mse: 3819.7703 - val_mae: 41.0328\n",
      "Epoch 63/1000\n",
      "16763/16763 [==============================] - 1s 44us/step - loss: 3659.0576 - mse: 3659.0576 - mae: 39.0703 - val_loss: 3806.3841 - val_mse: 3806.3843 - val_mae: 40.4426\n",
      "Epoch 64/1000\n",
      "16763/16763 [==============================] - 1s 38us/step - loss: 3637.5771 - mse: 3637.5769 - mae: 38.9232 - val_loss: 3745.2518 - val_mse: 3745.2515 - val_mae: 40.3871\n",
      "Epoch 65/1000\n",
      "16763/16763 [==============================] - 1s 39us/step - loss: 3625.0150 - mse: 3625.0146 - mae: 38.8771 - val_loss: 3768.0863 - val_mse: 3768.0867 - val_mae: 40.8046\n",
      "Epoch 66/1000\n",
      "16763/16763 [==============================] - 1s 41us/step - loss: 3619.6572 - mse: 3619.6555 - mae: 38.8776 - val_loss: 3755.1088 - val_mse: 3755.1089 - val_mae: 40.4198\n",
      "Epoch 67/1000\n",
      "16763/16763 [==============================] - 1s 42us/step - loss: 3595.0263 - mse: 3595.0264 - mae: 38.7136 - val_loss: 3724.3742 - val_mse: 3724.3738 - val_mae: 40.4050\n",
      "Epoch 68/1000\n",
      "16763/16763 [==============================] - 1s 38us/step - loss: 3584.8772 - mse: 3584.8774 - mae: 38.7650 - val_loss: 3812.5596 - val_mse: 3812.5596 - val_mae: 41.1371\n",
      "Epoch 69/1000\n",
      "16763/16763 [==============================] - 1s 43us/step - loss: 3591.7915 - mse: 3591.7917 - mae: 38.7346 - val_loss: 3749.6684 - val_mse: 3749.6682 - val_mae: 40.8650\n",
      "Epoch 70/1000\n",
      "16763/16763 [==============================] - 1s 47us/step - loss: 3574.3029 - mse: 3574.3020 - mae: 38.7748 - val_loss: 3706.0702 - val_mse: 3706.0698 - val_mae: 40.1584\n",
      "Epoch 71/1000\n",
      "16763/16763 [==============================] - 1s 46us/step - loss: 3567.0982 - mse: 3567.0989 - mae: 38.6564 - val_loss: 3723.2038 - val_mse: 3723.2041 - val_mae: 40.0728\n",
      "Epoch 72/1000\n",
      "16763/16763 [==============================] - 1s 45us/step - loss: 3551.0344 - mse: 3551.0342 - mae: 38.5535 - val_loss: 3738.8697 - val_mse: 3738.8687 - val_mae: 40.1083\n",
      "Epoch 73/1000\n",
      "16763/16763 [==============================] - 1s 46us/step - loss: 3549.3435 - mse: 3549.3438 - mae: 38.6569 - val_loss: 3699.7759 - val_mse: 3699.7756 - val_mae: 40.2478\n",
      "Epoch 74/1000\n",
      "16763/16763 [==============================] - 1s 37us/step - loss: 3533.8233 - mse: 3533.8242 - mae: 38.5656 - val_loss: 3689.7522 - val_mse: 3689.7522 - val_mae: 40.1086\n",
      "Epoch 75/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 3520.7872 - mse: 3520.7864 - mae: 38.4453 - val_loss: 3704.0162 - val_mse: 3704.0159 - val_mae: 40.2770\n",
      "Epoch 76/1000\n",
      "16763/16763 [==============================] - 1s 37us/step - loss: 3511.0015 - mse: 3510.9995 - mae: 38.4612 - val_loss: 3688.3272 - val_mse: 3688.3269 - val_mae: 40.0258\n",
      "Epoch 77/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 3498.0039 - mse: 3498.0032 - mae: 38.4250 - val_loss: 3702.5822 - val_mse: 3702.5823 - val_mae: 40.1237\n",
      "Epoch 78/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 3492.1015 - mse: 3492.1023 - mae: 38.4267 - val_loss: 3706.8053 - val_mse: 3706.8052 - val_mae: 39.9384\n",
      "Epoch 79/1000\n",
      "16763/16763 [==============================] - 1s 41us/step - loss: 3477.9932 - mse: 3477.9927 - mae: 38.3148 - val_loss: 3735.9702 - val_mse: 3735.9700 - val_mae: 39.9568\n",
      "Epoch 80/1000\n",
      "16763/16763 [==============================] - 1s 46us/step - loss: 3489.0861 - mse: 3489.0867 - mae: 38.4203 - val_loss: 3672.7735 - val_mse: 3672.7732 - val_mae: 39.8924\n",
      "Epoch 81/1000\n",
      "16763/16763 [==============================] - 1s 47us/step - loss: 3460.9068 - mse: 3460.9080 - mae: 38.2450 - val_loss: 3661.1497 - val_mse: 3661.1501 - val_mae: 40.2892\n",
      "Epoch 82/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16763/16763 [==============================] - 1s 45us/step - loss: 3443.9645 - mse: 3443.9631 - mae: 38.1575 - val_loss: 3672.4638 - val_mse: 3672.4631 - val_mae: 40.3993\n",
      "Epoch 83/1000\n",
      "16763/16763 [==============================] - 1s 44us/step - loss: 3461.1519 - mse: 3461.1514 - mae: 38.2878 - val_loss: 3633.0713 - val_mse: 3633.0715 - val_mae: 40.0807\n",
      "Epoch 84/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 3428.6363 - mse: 3428.6353 - mae: 38.1144 - val_loss: 3680.0933 - val_mse: 3680.0928 - val_mae: 40.4098\n",
      "Epoch 85/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 3421.3987 - mse: 3421.3999 - mae: 38.1311 - val_loss: 3683.1670 - val_mse: 3683.1658 - val_mae: 40.6148\n",
      "Epoch 86/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 3423.2744 - mse: 3423.2739 - mae: 38.1260 - val_loss: 3617.9405 - val_mse: 3617.9407 - val_mae: 39.6026\n",
      "Epoch 87/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 3407.1591 - mse: 3407.1602 - mae: 37.9928 - val_loss: 3655.8550 - val_mse: 3655.8552 - val_mae: 40.1704\n",
      "Epoch 88/1000\n",
      "16763/16763 [==============================] - 1s 40us/step - loss: 3383.7705 - mse: 3383.7698 - mae: 37.8789 - val_loss: 3641.3029 - val_mse: 3641.3032 - val_mae: 40.3646\n",
      "Epoch 89/1000\n",
      "16763/16763 [==============================] - 1s 41us/step - loss: 3391.1543 - mse: 3391.1543 - mae: 38.0530 - val_loss: 3687.3307 - val_mse: 3687.3296 - val_mae: 40.3539\n",
      "Epoch 90/1000\n",
      "16763/16763 [==============================] - 1s 44us/step - loss: 3381.5333 - mse: 3381.5330 - mae: 38.0483 - val_loss: 3653.4212 - val_mse: 3653.4214 - val_mae: 39.9636\n",
      "Epoch 91/1000\n",
      "16763/16763 [==============================] - 1s 46us/step - loss: 3361.4669 - mse: 3361.4658 - mae: 37.9226 - val_loss: 3687.1649 - val_mse: 3687.1646 - val_mae: 40.0874\n",
      "Epoch 92/1000\n",
      "16763/16763 [==============================] - 1s 45us/step - loss: 3356.2174 - mse: 3356.2175 - mae: 37.8564 - val_loss: 3631.2192 - val_mse: 3631.2192 - val_mae: 40.4198\n",
      "Epoch 93/1000\n",
      "16763/16763 [==============================] - 1s 46us/step - loss: 3355.7535 - mse: 3355.7537 - mae: 37.8661 - val_loss: 3624.2541 - val_mse: 3624.2537 - val_mae: 40.2855\n",
      "Epoch 94/1000\n",
      "16763/16763 [==============================] - 1s 37us/step - loss: 3323.2503 - mse: 3323.2512 - mae: 37.6822 - val_loss: 3668.8799 - val_mse: 3668.8801 - val_mae: 41.3259\n",
      "Epoch 95/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 3324.0411 - mse: 3324.0430 - mae: 37.7239 - val_loss: 3587.9454 - val_mse: 3587.9448 - val_mae: 39.7803\n",
      "Epoch 96/1000\n",
      "16763/16763 [==============================] - 1s 37us/step - loss: 3312.3442 - mse: 3312.3447 - mae: 37.6693 - val_loss: 3613.3443 - val_mse: 3613.3440 - val_mae: 40.4088\n",
      "Epoch 97/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 3301.1892 - mse: 3301.1909 - mae: 37.5444 - val_loss: 3614.7468 - val_mse: 3614.7468 - val_mae: 39.6799\n",
      "Epoch 98/1000\n",
      "16763/16763 [==============================] - 1s 37us/step - loss: 3310.6257 - mse: 3310.6260 - mae: 37.5992 - val_loss: 3616.3094 - val_mse: 3616.3098 - val_mae: 39.7832\n",
      "Epoch 99/1000\n",
      "16763/16763 [==============================] - 1s 40us/step - loss: 3287.0001 - mse: 3286.9990 - mae: 37.5541 - val_loss: 3604.4599 - val_mse: 3604.4602 - val_mae: 39.9511\n",
      "Epoch 100/1000\n",
      "16763/16763 [==============================] - 1s 44us/step - loss: 3270.8371 - mse: 3270.8379 - mae: 37.5504 - val_loss: 3614.7023 - val_mse: 3614.7017 - val_mae: 39.9579\n",
      "Epoch 101/1000\n",
      "16763/16763 [==============================] - 1s 45us/step - loss: 3285.0620 - mse: 3285.0610 - mae: 37.5711 - val_loss: 3596.2065 - val_mse: 3596.2053 - val_mae: 40.1199\n",
      "Epoch 102/1000\n",
      "16763/16763 [==============================] - 1s 48us/step - loss: 3256.9545 - mse: 3256.9539 - mae: 37.4004 - val_loss: 3585.0387 - val_mse: 3585.0391 - val_mae: 40.3705\n",
      "Epoch 103/1000\n",
      "16763/16763 [==============================] - 1s 45us/step - loss: 3254.6686 - mse: 3254.6677 - mae: 37.4701 - val_loss: 3559.0254 - val_mse: 3559.0261 - val_mae: 39.7128\n",
      "Epoch 104/1000\n",
      "16763/16763 [==============================] - 1s 37us/step - loss: 3261.8092 - mse: 3261.8093 - mae: 37.4719 - val_loss: 3745.9659 - val_mse: 3745.9658 - val_mae: 42.4524\n",
      "Epoch 105/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 3242.2217 - mse: 3242.2217 - mae: 37.4267 - val_loss: 3550.9117 - val_mse: 3550.9119 - val_mae: 39.6317\n",
      "Epoch 106/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 3232.4597 - mse: 3232.4592 - mae: 37.2369 - val_loss: 3570.3147 - val_mse: 3570.3152 - val_mae: 40.1810\n",
      "Epoch 107/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 3225.9754 - mse: 3225.9771 - mae: 37.2433 - val_loss: 3513.9766 - val_mse: 3513.9763 - val_mae: 39.2364\n",
      "Epoch 108/1000\n",
      "16763/16763 [==============================] - 1s 37us/step - loss: 3214.3077 - mse: 3214.3076 - mae: 37.1542 - val_loss: 3531.2954 - val_mse: 3531.2954 - val_mae: 39.6745\n",
      "Epoch 109/1000\n",
      "16763/16763 [==============================] - 1s 37us/step - loss: 3203.5235 - mse: 3203.5225 - mae: 37.1633 - val_loss: 3529.8373 - val_mse: 3529.8369 - val_mae: 40.2465\n",
      "Epoch 110/1000\n",
      "16763/16763 [==============================] - 1s 45us/step - loss: 3210.2847 - mse: 3210.2849 - mae: 37.3680 - val_loss: 3645.4967 - val_mse: 3645.4966 - val_mae: 40.0990\n",
      "Epoch 111/1000\n",
      "16763/16763 [==============================] - 1s 47us/step - loss: 3189.6532 - mse: 3189.6519 - mae: 37.1297 - val_loss: 3642.7358 - val_mse: 3642.7351 - val_mae: 40.4433\n",
      "Epoch 112/1000\n",
      "16763/16763 [==============================] - 1s 48us/step - loss: 3176.5899 - mse: 3176.5874 - mae: 37.1540 - val_loss: 3516.5709 - val_mse: 3516.5723 - val_mae: 39.4205\n",
      "Epoch 113/1000\n",
      "16763/16763 [==============================] - 1s 45us/step - loss: 3170.1962 - mse: 3170.1963 - mae: 36.9821 - val_loss: 3489.6182 - val_mse: 3489.6184 - val_mae: 39.5725\n",
      "Epoch 114/1000\n",
      "16763/16763 [==============================] - 1s 45us/step - loss: 3161.2025 - mse: 3161.2031 - mae: 37.1120 - val_loss: 3481.3666 - val_mse: 3481.3672 - val_mae: 39.4164\n",
      "Epoch 115/1000\n",
      "16763/16763 [==============================] - 1s 46us/step - loss: 3153.0687 - mse: 3153.0696 - mae: 36.9579 - val_loss: 3542.5504 - val_mse: 3542.5496 - val_mae: 39.2997\n",
      "Epoch 116/1000\n",
      "16763/16763 [==============================] - 1s 46us/step - loss: 3158.1758 - mse: 3158.1753 - mae: 37.0460 - val_loss: 3544.8569 - val_mse: 3544.8574 - val_mae: 39.7825\n",
      "Epoch 117/1000\n",
      "16763/16763 [==============================] - 1s 39us/step - loss: 3149.8053 - mse: 3149.8059 - mae: 36.9789 - val_loss: 3473.6206 - val_mse: 3473.6199 - val_mae: 39.0395\n",
      "Epoch 118/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 3126.4628 - mse: 3126.4617 - mae: 36.8432 - val_loss: 3541.7867 - val_mse: 3541.7866 - val_mae: 40.0972\n",
      "Epoch 119/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 3124.4758 - mse: 3124.4758 - mae: 36.7748 - val_loss: 3520.0507 - val_mse: 3520.0508 - val_mae: 39.8077\n",
      "Epoch 120/1000\n",
      "16763/16763 [==============================] - 1s 37us/step - loss: 3121.5951 - mse: 3121.5952 - mae: 36.7986 - val_loss: 3486.0568 - val_mse: 3486.0569 - val_mae: 39.7209\n",
      "Epoch 121/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 3109.1418 - mse: 3109.1416 - mae: 36.7326 - val_loss: 3470.1291 - val_mse: 3470.1294 - val_mae: 39.3493\n",
      "Epoch 122/1000\n",
      "16763/16763 [==============================] - 1s 39us/step - loss: 3090.6993 - mse: 3090.6992 - mae: 36.6839 - val_loss: 3526.0172 - val_mse: 3526.0171 - val_mae: 39.8127\n",
      "Epoch 123/1000\n",
      "16763/16763 [==============================] - 1s 46us/step - loss: 3077.6066 - mse: 3077.6074 - mae: 36.5283 - val_loss: 3491.3395 - val_mse: 3491.3391 - val_mae: 39.8428\n",
      "Epoch 124/1000\n",
      "16763/16763 [==============================] - 1s 46us/step - loss: 3089.5228 - mse: 3089.5217 - mae: 36.6037 - val_loss: 3443.5490 - val_mse: 3443.5493 - val_mae: 39.5641\n",
      "Epoch 125/1000\n",
      "16763/16763 [==============================] - 1s 45us/step - loss: 3075.1174 - mse: 3075.1172 - mae: 36.6452 - val_loss: 3465.4557 - val_mse: 3465.4553 - val_mae: 39.4676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 126/1000\n",
      "16763/16763 [==============================] - 1s 45us/step - loss: 3066.4520 - mse: 3066.4536 - mae: 36.3858 - val_loss: 3482.0953 - val_mse: 3482.0955 - val_mae: 39.0467\n",
      "Epoch 127/1000\n",
      "16763/16763 [==============================] - 1s 51us/step - loss: 3055.3711 - mse: 3055.3711 - mae: 36.5241 - val_loss: 3460.9782 - val_mse: 3460.9785 - val_mae: 39.5533\n",
      "Epoch 128/1000\n",
      "16763/16763 [==============================] - 1s 46us/step - loss: 3042.2402 - mse: 3042.2388 - mae: 36.3615 - val_loss: 3471.4952 - val_mse: 3471.4954 - val_mae: 39.7748\n",
      "Epoch 129/1000\n",
      "16763/16763 [==============================] - 1s 46us/step - loss: 3037.4264 - mse: 3037.4272 - mae: 36.5667 - val_loss: 3612.5692 - val_mse: 3612.5688 - val_mae: 40.4091\n",
      "Epoch 130/1000\n",
      "16763/16763 [==============================] - 1s 37us/step - loss: 3019.8312 - mse: 3019.8320 - mae: 36.5055 - val_loss: 3466.4333 - val_mse: 3466.4343 - val_mae: 39.5239\n",
      "Epoch 131/1000\n",
      "16763/16763 [==============================] - 1s 37us/step - loss: 3010.8315 - mse: 3010.8306 - mae: 36.2670 - val_loss: 3508.6914 - val_mse: 3508.6912 - val_mae: 39.5898\n",
      "Epoch 132/1000\n",
      "16763/16763 [==============================] - 1s 35us/step - loss: 3023.9379 - mse: 3023.9380 - mae: 36.3098 - val_loss: 3484.6809 - val_mse: 3484.6804 - val_mae: 39.2072\n",
      "Epoch 133/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 2995.7381 - mse: 2995.7383 - mae: 36.2016 - val_loss: 3473.9984 - val_mse: 3473.9985 - val_mae: 39.0103\n",
      "Epoch 134/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 3004.2480 - mse: 3004.2468 - mae: 36.3445 - val_loss: 3418.3809 - val_mse: 3418.3806 - val_mae: 38.8881\n",
      "Epoch 135/1000\n",
      "16763/16763 [==============================] - 1s 37us/step - loss: 2988.3861 - mse: 2988.3857 - mae: 36.2116 - val_loss: 3482.8724 - val_mse: 3482.8733 - val_mae: 39.6223\n",
      "Epoch 136/1000\n",
      "16763/16763 [==============================] - 1s 44us/step - loss: 2991.7338 - mse: 2991.7336 - mae: 36.2759 - val_loss: 3435.3634 - val_mse: 3435.3640 - val_mae: 39.2806\n",
      "Epoch 137/1000\n",
      "16763/16763 [==============================] - 1s 43us/step - loss: 2971.5729 - mse: 2971.5730 - mae: 36.1303 - val_loss: 3410.7495 - val_mse: 3410.7485 - val_mae: 38.9941\n",
      "Epoch 138/1000\n",
      "16763/16763 [==============================] - 1s 46us/step - loss: 2972.4851 - mse: 2972.4858 - mae: 36.0746 - val_loss: 3407.3703 - val_mse: 3407.3706 - val_mae: 39.4816\n",
      "Epoch 139/1000\n",
      "16763/16763 [==============================] - 1s 48us/step - loss: 2980.6003 - mse: 2980.6006 - mae: 36.2408 - val_loss: 3406.3873 - val_mse: 3406.3872 - val_mae: 39.0463\n",
      "Epoch 140/1000\n",
      "16763/16763 [==============================] - 1s 44us/step - loss: 2968.2584 - mse: 2968.2588 - mae: 36.1705 - val_loss: 3542.6213 - val_mse: 3542.6208 - val_mae: 40.1745\n",
      "Epoch 141/1000\n",
      "16763/16763 [==============================] - 1s 44us/step - loss: 2978.1583 - mse: 2978.1580 - mae: 36.2444 - val_loss: 3436.7440 - val_mse: 3436.7439 - val_mae: 38.9069\n",
      "Epoch 142/1000\n",
      "16763/16763 [==============================] - 1s 44us/step - loss: 2953.1784 - mse: 2953.1804 - mae: 36.1520 - val_loss: 3504.5558 - val_mse: 3504.5547 - val_mae: 39.3408\n",
      "Epoch 143/1000\n",
      "16763/16763 [==============================] - 1s 44us/step - loss: 2939.0626 - mse: 2939.0625 - mae: 35.9277 - val_loss: 3449.0739 - val_mse: 3449.0742 - val_mae: 40.1384\n",
      "Epoch 144/1000\n",
      "16763/16763 [==============================] - 1s 45us/step - loss: 2937.6350 - mse: 2937.6348 - mae: 36.0611 - val_loss: 3488.7908 - val_mse: 3488.7905 - val_mae: 39.7446\n",
      "Epoch 145/1000\n",
      "16763/16763 [==============================] - 1s 44us/step - loss: 2917.7012 - mse: 2917.7009 - mae: 35.9043 - val_loss: 3403.1834 - val_mse: 3403.1836 - val_mae: 39.0338\n",
      "Epoch 146/1000\n",
      "16763/16763 [==============================] - 1s 41us/step - loss: 2927.4934 - mse: 2927.4939 - mae: 36.0168 - val_loss: 3391.8749 - val_mse: 3391.8745 - val_mae: 39.1312\n",
      "Epoch 147/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 2931.7684 - mse: 2931.7693 - mae: 36.0842 - val_loss: 3378.6548 - val_mse: 3378.6555 - val_mae: 39.0256\n",
      "Epoch 148/1000\n",
      "16763/16763 [==============================] - 1s 35us/step - loss: 2902.4494 - mse: 2902.4495 - mae: 35.8390 - val_loss: 3418.9906 - val_mse: 3418.9905 - val_mae: 39.1030\n",
      "Epoch 149/1000\n",
      "16763/16763 [==============================] - 1s 37us/step - loss: 2896.0970 - mse: 2896.0969 - mae: 35.7664 - val_loss: 3457.5013 - val_mse: 3457.5020 - val_mae: 39.6798\n",
      "Epoch 150/1000\n",
      "16763/16763 [==============================] - 1s 37us/step - loss: 2900.0227 - mse: 2900.0229 - mae: 35.8702 - val_loss: 3395.7993 - val_mse: 3395.7998 - val_mae: 39.7466\n",
      "Epoch 151/1000\n",
      "16763/16763 [==============================] - 1s 37us/step - loss: 2880.4965 - mse: 2880.4963 - mae: 35.7691 - val_loss: 3491.6503 - val_mse: 3491.6506 - val_mae: 40.7682\n",
      "Epoch 152/1000\n",
      "16763/16763 [==============================] - 1s 44us/step - loss: 2875.5320 - mse: 2875.5330 - mae: 35.6636 - val_loss: 3399.1252 - val_mse: 3399.1250 - val_mae: 38.7620\n",
      "Epoch 153/1000\n",
      "16763/16763 [==============================] - 1s 46us/step - loss: 2874.1773 - mse: 2874.1772 - mae: 35.7305 - val_loss: 3356.7998 - val_mse: 3356.7998 - val_mae: 38.8447\n",
      "Epoch 154/1000\n",
      "16763/16763 [==============================] - 1s 46us/step - loss: 2857.4373 - mse: 2857.4395 - mae: 35.5985 - val_loss: 3371.8519 - val_mse: 3371.8521 - val_mae: 38.6004\n",
      "Epoch 155/1000\n",
      "16763/16763 [==============================] - 1s 46us/step - loss: 2881.2880 - mse: 2881.2876 - mae: 35.7657 - val_loss: 3413.7651 - val_mse: 3413.7654 - val_mae: 39.8947\n",
      "Epoch 156/1000\n",
      "16763/16763 [==============================] - 1s 47us/step - loss: 2863.5419 - mse: 2863.5415 - mae: 35.7359 - val_loss: 3400.5882 - val_mse: 3400.5881 - val_mae: 39.2260\n",
      "Epoch 157/1000\n",
      "16763/16763 [==============================] - 1s 45us/step - loss: 2857.3952 - mse: 2857.3953 - mae: 35.6764 - val_loss: 3413.8162 - val_mse: 3413.8157 - val_mae: 39.5590\n",
      "Epoch 158/1000\n",
      "16763/16763 [==============================] - 1s 37us/step - loss: 2841.5862 - mse: 2841.5864 - mae: 35.5679 - val_loss: 3401.5623 - val_mse: 3401.5625 - val_mae: 39.5626\n",
      "Epoch 159/1000\n",
      "16763/16763 [==============================] - 1s 41us/step - loss: 2829.1247 - mse: 2829.1245 - mae: 35.4931 - val_loss: 3405.2907 - val_mse: 3405.2903 - val_mae: 40.0687\n",
      "Epoch 160/1000\n",
      "16763/16763 [==============================] - 1s 42us/step - loss: 2836.5438 - mse: 2836.5430 - mae: 35.5085 - val_loss: 3385.9339 - val_mse: 3385.9338 - val_mae: 38.8890\n",
      "Epoch 161/1000\n",
      "16763/16763 [==============================] - 1s 43us/step - loss: 2838.0924 - mse: 2838.0908 - mae: 35.5968 - val_loss: 3347.8099 - val_mse: 3347.8096 - val_mae: 38.9135\n",
      "Epoch 162/1000\n",
      "16763/16763 [==============================] - 1s 41us/step - loss: 2814.4688 - mse: 2814.4692 - mae: 35.4934 - val_loss: 3462.4300 - val_mse: 3462.4299 - val_mae: 39.8650\n",
      "Epoch 163/1000\n",
      "16763/16763 [==============================] - 1s 51us/step - loss: 2820.4944 - mse: 2820.4946 - mae: 35.4539 - val_loss: 3317.1804 - val_mse: 3317.1804 - val_mae: 38.7842\n",
      "Epoch 164/1000\n",
      "16763/16763 [==============================] - 1s 52us/step - loss: 2800.0881 - mse: 2800.0876 - mae: 35.2913 - val_loss: 3352.9530 - val_mse: 3352.9534 - val_mae: 39.5793\n",
      "Epoch 165/1000\n",
      "16763/16763 [==============================] - 1s 53us/step - loss: 2803.7372 - mse: 2803.7375 - mae: 35.4380 - val_loss: 3412.2009 - val_mse: 3412.2009 - val_mae: 40.0676\n",
      "Epoch 166/1000\n",
      "16763/16763 [==============================] - 1s 50us/step - loss: 2807.7730 - mse: 2807.7732 - mae: 35.3936 - val_loss: 3425.1746 - val_mse: 3425.1738 - val_mae: 40.3310\n",
      "Epoch 167/1000\n",
      "16763/16763 [==============================] - 1s 49us/step - loss: 2814.2419 - mse: 2814.2419 - mae: 35.5269 - val_loss: 3316.3082 - val_mse: 3316.3079 - val_mae: 38.9280\n",
      "Epoch 168/1000\n",
      "16763/16763 [==============================] - 1s 52us/step - loss: 2776.6084 - mse: 2776.6091 - mae: 35.2117 - val_loss: 3427.4666 - val_mse: 3427.4663 - val_mae: 38.8240\n",
      "Epoch 169/1000\n",
      "16763/16763 [==============================] - 1s 54us/step - loss: 2777.2401 - mse: 2777.2397 - mae: 35.2758 - val_loss: 3399.0285 - val_mse: 3399.0281 - val_mae: 40.0125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 170/1000\n",
      "16763/16763 [==============================] - 1s 52us/step - loss: 2778.9956 - mse: 2778.9946 - mae: 35.2734 - val_loss: 3353.5262 - val_mse: 3353.5259 - val_mae: 38.5621\n",
      "Epoch 171/1000\n",
      "16763/16763 [==============================] - 1s 49us/step - loss: 2775.0032 - mse: 2775.0029 - mae: 35.3452 - val_loss: 3381.9897 - val_mse: 3381.9905 - val_mae: 39.4346\n",
      "Epoch 172/1000\n",
      "16763/16763 [==============================] - 1s 40us/step - loss: 2781.7756 - mse: 2781.7766 - mae: 35.3061 - val_loss: 3380.7024 - val_mse: 3380.7024 - val_mae: 39.0260\n",
      "Epoch 173/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 2758.2327 - mse: 2758.2346 - mae: 35.1211 - val_loss: 3463.3409 - val_mse: 3463.3408 - val_mae: 40.6952\n",
      "Epoch 174/1000\n",
      "16763/16763 [==============================] - 1s 37us/step - loss: 2757.0518 - mse: 2757.0522 - mae: 35.1155 - val_loss: 3414.8869 - val_mse: 3414.8870 - val_mae: 39.7253\n",
      "Epoch 175/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 2744.5119 - mse: 2744.5132 - mae: 35.0536 - val_loss: 3324.7820 - val_mse: 3324.7822 - val_mae: 38.8379\n",
      "Epoch 176/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 2750.6786 - mse: 2750.6780 - mae: 35.1889 - val_loss: 3286.1387 - val_mse: 3286.1384 - val_mae: 38.7134\n",
      "Epoch 177/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 2737.6302 - mse: 2737.6311 - mae: 35.0556 - val_loss: 3315.8680 - val_mse: 3315.8674 - val_mae: 39.2161\n",
      "Epoch 178/1000\n",
      "16763/16763 [==============================] - 1s 44us/step - loss: 2722.7930 - mse: 2722.7927 - mae: 34.9602 - val_loss: 3290.4271 - val_mse: 3290.4270 - val_mae: 38.8835\n",
      "Epoch 179/1000\n",
      "16763/16763 [==============================] - 1s 44us/step - loss: 2731.9809 - mse: 2731.9800 - mae: 35.1722 - val_loss: 3441.6509 - val_mse: 3441.6504 - val_mae: 39.0699\n",
      "Epoch 180/1000\n",
      "16763/16763 [==============================] - 1s 45us/step - loss: 2727.3016 - mse: 2727.3025 - mae: 34.9425 - val_loss: 3300.9540 - val_mse: 3300.9541 - val_mae: 38.4111\n",
      "Epoch 181/1000\n",
      "16763/16763 [==============================] - 1s 44us/step - loss: 2703.8336 - mse: 2703.8340 - mae: 34.7767 - val_loss: 3342.3878 - val_mse: 3342.3887 - val_mae: 39.1975\n",
      "Epoch 182/1000\n",
      "16763/16763 [==============================] - 1s 44us/step - loss: 2710.6335 - mse: 2710.6323 - mae: 34.9832 - val_loss: 3308.6482 - val_mse: 3308.6484 - val_mae: 38.7997\n",
      "Epoch 183/1000\n",
      "16763/16763 [==============================] - 1s 46us/step - loss: 2699.9546 - mse: 2699.9541 - mae: 34.7745 - val_loss: 3327.2109 - val_mse: 3327.2104 - val_mae: 38.7231\n",
      "Epoch 184/1000\n",
      "16763/16763 [==============================] - 1s 45us/step - loss: 2691.7880 - mse: 2691.7886 - mae: 34.7805 - val_loss: 3327.0502 - val_mse: 3327.0508 - val_mae: 39.1092\n",
      "Epoch 185/1000\n",
      "16763/16763 [==============================] - 1s 45us/step - loss: 2694.2158 - mse: 2694.2148 - mae: 34.7855 - val_loss: 3365.9903 - val_mse: 3365.9900 - val_mae: 38.9259\n",
      "Epoch 186/1000\n",
      "16763/16763 [==============================] - 1s 50us/step - loss: 2701.3440 - mse: 2701.3445 - mae: 34.8507 - val_loss: 3275.4658 - val_mse: 3275.4658 - val_mae: 39.2641\n",
      "Epoch 187/1000\n",
      "16763/16763 [==============================] - 1s 46us/step - loss: 2693.2544 - mse: 2693.2539 - mae: 34.8342 - val_loss: 3315.0957 - val_mse: 3315.0959 - val_mae: 39.1809\n",
      "Epoch 188/1000\n",
      "16763/16763 [==============================] - 1s 47us/step - loss: 2679.8324 - mse: 2679.8320 - mae: 34.7961 - val_loss: 3271.7180 - val_mse: 3271.7178 - val_mae: 38.4473\n",
      "Epoch 189/1000\n",
      "16763/16763 [==============================] - 1s 45us/step - loss: 2665.1321 - mse: 2665.1326 - mae: 34.6986 - val_loss: 3311.8608 - val_mse: 3311.8611 - val_mae: 39.0570\n",
      "Epoch 190/1000\n",
      "16763/16763 [==============================] - 1s 37us/step - loss: 2662.5316 - mse: 2662.5312 - mae: 34.5971 - val_loss: 3335.2462 - val_mse: 3335.2463 - val_mae: 39.5854\n",
      "Epoch 191/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 2677.6952 - mse: 2677.6938 - mae: 34.8349 - val_loss: 3305.9905 - val_mse: 3305.9910 - val_mae: 38.7349\n",
      "Epoch 192/1000\n",
      "16763/16763 [==============================] - 1s 37us/step - loss: 2660.1964 - mse: 2660.1956 - mae: 34.6445 - val_loss: 3251.5974 - val_mse: 3251.5977 - val_mae: 38.7179\n",
      "Epoch 193/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 2650.1899 - mse: 2650.1892 - mae: 34.6596 - val_loss: 3316.9952 - val_mse: 3316.9956 - val_mae: 39.0671\n",
      "Epoch 194/1000\n",
      "16763/16763 [==============================] - 1s 46us/step - loss: 2664.3461 - mse: 2664.3472 - mae: 34.7467 - val_loss: 3304.8715 - val_mse: 3304.8716 - val_mae: 38.4990\n",
      "Epoch 195/1000\n",
      "16763/16763 [==============================] - 1s 44us/step - loss: 2650.1247 - mse: 2650.1255 - mae: 34.5774 - val_loss: 3267.8783 - val_mse: 3267.8792 - val_mae: 39.0590\n",
      "Epoch 196/1000\n",
      "16763/16763 [==============================] - 1s 47us/step - loss: 2643.4076 - mse: 2643.4080 - mae: 34.5664 - val_loss: 3279.5965 - val_mse: 3279.5964 - val_mae: 38.5469\n",
      "Epoch 197/1000\n",
      "16763/16763 [==============================] - 1s 57us/step - loss: 2643.7095 - mse: 2643.7097 - mae: 34.5863 - val_loss: 3263.8162 - val_mse: 3263.8162 - val_mae: 38.7078\n",
      "Epoch 198/1000\n",
      "16763/16763 [==============================] - 1s 81us/step - loss: 2647.2535 - mse: 2647.2542 - mae: 34.5561 - val_loss: 3262.5225 - val_mse: 3262.5222 - val_mae: 39.0630\n",
      "Epoch 199/1000\n",
      "16763/16763 [==============================] - 1s 65us/step - loss: 2618.0622 - mse: 2618.0630 - mae: 34.3679 - val_loss: 3279.0243 - val_mse: 3279.0244 - val_mae: 38.4943\n",
      "Epoch 200/1000\n",
      "16763/16763 [==============================] - 1s 49us/step - loss: 2622.4628 - mse: 2622.4636 - mae: 34.3784 - val_loss: 3310.1447 - val_mse: 3310.1458 - val_mae: 38.8456\n",
      "Epoch 201/1000\n",
      "16763/16763 [==============================] - 1s 59us/step - loss: 2624.1225 - mse: 2624.1226 - mae: 34.4869 - val_loss: 3305.0777 - val_mse: 3305.0776 - val_mae: 38.8837\n",
      "Epoch 202/1000\n",
      "16763/16763 [==============================] - 1s 51us/step - loss: 2603.4430 - mse: 2603.4421 - mae: 34.3488 - val_loss: 3284.7033 - val_mse: 3284.7036 - val_mae: 38.5691\n",
      "Epoch 203/1000\n",
      "16763/16763 [==============================] - 1s 53us/step - loss: 2616.3115 - mse: 2616.3113 - mae: 34.4337 - val_loss: 3395.6207 - val_mse: 3395.6201 - val_mae: 38.9962\n",
      "Epoch 204/1000\n",
      "16763/16763 [==============================] - 1s 70us/step - loss: 2604.3067 - mse: 2604.3066 - mae: 34.3413 - val_loss: 3255.2868 - val_mse: 3255.2869 - val_mae: 38.2182\n",
      "Epoch 205/1000\n",
      "16763/16763 [==============================] - 1s 53us/step - loss: 2599.6123 - mse: 2599.6130 - mae: 34.2921 - val_loss: 3242.0054 - val_mse: 3242.0046 - val_mae: 38.3994\n",
      "Epoch 206/1000\n",
      "16763/16763 [==============================] - 1s 44us/step - loss: 2594.9264 - mse: 2594.9263 - mae: 34.3302 - val_loss: 3284.6172 - val_mse: 3284.6182 - val_mae: 38.4856\n",
      "Epoch 207/1000\n",
      "16763/16763 [==============================] - 1s 43us/step - loss: 2583.0129 - mse: 2583.0127 - mae: 34.2457 - val_loss: 3406.1877 - val_mse: 3406.1877 - val_mae: 39.2252\n",
      "Epoch 208/1000\n",
      "16763/16763 [==============================] - 1s 46us/step - loss: 2600.6215 - mse: 2600.6226 - mae: 34.2782 - val_loss: 3255.5638 - val_mse: 3255.5635 - val_mae: 38.6686\n",
      "Epoch 209/1000\n",
      "16763/16763 [==============================] - 1s 43us/step - loss: 2574.0637 - mse: 2574.0632 - mae: 34.1120 - val_loss: 3272.7457 - val_mse: 3272.7456 - val_mae: 38.4049\n",
      "Epoch 210/1000\n",
      "16763/16763 [==============================] - 1s 44us/step - loss: 2588.7674 - mse: 2588.7673 - mae: 34.2974 - val_loss: 3233.9506 - val_mse: 3233.9509 - val_mae: 38.4031\n",
      "Epoch 211/1000\n",
      "16763/16763 [==============================] - 1s 57us/step - loss: 2576.3705 - mse: 2576.3708 - mae: 34.1580 - val_loss: 3275.1846 - val_mse: 3275.1846 - val_mae: 38.3329\n",
      "Epoch 212/1000\n",
      "16763/16763 [==============================] - 1s 52us/step - loss: 2565.1258 - mse: 2565.1255 - mae: 34.1102 - val_loss: 3291.5306 - val_mse: 3291.5305 - val_mae: 38.7893\n",
      "Epoch 213/1000\n",
      "16763/16763 [==============================] - 1s 56us/step - loss: 2572.6987 - mse: 2572.6982 - mae: 34.2438 - val_loss: 3219.3849 - val_mse: 3219.3857 - val_mae: 38.2304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 214/1000\n",
      "16763/16763 [==============================] - 1s 51us/step - loss: 2567.3698 - mse: 2567.3687 - mae: 34.1250 - val_loss: 3312.3715 - val_mse: 3312.3723 - val_mae: 39.1936\n",
      "Epoch 215/1000\n",
      "16763/16763 [==============================] - 1s 66us/step - loss: 2570.6599 - mse: 2570.6587 - mae: 34.2335 - val_loss: 3230.9428 - val_mse: 3230.9426 - val_mae: 38.2097\n",
      "Epoch 216/1000\n",
      "16763/16763 [==============================] - 1s 66us/step - loss: 2548.4042 - mse: 2548.4031 - mae: 34.0634 - val_loss: 3253.3540 - val_mse: 3253.3540 - val_mae: 38.5298\n",
      "Epoch 217/1000\n",
      "16763/16763 [==============================] - 1s 63us/step - loss: 2544.3675 - mse: 2544.3677 - mae: 34.0094 - val_loss: 3310.8410 - val_mse: 3310.8411 - val_mae: 39.0041\n",
      "Epoch 218/1000\n",
      "16763/16763 [==============================] - 1s 58us/step - loss: 2568.7289 - mse: 2568.7278 - mae: 34.2268 - val_loss: 3248.2913 - val_mse: 3248.2905 - val_mae: 38.3342\n",
      "Epoch 219/1000\n",
      "16763/16763 [==============================] - 1s 57us/step - loss: 2542.8459 - mse: 2542.8457 - mae: 33.9898 - val_loss: 3308.5665 - val_mse: 3308.5654 - val_mae: 38.9648\n",
      "Epoch 220/1000\n",
      "16763/16763 [==============================] - 1s 48us/step - loss: 2548.6202 - mse: 2548.6206 - mae: 34.1303 - val_loss: 3357.2391 - val_mse: 3357.2393 - val_mae: 40.5249\n",
      "Epoch 221/1000\n",
      "16763/16763 [==============================] - 1s 46us/step - loss: 2540.9638 - mse: 2540.9644 - mae: 34.0744 - val_loss: 3254.3906 - val_mse: 3254.3909 - val_mae: 38.2441\n",
      "Epoch 222/1000\n",
      "16763/16763 [==============================] - 1s 45us/step - loss: 2532.4885 - mse: 2532.4890 - mae: 34.0163 - val_loss: 3204.8613 - val_mse: 3204.8613 - val_mae: 38.1123\n",
      "Epoch 223/1000\n",
      "16763/16763 [==============================] - 1s 37us/step - loss: 2527.1243 - mse: 2527.1245 - mae: 33.9647 - val_loss: 3218.2823 - val_mse: 3218.2815 - val_mae: 38.1203\n",
      "Epoch 224/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 2532.1704 - mse: 2532.1704 - mae: 33.8838 - val_loss: 3208.1082 - val_mse: 3208.1079 - val_mae: 38.1109\n",
      "Epoch 225/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 2536.6402 - mse: 2536.6389 - mae: 33.9076 - val_loss: 3233.5713 - val_mse: 3233.5708 - val_mae: 38.0855\n",
      "Epoch 226/1000\n",
      "16763/16763 [==============================] - 1s 37us/step - loss: 2506.5763 - mse: 2506.5759 - mae: 33.8032 - val_loss: 3215.0593 - val_mse: 3215.0596 - val_mae: 38.6225\n",
      "Epoch 227/1000\n",
      "16763/16763 [==============================] - 1s 37us/step - loss: 2505.8915 - mse: 2505.8926 - mae: 33.8802 - val_loss: 3308.4192 - val_mse: 3308.4187 - val_mae: 40.1234\n",
      "Epoch 228/1000\n",
      "16763/16763 [==============================] - 1s 38us/step - loss: 2503.5941 - mse: 2503.5938 - mae: 33.8270 - val_loss: 3289.2126 - val_mse: 3289.2136 - val_mae: 38.4362\n",
      "Epoch 229/1000\n",
      "16763/16763 [==============================] - 1s 46us/step - loss: 2507.4069 - mse: 2507.4080 - mae: 33.8517 - val_loss: 3234.7411 - val_mse: 3234.7415 - val_mae: 38.6505\n",
      "Epoch 230/1000\n",
      "16763/16763 [==============================] - 1s 46us/step - loss: 2501.9547 - mse: 2501.9553 - mae: 33.8472 - val_loss: 3288.5910 - val_mse: 3288.5916 - val_mae: 39.7903\n",
      "Epoch 231/1000\n",
      "16763/16763 [==============================] - 1s 45us/step - loss: 2511.0251 - mse: 2511.0244 - mae: 33.8939 - val_loss: 3239.4464 - val_mse: 3239.4465 - val_mae: 38.4932\n",
      "Epoch 232/1000\n",
      "16763/16763 [==============================] - 1s 46us/step - loss: 2499.0120 - mse: 2499.0120 - mae: 33.8193 - val_loss: 3175.1313 - val_mse: 3175.1309 - val_mae: 38.3464\n",
      "Epoch 233/1000\n",
      "16763/16763 [==============================] - 1s 46us/step - loss: 2489.2027 - mse: 2489.2031 - mae: 33.6477 - val_loss: 3223.8626 - val_mse: 3223.8630 - val_mae: 38.5180\n",
      "Epoch 234/1000\n",
      "16763/16763 [==============================] - 1s 48us/step - loss: 2488.1286 - mse: 2488.1282 - mae: 33.7197 - val_loss: 3237.1998 - val_mse: 3237.2000 - val_mae: 38.3644\n",
      "Epoch 235/1000\n",
      "16763/16763 [==============================] - 1s 46us/step - loss: 2483.1812 - mse: 2483.1809 - mae: 33.6821 - val_loss: 3250.7841 - val_mse: 3250.7837 - val_mae: 38.7780\n",
      "Epoch 236/1000\n",
      "16763/16763 [==============================] - 1s 45us/step - loss: 2477.1088 - mse: 2477.1082 - mae: 33.6131 - val_loss: 3286.5669 - val_mse: 3286.5671 - val_mae: 39.4384\n",
      "Epoch 237/1000\n",
      "16763/16763 [==============================] - 1s 45us/step - loss: 2481.2632 - mse: 2481.2625 - mae: 33.6848 - val_loss: 3296.2012 - val_mse: 3296.2012 - val_mae: 38.1582\n",
      "Epoch 238/1000\n",
      "16763/16763 [==============================] - 1s 47us/step - loss: 2466.4765 - mse: 2466.4771 - mae: 33.5347 - val_loss: 3260.5458 - val_mse: 3260.5459 - val_mae: 38.3280\n",
      "Epoch 239/1000\n",
      "16763/16763 [==============================] - 1s 46us/step - loss: 2470.8401 - mse: 2470.8408 - mae: 33.5485 - val_loss: 3247.2009 - val_mse: 3247.2014 - val_mae: 38.6889\n",
      "Epoch 240/1000\n",
      "16763/16763 [==============================] - 1s 46us/step - loss: 2441.5336 - mse: 2441.5344 - mae: 33.4059 - val_loss: 3189.3558 - val_mse: 3189.3552 - val_mae: 38.3504\n",
      "Epoch 241/1000\n",
      "16763/16763 [==============================] - 1s 47us/step - loss: 2462.2759 - mse: 2462.2747 - mae: 33.5374 - val_loss: 3205.9281 - val_mse: 3205.9275 - val_mae: 38.2136\n",
      "Epoch 242/1000\n",
      "16763/16763 [==============================] - ETA: 0s - loss: 2443.5283 - mse: 2443.5273 - mae: 33.54 - 1s 48us/step - loss: 2445.7901 - mse: 2445.7888 - mae: 33.5261 - val_loss: 3314.9593 - val_mse: 3314.9597 - val_mae: 39.0555\n",
      "Epoch 243/1000\n",
      "16763/16763 [==============================] - 1s 44us/step - loss: 2449.1476 - mse: 2449.1470 - mae: 33.5603 - val_loss: 3346.1929 - val_mse: 3346.1936 - val_mae: 39.0555\n",
      "Epoch 244/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 2443.1363 - mse: 2443.1357 - mae: 33.4165 - val_loss: 3178.8896 - val_mse: 3178.8899 - val_mae: 38.1717\n",
      "Epoch 245/1000\n",
      "16763/16763 [==============================] - 1s 40us/step - loss: 2456.3565 - mse: 2456.3562 - mae: 33.6310 - val_loss: 3204.2188 - val_mse: 3204.2180 - val_mae: 38.0247\n",
      "Epoch 246/1000\n",
      "16763/16763 [==============================] - 1s 38us/step - loss: 2441.7288 - mse: 2441.7300 - mae: 33.4820 - val_loss: 3199.4371 - val_mse: 3199.4368 - val_mae: 38.2329\n",
      "Epoch 247/1000\n",
      "16763/16763 [==============================] - 1s 37us/step - loss: 2448.8618 - mse: 2448.8611 - mae: 33.4484 - val_loss: 3165.8107 - val_mse: 3165.8105 - val_mae: 37.8387\n",
      "Epoch 248/1000\n",
      "16763/16763 [==============================] - 1s 38us/step - loss: 2441.0680 - mse: 2441.0684 - mae: 33.4131 - val_loss: 3404.2920 - val_mse: 3404.2920 - val_mae: 38.9634\n",
      "Epoch 249/1000\n",
      "16763/16763 [==============================] - 1s 46us/step - loss: 2415.9066 - mse: 2415.9080 - mae: 33.2193 - val_loss: 3240.1382 - val_mse: 3240.1379 - val_mae: 38.8041\n",
      "Epoch 250/1000\n",
      "16763/16763 [==============================] - 1s 49us/step - loss: 2429.9380 - mse: 2429.9373 - mae: 33.4817 - val_loss: 3248.3072 - val_mse: 3248.3079 - val_mae: 38.1251\n",
      "Epoch 251/1000\n",
      "16763/16763 [==============================] - 1s 47us/step - loss: 2424.5129 - mse: 2424.5132 - mae: 33.3500 - val_loss: 3159.2703 - val_mse: 3159.2695 - val_mae: 37.7413\n",
      "Epoch 252/1000\n",
      "16763/16763 [==============================] - 1s 46us/step - loss: 2417.2770 - mse: 2417.2771 - mae: 33.2832 - val_loss: 3226.5728 - val_mse: 3226.5732 - val_mae: 38.5060\n",
      "Epoch 253/1000\n",
      "16763/16763 [==============================] - 1s 47us/step - loss: 2402.8545 - mse: 2402.8550 - mae: 33.2059 - val_loss: 3296.7209 - val_mse: 3296.7209 - val_mae: 39.1664\n",
      "Epoch 254/1000\n",
      "16763/16763 [==============================] - 1s 46us/step - loss: 2437.0658 - mse: 2437.0659 - mae: 33.4810 - val_loss: 3193.4350 - val_mse: 3193.4351 - val_mae: 37.9168\n",
      "Epoch 255/1000\n",
      "16763/16763 [==============================] - 1s 46us/step - loss: 2417.8858 - mse: 2417.8853 - mae: 33.3678 - val_loss: 3208.8131 - val_mse: 3208.8125 - val_mae: 38.7252\n",
      "Epoch 256/1000\n",
      "16763/16763 [==============================] - 1s 46us/step - loss: 2400.3958 - mse: 2400.3950 - mae: 33.2266 - val_loss: 3242.7040 - val_mse: 3242.7043 - val_mae: 39.4863\n",
      "Epoch 257/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16763/16763 [==============================] - 1s 43us/step - loss: 2406.9963 - mse: 2406.9966 - mae: 33.3445 - val_loss: 3211.1230 - val_mse: 3211.1228 - val_mae: 38.4315\n",
      "Epoch 258/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 2408.2195 - mse: 2408.2195 - mae: 33.2498 - val_loss: 3266.6458 - val_mse: 3266.6462 - val_mae: 39.5153\n",
      "Epoch 259/1000\n",
      "16763/16763 [==============================] - 1s 37us/step - loss: 2392.8875 - mse: 2392.8872 - mae: 33.1979 - val_loss: 3138.8502 - val_mse: 3138.8503 - val_mae: 37.7837\n",
      "Epoch 260/1000\n",
      "16763/16763 [==============================] - 1s 37us/step - loss: 2406.4808 - mse: 2406.4805 - mae: 33.3313 - val_loss: 3280.8334 - val_mse: 3280.8335 - val_mae: 38.7383\n",
      "Epoch 261/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 2393.5103 - mse: 2393.5100 - mae: 33.2415 - val_loss: 3294.3561 - val_mse: 3294.3562 - val_mae: 38.4880\n",
      "Epoch 262/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 2387.3777 - mse: 2387.3787 - mae: 33.1400 - val_loss: 3167.0613 - val_mse: 3167.0615 - val_mae: 37.8941\n",
      "Epoch 263/1000\n",
      "16763/16763 [==============================] - 1s 44us/step - loss: 2388.9617 - mse: 2388.9619 - mae: 33.1881 - val_loss: 3160.1517 - val_mse: 3160.1526 - val_mae: 38.0145\n",
      "Epoch 264/1000\n",
      "16763/16763 [==============================] - 1s 47us/step - loss: 2376.8475 - mse: 2376.8472 - mae: 33.1712 - val_loss: 3239.7505 - val_mse: 3239.7505 - val_mae: 38.5674\n",
      "Epoch 265/1000\n",
      "16763/16763 [==============================] - 1s 45us/step - loss: 2382.4223 - mse: 2382.4221 - mae: 33.1231 - val_loss: 3145.5259 - val_mse: 3145.5264 - val_mae: 37.8180\n",
      "Epoch 266/1000\n",
      "16763/16763 [==============================] - 1s 46us/step - loss: 2390.4520 - mse: 2390.4519 - mae: 33.2616 - val_loss: 3154.0397 - val_mse: 3154.0396 - val_mae: 37.8150\n",
      "Epoch 267/1000\n",
      "16763/16763 [==============================] - 1s 73us/step - loss: 2387.9440 - mse: 2387.9431 - mae: 33.2003 - val_loss: 3200.6253 - val_mse: 3200.6257 - val_mae: 38.1259\n",
      "Epoch 268/1000\n",
      "16763/16763 [==============================] - 1s 59us/step - loss: 2376.1934 - mse: 2376.1931 - mae: 33.1968 - val_loss: 3227.3896 - val_mse: 3227.3896 - val_mae: 38.9673\n",
      "Epoch 269/1000\n",
      "16763/16763 [==============================] - 1s 74us/step - loss: 2373.1024 - mse: 2373.1030 - mae: 33.1738 - val_loss: 3206.0740 - val_mse: 3206.0732 - val_mae: 38.3172\n",
      "Epoch 270/1000\n",
      "16763/16763 [==============================] - 1s 63us/step - loss: 2371.7766 - mse: 2371.7759 - mae: 33.0999 - val_loss: 3219.1033 - val_mse: 3219.1038 - val_mae: 38.1204\n",
      "Epoch 271/1000\n",
      "16763/16763 [==============================] - 1s 58us/step - loss: 2362.7978 - mse: 2362.7964 - mae: 32.9595 - val_loss: 3218.8545 - val_mse: 3218.8542 - val_mae: 39.0659\n",
      "Epoch 272/1000\n",
      "16763/16763 [==============================] - 1s 53us/step - loss: 2368.1226 - mse: 2368.1226 - mae: 33.0400 - val_loss: 3294.1207 - val_mse: 3294.1211 - val_mae: 39.9317\n",
      "Epoch 273/1000\n",
      "16763/16763 [==============================] - 1s 55us/step - loss: 2368.2495 - mse: 2368.2483 - mae: 33.0227 - val_loss: 3138.4375 - val_mse: 3138.4368 - val_mae: 37.9110\n",
      "Epoch 274/1000\n",
      "16763/16763 [==============================] - 1s 44us/step - loss: 2356.2504 - mse: 2356.2500 - mae: 33.0131 - val_loss: 3242.2066 - val_mse: 3242.2070 - val_mae: 38.1474\n",
      "Epoch 275/1000\n",
      "16763/16763 [==============================] - 1s 42us/step - loss: 2358.3050 - mse: 2358.3057 - mae: 33.0242 - val_loss: 3178.7831 - val_mse: 3178.7832 - val_mae: 38.0714\n",
      "Epoch 276/1000\n",
      "16763/16763 [==============================] - 1s 43us/step - loss: 2350.1174 - mse: 2350.1182 - mae: 32.8603 - val_loss: 3175.9664 - val_mse: 3175.9661 - val_mae: 37.9367\n",
      "Epoch 277/1000\n",
      "16763/16763 [==============================] - 1s 40us/step - loss: 2358.5693 - mse: 2358.5681 - mae: 33.0442 - val_loss: 3169.2045 - val_mse: 3169.2053 - val_mae: 38.2135\n",
      "Epoch 278/1000\n",
      "16763/16763 [==============================] - 1s 39us/step - loss: 2344.4256 - mse: 2344.4263 - mae: 32.9310 - val_loss: 3194.1326 - val_mse: 3194.1338 - val_mae: 38.4356\n",
      "Epoch 279/1000\n",
      "16763/16763 [==============================] - 2s 94us/step - loss: 2338.8777 - mse: 2338.8772 - mae: 32.9164 - val_loss: 3179.0481 - val_mse: 3179.0483 - val_mae: 38.3593\n",
      "Epoch 280/1000\n",
      "16763/16763 [==============================] - 2s 91us/step - loss: 2329.6176 - mse: 2329.6177 - mae: 32.7968 - val_loss: 3128.9253 - val_mse: 3128.9246 - val_mae: 37.4922\n",
      "Epoch 281/1000\n",
      "16763/16763 [==============================] - 1s 83us/step - loss: 2330.6269 - mse: 2330.6274 - mae: 32.9688 - val_loss: 3178.4484 - val_mse: 3178.4487 - val_mae: 38.1859\n",
      "Epoch 282/1000\n",
      "16763/16763 [==============================] - 1s 80us/step - loss: 2333.7810 - mse: 2333.7827 - mae: 33.0417 - val_loss: 3205.5436 - val_mse: 3205.5437 - val_mae: 38.0543\n",
      "Epoch 283/1000\n",
      "16763/16763 [==============================] - 1s 72us/step - loss: 2324.7371 - mse: 2324.7378 - mae: 32.8343 - val_loss: 3124.5612 - val_mse: 3124.5610 - val_mae: 37.5307\n",
      "Epoch 284/1000\n",
      "16763/16763 [==============================] - 1s 74us/step - loss: 2317.2043 - mse: 2317.2039 - mae: 32.7085 - val_loss: 3207.4175 - val_mse: 3207.4180 - val_mae: 38.4727\n",
      "Epoch 285/1000\n",
      "16763/16763 [==============================] - 1s 66us/step - loss: 2310.6680 - mse: 2310.6675 - mae: 32.6631 - val_loss: 3179.0303 - val_mse: 3179.0310 - val_mae: 38.3251\n",
      "Epoch 286/1000\n",
      "16763/16763 [==============================] - 1s 60us/step - loss: 2321.7067 - mse: 2321.7070 - mae: 32.8470 - val_loss: 3176.3762 - val_mse: 3176.3760 - val_mae: 38.3970\n",
      "Epoch 287/1000\n",
      "16763/16763 [==============================] - 1s 61us/step - loss: 2308.2336 - mse: 2308.2329 - mae: 32.6939 - val_loss: 3126.7587 - val_mse: 3126.7593 - val_mae: 37.8811\n",
      "Epoch 288/1000\n",
      "16763/16763 [==============================] - 1s 65us/step - loss: 2306.2557 - mse: 2306.2559 - mae: 32.7416 - val_loss: 3164.8176 - val_mse: 3164.8179 - val_mae: 37.9600\n",
      "Epoch 289/1000\n",
      "16763/16763 [==============================] - 1s 57us/step - loss: 2315.2146 - mse: 2315.2144 - mae: 32.7459 - val_loss: 3165.3700 - val_mse: 3165.3696 - val_mae: 38.0600\n",
      "Epoch 290/1000\n",
      "16763/16763 [==============================] - 1s 57us/step - loss: 2295.8071 - mse: 2295.8069 - mae: 32.6804 - val_loss: 3086.8735 - val_mse: 3086.8735 - val_mae: 37.4765\n",
      "Epoch 291/1000\n",
      "16763/16763 [==============================] - 1s 71us/step - loss: 2290.2701 - mse: 2290.2698 - mae: 32.5730 - val_loss: 3132.4450 - val_mse: 3132.4448 - val_mae: 37.5486\n",
      "Epoch 292/1000\n",
      "16763/16763 [==============================] - 1s 89us/step - loss: 2302.3036 - mse: 2302.3049 - mae: 32.6605 - val_loss: 3156.8750 - val_mse: 3156.8750 - val_mae: 37.9395\n",
      "Epoch 293/1000\n",
      "16763/16763 [==============================] - 1s 68us/step - loss: 2303.7740 - mse: 2303.7734 - mae: 32.7594 - val_loss: 3159.4404 - val_mse: 3159.4399 - val_mae: 38.7945\n",
      "Epoch 294/1000\n",
      "16763/16763 [==============================] - 1s 60us/step - loss: 2281.8708 - mse: 2281.8704 - mae: 32.5785 - val_loss: 3111.7369 - val_mse: 3111.7371 - val_mae: 37.5630\n",
      "Epoch 295/1000\n",
      "16763/16763 [==============================] - 1s 60us/step - loss: 2298.5701 - mse: 2298.5710 - mae: 32.6381 - val_loss: 3196.7724 - val_mse: 3196.7727 - val_mae: 39.1102\n",
      "Epoch 296/1000\n",
      "16763/16763 [==============================] - 1s 51us/step - loss: 2298.8477 - mse: 2298.8484 - mae: 32.7498 - val_loss: 3156.7495 - val_mse: 3156.7493 - val_mae: 38.0673\n",
      "Epoch 297/1000\n",
      "16763/16763 [==============================] - 1s 50us/step - loss: 2289.2395 - mse: 2289.2390 - mae: 32.6743 - val_loss: 3115.2715 - val_mse: 3115.2717 - val_mae: 37.9894\n",
      "Epoch 298/1000\n",
      "16763/16763 [==============================] - 1s 51us/step - loss: 2279.7073 - mse: 2279.7078 - mae: 32.5263 - val_loss: 3207.5882 - val_mse: 3207.5879 - val_mae: 37.9132\n",
      "Epoch 299/1000\n",
      "16763/16763 [==============================] - 1s 41us/step - loss: 2274.1788 - mse: 2274.1794 - mae: 32.5555 - val_loss: 3212.4239 - val_mse: 3212.4231 - val_mae: 38.4274\n",
      "Epoch 300/1000\n",
      "16763/16763 [==============================] - 1s 45us/step - loss: 2271.3836 - mse: 2271.3835 - mae: 32.5481 - val_loss: 3154.3201 - val_mse: 3154.3203 - val_mae: 37.8477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 301/1000\n",
      "16763/16763 [==============================] - 1s 37us/step - loss: 2267.3184 - mse: 2267.3184 - mae: 32.5022 - val_loss: 3114.4505 - val_mse: 3114.4504 - val_mae: 38.0969\n",
      "Epoch 302/1000\n",
      "16763/16763 [==============================] - 1s 43us/step - loss: 2273.6203 - mse: 2273.6199 - mae: 32.5750 - val_loss: 3186.5257 - val_mse: 3186.5256 - val_mae: 38.1685\n",
      "Epoch 303/1000\n",
      "16763/16763 [==============================] - 1s 46us/step - loss: 2270.6177 - mse: 2270.6174 - mae: 32.5027 - val_loss: 3255.9237 - val_mse: 3255.9238 - val_mae: 38.8344\n",
      "Epoch 304/1000\n",
      "16763/16763 [==============================] - 1s 56us/step - loss: 2262.8170 - mse: 2262.8162 - mae: 32.4330 - val_loss: 3196.4102 - val_mse: 3196.4099 - val_mae: 37.9412\n",
      "Epoch 305/1000\n",
      "16763/16763 [==============================] - 1s 54us/step - loss: 2282.9824 - mse: 2282.9829 - mae: 32.6595 - val_loss: 3206.0688 - val_mse: 3206.0688 - val_mae: 38.0236\n",
      "Epoch 306/1000\n",
      "16763/16763 [==============================] - 1s 71us/step - loss: 2265.0857 - mse: 2265.0867 - mae: 32.5219 - val_loss: 3126.8034 - val_mse: 3126.8030 - val_mae: 37.8919\n",
      "Epoch 307/1000\n",
      "16763/16763 [==============================] - 1s 67us/step - loss: 2247.1563 - mse: 2247.1565 - mae: 32.3552 - val_loss: 3140.0694 - val_mse: 3140.0691 - val_mae: 37.7310\n",
      "Epoch 308/1000\n",
      "16763/16763 [==============================] - 1s 59us/step - loss: 2253.9279 - mse: 2253.9285 - mae: 32.3584 - val_loss: 3257.3823 - val_mse: 3257.3831 - val_mae: 38.1520\n",
      "Epoch 309/1000\n",
      "16763/16763 [==============================] - 1s 48us/step - loss: 2255.3616 - mse: 2255.3613 - mae: 32.5603 - val_loss: 3221.1243 - val_mse: 3221.1238 - val_mae: 38.0860\n",
      "Epoch 310/1000\n",
      "16763/16763 [==============================] - 1s 53us/step - loss: 2250.9187 - mse: 2250.9180 - mae: 32.4563 - val_loss: 3148.4976 - val_mse: 3148.4980 - val_mae: 37.6465\n",
      "Epoch 311/1000\n",
      "16763/16763 [==============================] - 1s 44us/step - loss: 2259.7157 - mse: 2259.7163 - mae: 32.4336 - val_loss: 3096.1329 - val_mse: 3096.1335 - val_mae: 37.5053\n",
      "Epoch 312/1000\n",
      "16763/16763 [==============================] - 1s 47us/step - loss: 2224.8047 - mse: 2224.8054 - mae: 32.2601 - val_loss: 3158.1887 - val_mse: 3158.1885 - val_mae: 38.5707\n",
      "Epoch 313/1000\n",
      "16763/16763 [==============================] - 1s 45us/step - loss: 2240.6409 - mse: 2240.6406 - mae: 32.3834 - val_loss: 3182.9580 - val_mse: 3182.9573 - val_mae: 38.3719\n",
      "Epoch 314/1000\n",
      "16763/16763 [==============================] - 1s 46us/step - loss: 2251.0068 - mse: 2251.0076 - mae: 32.6096 - val_loss: 3094.8841 - val_mse: 3094.8835 - val_mae: 37.5660\n",
      "Epoch 315/1000\n",
      "16763/16763 [==============================] - 1s 46us/step - loss: 2225.5773 - mse: 2225.5771 - mae: 32.2739 - val_loss: 3095.1929 - val_mse: 3095.1931 - val_mae: 37.4207\n",
      "Epoch 316/1000\n",
      "16763/16763 [==============================] - 1s 46us/step - loss: 2232.1641 - mse: 2232.1648 - mae: 32.2517 - val_loss: 3183.6253 - val_mse: 3183.6255 - val_mae: 38.7352\n",
      "Epoch 317/1000\n",
      "16763/16763 [==============================] - 1s 38us/step - loss: 2224.4464 - mse: 2224.4458 - mae: 32.3855 - val_loss: 3146.9918 - val_mse: 3146.9922 - val_mae: 37.6330\n",
      "Epoch 318/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 2248.9063 - mse: 2248.9065 - mae: 32.4191 - val_loss: 3095.8920 - val_mse: 3095.8921 - val_mae: 37.5596\n",
      "Epoch 319/1000\n",
      "16763/16763 [==============================] - 1s 37us/step - loss: 2245.2761 - mse: 2245.2747 - mae: 32.3996 - val_loss: 3172.0088 - val_mse: 3172.0081 - val_mae: 38.9174\n",
      "Epoch 320/1000\n",
      "16763/16763 [==============================] - 1s 37us/step - loss: 2220.5972 - mse: 2220.5986 - mae: 32.2626 - val_loss: 3280.4153 - val_mse: 3280.4153 - val_mae: 39.1316\n",
      "Epoch 321/1000\n",
      "16763/16763 [==============================] - 1s 37us/step - loss: 2225.3999 - mse: 2225.3994 - mae: 32.2805 - val_loss: 3116.0438 - val_mse: 3116.0444 - val_mae: 37.5866\n",
      "Epoch 322/1000\n",
      "16763/16763 [==============================] - 1s 41us/step - loss: 2204.0351 - mse: 2204.0354 - mae: 32.0662 - val_loss: 3077.4497 - val_mse: 3077.4507 - val_mae: 37.4377\n",
      "Epoch 323/1000\n",
      "16763/16763 [==============================] - 1s 46us/step - loss: 2215.1262 - mse: 2215.1260 - mae: 32.2496 - val_loss: 3152.8875 - val_mse: 3152.8875 - val_mae: 37.9375\n",
      "Epoch 324/1000\n",
      "16763/16763 [==============================] - 1s 45us/step - loss: 2202.0210 - mse: 2202.0220 - mae: 32.0830 - val_loss: 3149.1322 - val_mse: 3149.1321 - val_mae: 38.2495\n",
      "Epoch 325/1000\n",
      "16763/16763 [==============================] - 1s 46us/step - loss: 2205.3768 - mse: 2205.3777 - mae: 32.1608 - val_loss: 3123.1540 - val_mse: 3123.1541 - val_mae: 37.5362\n",
      "Epoch 326/1000\n",
      "16763/16763 [==============================] - 1s 46us/step - loss: 2221.9738 - mse: 2221.9744 - mae: 32.2465 - val_loss: 3072.6559 - val_mse: 3072.6560 - val_mae: 37.5338\n",
      "Epoch 327/1000\n",
      "16763/16763 [==============================] - 1s 47us/step - loss: 2197.8564 - mse: 2197.8569 - mae: 32.1114 - val_loss: 3102.5387 - val_mse: 3102.5396 - val_mae: 37.8190\n",
      "Epoch 328/1000\n",
      "16763/16763 [==============================] - 1s 45us/step - loss: 2199.0664 - mse: 2199.0662 - mae: 32.1688 - val_loss: 3144.8461 - val_mse: 3144.8459 - val_mae: 37.6181\n",
      "Epoch 329/1000\n",
      "16763/16763 [==============================] - 1s 52us/step - loss: 2208.2134 - mse: 2208.2141 - mae: 32.1255 - val_loss: 3182.4698 - val_mse: 3182.4697 - val_mae: 38.2111\n",
      "Epoch 330/1000\n",
      "16763/16763 [==============================] - 1s 47us/step - loss: 2184.0572 - mse: 2184.0571 - mae: 32.0174 - val_loss: 3268.1495 - val_mse: 3268.1494 - val_mae: 37.9817\n",
      "Epoch 331/1000\n",
      "16763/16763 [==============================] - 1s 45us/step - loss: 2207.4030 - mse: 2207.4028 - mae: 32.1976 - val_loss: 3182.8929 - val_mse: 3182.8926 - val_mae: 39.1596\n",
      "Epoch 332/1000\n",
      "16763/16763 [==============================] - 1s 45us/step - loss: 2195.7771 - mse: 2195.7773 - mae: 32.1924 - val_loss: 3170.0710 - val_mse: 3170.0715 - val_mae: 38.5955\n",
      "Epoch 333/1000\n",
      "16763/16763 [==============================] - 1s 47us/step - loss: 2191.2044 - mse: 2191.2036 - mae: 32.1000 - val_loss: 3115.4423 - val_mse: 3115.4421 - val_mae: 37.4862\n",
      "Epoch 334/1000\n",
      "16763/16763 [==============================] - 1s 48us/step - loss: 2194.4610 - mse: 2194.4607 - mae: 32.0904 - val_loss: 3128.7452 - val_mse: 3128.7449 - val_mae: 37.9544\n",
      "Epoch 335/1000\n",
      "16763/16763 [==============================] - 1s 46us/step - loss: 2185.7559 - mse: 2185.7559 - mae: 32.0412 - val_loss: 3212.1317 - val_mse: 3212.1313 - val_mae: 38.2923\n",
      "Epoch 336/1000\n",
      "16763/16763 [==============================] - 1s 46us/step - loss: 2199.9979 - mse: 2199.9976 - mae: 32.0874 - val_loss: 3082.7238 - val_mse: 3082.7239 - val_mae: 37.4041\n",
      "Epoch 337/1000\n",
      "16763/16763 [==============================] - 1s 46us/step - loss: 2186.0996 - mse: 2186.0984 - mae: 32.0932 - val_loss: 3113.9741 - val_mse: 3113.9739 - val_mae: 37.5574\n",
      "Epoch 338/1000\n",
      "16763/16763 [==============================] - 1s 48us/step - loss: 2156.7702 - mse: 2156.7705 - mae: 31.7671 - val_loss: 3095.4660 - val_mse: 3095.4656 - val_mae: 37.6177\n",
      "Epoch 339/1000\n",
      "16763/16763 [==============================] - 1s 47us/step - loss: 2181.7922 - mse: 2181.7915 - mae: 31.9592 - val_loss: 3090.6295 - val_mse: 3090.6296 - val_mae: 37.6003\n",
      "Epoch 340/1000\n",
      "16763/16763 [==============================] - 1s 47us/step - loss: 2192.6705 - mse: 2192.6699 - mae: 32.2394 - val_loss: 3104.6288 - val_mse: 3104.6287 - val_mae: 37.4769\n",
      "Epoch 341/1000\n",
      "16763/16763 [==============================] - 1s 47us/step - loss: 2166.2244 - mse: 2166.2241 - mae: 31.8798 - val_loss: 3072.1781 - val_mse: 3072.1775 - val_mae: 37.3661\n",
      "Epoch 342/1000\n",
      "16763/16763 [==============================] - 1s 47us/step - loss: 2166.1105 - mse: 2166.1111 - mae: 31.8782 - val_loss: 3040.2089 - val_mse: 3040.2095 - val_mae: 37.4151\n",
      "Epoch 343/1000\n",
      "16763/16763 [==============================] - 1s 47us/step - loss: 2175.1909 - mse: 2175.1912 - mae: 32.0547 - val_loss: 3171.3240 - val_mse: 3171.3242 - val_mae: 38.9666\n",
      "Epoch 344/1000\n",
      "16763/16763 [==============================] - 1s 48us/step - loss: 2169.8451 - mse: 2169.8455 - mae: 32.0430 - val_loss: 3073.9102 - val_mse: 3073.9099 - val_mae: 37.4580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 345/1000\n",
      "16763/16763 [==============================] - 1s 47us/step - loss: 2160.4040 - mse: 2160.4045 - mae: 31.8758 - val_loss: 3092.6292 - val_mse: 3092.6299 - val_mae: 38.0707\n",
      "Epoch 346/1000\n",
      "16763/16763 [==============================] - 1s 37us/step - loss: 2157.5879 - mse: 2157.5881 - mae: 31.8577 - val_loss: 3283.3498 - val_mse: 3283.3503 - val_mae: 38.2844\n",
      "Epoch 347/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 2159.3399 - mse: 2159.3398 - mae: 31.9034 - val_loss: 3109.1662 - val_mse: 3109.1660 - val_mae: 38.0287\n",
      "Epoch 348/1000\n",
      "16763/16763 [==============================] - 1s 37us/step - loss: 2157.9766 - mse: 2157.9780 - mae: 32.0060 - val_loss: 3105.2305 - val_mse: 3105.2305 - val_mae: 37.4515\n",
      "Epoch 349/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 2146.0577 - mse: 2146.0583 - mae: 31.7417 - val_loss: 3174.5648 - val_mse: 3174.5652 - val_mae: 38.9099\n",
      "Epoch 350/1000\n",
      "16763/16763 [==============================] - 1s 37us/step - loss: 2147.7962 - mse: 2147.7947 - mae: 31.7665 - val_loss: 3066.3493 - val_mse: 3066.3496 - val_mae: 37.3963\n",
      "Epoch 351/1000\n",
      "16763/16763 [==============================] - 1s 40us/step - loss: 2152.0376 - mse: 2152.0374 - mae: 31.8869 - val_loss: 3095.7495 - val_mse: 3095.7502 - val_mae: 38.3028\n",
      "Epoch 352/1000\n",
      "16763/16763 [==============================] - 1s 47us/step - loss: 2142.2094 - mse: 2142.2087 - mae: 31.7917 - val_loss: 3077.8390 - val_mse: 3077.8396 - val_mae: 37.5075\n",
      "Epoch 353/1000\n",
      "16763/16763 [==============================] - 1s 46us/step - loss: 2169.6334 - mse: 2169.6331 - mae: 31.9825 - val_loss: 3079.7101 - val_mse: 3079.7107 - val_mae: 37.3617\n",
      "Epoch 354/1000\n",
      "16763/16763 [==============================] - 1s 46us/step - loss: 2175.5716 - mse: 2175.5708 - mae: 32.1002 - val_loss: 3153.9571 - val_mse: 3153.9578 - val_mae: 38.6924\n",
      "Epoch 355/1000\n",
      "16763/16763 [==============================] - 1s 46us/step - loss: 2139.4723 - mse: 2139.4722 - mae: 31.8418 - val_loss: 3040.7035 - val_mse: 3040.7029 - val_mae: 37.3397\n",
      "Epoch 356/1000\n",
      "16763/16763 [==============================] - 1s 50us/step - loss: 2134.3696 - mse: 2134.3691 - mae: 31.7394 - val_loss: 3104.5403 - val_mse: 3104.5403 - val_mae: 38.1094\n",
      "Epoch 357/1000\n",
      "16763/16763 [==============================] - 1s 45us/step - loss: 2141.0395 - mse: 2141.0403 - mae: 31.7116 - val_loss: 3067.0226 - val_mse: 3067.0217 - val_mae: 37.2735\n",
      "Epoch 358/1000\n",
      "16763/16763 [==============================] - 1s 44us/step - loss: 2128.3619 - mse: 2128.3613 - mae: 31.6551 - val_loss: 3114.2916 - val_mse: 3114.2917 - val_mae: 37.4953\n",
      "Epoch 359/1000\n",
      "16763/16763 [==============================] - 1s 47us/step - loss: 2144.8911 - mse: 2144.8909 - mae: 31.7264 - val_loss: 3081.4979 - val_mse: 3081.4976 - val_mae: 37.5309\n",
      "Epoch 360/1000\n",
      "16763/16763 [==============================] - 1s 46us/step - loss: 2133.1149 - mse: 2133.1150 - mae: 31.7757 - val_loss: 3069.8202 - val_mse: 3069.8208 - val_mae: 37.2865\n",
      "Epoch 361/1000\n",
      "16763/16763 [==============================] - 1s 45us/step - loss: 2132.5711 - mse: 2132.5710 - mae: 31.7341 - val_loss: 3082.8648 - val_mse: 3082.8650 - val_mae: 37.7103\n",
      "Epoch 362/1000\n",
      "16763/16763 [==============================] - 1s 44us/step - loss: 2114.9522 - mse: 2114.9517 - mae: 31.5490 - val_loss: 3090.5830 - val_mse: 3090.5830 - val_mae: 37.5651\n",
      "Epoch 363/1000\n",
      "16763/16763 [==============================] - 1s 46us/step - loss: 2129.9924 - mse: 2129.9934 - mae: 31.6465 - val_loss: 3035.4363 - val_mse: 3035.4353 - val_mae: 37.7068\n",
      "Epoch 364/1000\n",
      "16763/16763 [==============================] - 1s 47us/step - loss: 2138.9066 - mse: 2138.9062 - mae: 31.7916 - val_loss: 3138.7416 - val_mse: 3138.7412 - val_mae: 38.0965\n",
      "Epoch 365/1000\n",
      "16763/16763 [==============================] - 1s 46us/step - loss: 2124.6753 - mse: 2124.6746 - mae: 31.7776 - val_loss: 3137.2797 - val_mse: 3137.2791 - val_mae: 37.9602\n",
      "Epoch 366/1000\n",
      "16763/16763 [==============================] - 1s 40us/step - loss: 2099.2543 - mse: 2099.2542 - mae: 31.4411 - val_loss: 3149.6064 - val_mse: 3149.6064 - val_mae: 38.3125\n",
      "Epoch 367/1000\n",
      "16763/16763 [==============================] - 1s 37us/step - loss: 2133.7067 - mse: 2133.7070 - mae: 31.8026 - val_loss: 3057.1610 - val_mse: 3057.1616 - val_mae: 37.3327\n",
      "Epoch 368/1000\n",
      "16763/16763 [==============================] - 1s 37us/step - loss: 2110.7209 - mse: 2110.7214 - mae: 31.5563 - val_loss: 3128.5483 - val_mse: 3128.5496 - val_mae: 37.6576\n",
      "Epoch 369/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 2105.5227 - mse: 2105.5227 - mae: 31.6202 - val_loss: 3047.6199 - val_mse: 3047.6204 - val_mae: 37.1969\n",
      "Epoch 370/1000\n",
      "16763/16763 [==============================] - 1s 38us/step - loss: 2119.7185 - mse: 2119.7185 - mae: 31.5721 - val_loss: 3063.2518 - val_mse: 3063.2515 - val_mae: 37.3645\n",
      "Epoch 371/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 2112.7537 - mse: 2112.7537 - mae: 31.5556 - val_loss: 3137.9268 - val_mse: 3137.9265 - val_mae: 38.8299\n",
      "Epoch 372/1000\n",
      "16763/16763 [==============================] - 1s 45us/step - loss: 2110.3051 - mse: 2110.3044 - mae: 31.5817 - val_loss: 3169.4868 - val_mse: 3169.4868 - val_mae: 38.5730\n",
      "Epoch 373/1000\n",
      "16763/16763 [==============================] - 1s 47us/step - loss: 2094.9144 - mse: 2094.9148 - mae: 31.4834 - val_loss: 3074.7446 - val_mse: 3074.7446 - val_mae: 37.2253\n",
      "Epoch 374/1000\n",
      "16763/16763 [==============================] - 1s 46us/step - loss: 2110.3442 - mse: 2110.3440 - mae: 31.5157 - val_loss: 3349.4873 - val_mse: 3349.4866 - val_mae: 40.7168\n",
      "Epoch 375/1000\n",
      "16763/16763 [==============================] - 1s 46us/step - loss: 2108.3157 - mse: 2108.3157 - mae: 31.7040 - val_loss: 3109.1555 - val_mse: 3109.1550 - val_mae: 37.7818\n",
      "Epoch 376/1000\n",
      "16763/16763 [==============================] - 1s 46us/step - loss: 2103.4178 - mse: 2103.4175 - mae: 31.5168 - val_loss: 3054.2358 - val_mse: 3054.2358 - val_mae: 37.0424\n",
      "Epoch 377/1000\n",
      "16763/16763 [==============================] - 1s 45us/step - loss: 2092.5671 - mse: 2092.5664 - mae: 31.4154 - val_loss: 3045.7634 - val_mse: 3045.7634 - val_mae: 37.5774\n",
      "Epoch 378/1000\n",
      "16763/16763 [==============================] - 1s 47us/step - loss: 2090.0362 - mse: 2090.0369 - mae: 31.4342 - val_loss: 3052.1764 - val_mse: 3052.1755 - val_mae: 37.5748\n",
      "Epoch 379/1000\n",
      "16763/16763 [==============================] - 1s 46us/step - loss: 2102.1320 - mse: 2102.1316 - mae: 31.6281 - val_loss: 3061.3293 - val_mse: 3061.3298 - val_mae: 37.4414\n",
      "Epoch 380/1000\n",
      "16763/16763 [==============================] - 1s 45us/step - loss: 2101.5054 - mse: 2101.5056 - mae: 31.5472 - val_loss: 3077.7418 - val_mse: 3077.7415 - val_mae: 37.4739\n",
      "Epoch 381/1000\n",
      "16763/16763 [==============================] - 1s 46us/step - loss: 2074.7870 - mse: 2074.7871 - mae: 31.3776 - val_loss: 3119.9765 - val_mse: 3119.9758 - val_mae: 37.6503\n",
      "Epoch 382/1000\n",
      "16763/16763 [==============================] - 1s 46us/step - loss: 2098.6237 - mse: 2098.6228 - mae: 31.6285 - val_loss: 3166.1919 - val_mse: 3166.1921 - val_mae: 38.3582\n",
      "Epoch 383/1000\n",
      "16763/16763 [==============================] - 1s 46us/step - loss: 2078.5688 - mse: 2078.5696 - mae: 31.3426 - val_loss: 3046.0925 - val_mse: 3046.0920 - val_mae: 37.4911\n",
      "Epoch 384/1000\n",
      "16763/16763 [==============================] - 1s 46us/step - loss: 2086.3005 - mse: 2086.2996 - mae: 31.3627 - val_loss: 3089.6243 - val_mse: 3089.6245 - val_mae: 37.8275\n",
      "Epoch 385/1000\n",
      "16763/16763 [==============================] - 1s 40us/step - loss: 2086.5432 - mse: 2086.5422 - mae: 31.5703 - val_loss: 3081.1299 - val_mse: 3081.1304 - val_mae: 37.5589\n",
      "Epoch 386/1000\n",
      "16763/16763 [==============================] - 1s 37us/step - loss: 2071.8318 - mse: 2071.8318 - mae: 31.3225 - val_loss: 3071.7227 - val_mse: 3071.7229 - val_mae: 37.2504\n",
      "Epoch 387/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 2073.4658 - mse: 2073.4653 - mae: 31.3850 - val_loss: 3203.5355 - val_mse: 3203.5347 - val_mae: 38.5637\n",
      "Epoch 388/1000\n",
      "16763/16763 [==============================] - 1s 42us/step - loss: 2090.9045 - mse: 2090.9050 - mae: 31.4758 - val_loss: 3071.9489 - val_mse: 3071.9490 - val_mae: 37.5231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 389/1000\n",
      "16763/16763 [==============================] - 1s 38us/step - loss: 2073.2090 - mse: 2073.2087 - mae: 31.3945 - val_loss: 3152.9391 - val_mse: 3152.9399 - val_mae: 37.8381\n",
      "Epoch 390/1000\n",
      "16763/16763 [==============================] - 1s 38us/step - loss: 2083.3662 - mse: 2083.3662 - mae: 31.5166 - val_loss: 3072.8030 - val_mse: 3072.8025 - val_mae: 37.3038\n",
      "Epoch 391/1000\n",
      "16763/16763 [==============================] - 1s 54us/step - loss: 2075.0518 - mse: 2075.0520 - mae: 31.3751 - val_loss: 3079.5992 - val_mse: 3079.5994 - val_mae: 37.7662\n",
      "Epoch 392/1000\n",
      "16763/16763 [==============================] - 1s 47us/step - loss: 2072.4382 - mse: 2072.4387 - mae: 31.3557 - val_loss: 3116.6956 - val_mse: 3116.6960 - val_mae: 38.7584\n",
      "Epoch 393/1000\n",
      "16763/16763 [==============================] - 1s 49us/step - loss: 2054.8832 - mse: 2054.8831 - mae: 31.2267 - val_loss: 3070.4140 - val_mse: 3070.4146 - val_mae: 37.5770\n",
      "Epoch 394/1000\n",
      "16763/16763 [==============================] - 1s 45us/step - loss: 2064.7655 - mse: 2064.7651 - mae: 31.3308 - val_loss: 3076.5836 - val_mse: 3076.5833 - val_mae: 37.7041\n",
      "Epoch 395/1000\n",
      "16763/16763 [==============================] - 1s 46us/step - loss: 2059.9031 - mse: 2059.9023 - mae: 31.2573 - val_loss: 3265.7463 - val_mse: 3265.7463 - val_mae: 39.9573\n",
      "Epoch 396/1000\n",
      "16763/16763 [==============================] - 1s 50us/step - loss: 2069.1530 - mse: 2069.1538 - mae: 31.3459 - val_loss: 3117.5279 - val_mse: 3117.5276 - val_mae: 37.6902\n",
      "Epoch 397/1000\n",
      "16763/16763 [==============================] - 1s 47us/step - loss: 2073.5907 - mse: 2073.5906 - mae: 31.3801 - val_loss: 3095.5271 - val_mse: 3095.5273 - val_mae: 37.8646\n",
      "Epoch 398/1000\n",
      "16763/16763 [==============================] - 1s 39us/step - loss: 2057.1665 - mse: 2057.1663 - mae: 31.3637 - val_loss: 3030.7391 - val_mse: 3030.7390 - val_mae: 37.3760\n",
      "Epoch 399/1000\n",
      "16763/16763 [==============================] - 1s 39us/step - loss: 2064.7052 - mse: 2064.7051 - mae: 31.3199 - val_loss: 3235.4566 - val_mse: 3235.4570 - val_mae: 39.1765\n",
      "Epoch 400/1000\n",
      "16763/16763 [==============================] - 1s 37us/step - loss: 2046.1023 - mse: 2046.1011 - mae: 31.1959 - val_loss: 3134.8935 - val_mse: 3134.8943 - val_mae: 37.9420\n",
      "Epoch 401/1000\n",
      "16763/16763 [==============================] - 1s 40us/step - loss: 2047.5109 - mse: 2047.5101 - mae: 31.1585 - val_loss: 3052.8450 - val_mse: 3052.8452 - val_mae: 37.8422\n",
      "Epoch 402/1000\n",
      "16763/16763 [==============================] - 1s 38us/step - loss: 2066.5070 - mse: 2066.5066 - mae: 31.3302 - val_loss: 3086.9641 - val_mse: 3086.9646 - val_mae: 37.2799\n",
      "Epoch 403/1000\n",
      "16763/16763 [==============================] - 1s 44us/step - loss: 2058.7386 - mse: 2058.7385 - mae: 31.2351 - val_loss: 3086.8880 - val_mse: 3086.8882 - val_mae: 37.7631\n",
      "Epoch 404/1000\n",
      "16763/16763 [==============================] - 1s 46us/step - loss: 2048.2862 - mse: 2048.2854 - mae: 31.1440 - val_loss: 3103.8245 - val_mse: 3103.8240 - val_mae: 37.7858\n",
      "Epoch 405/1000\n",
      "16763/16763 [==============================] - 1s 47us/step - loss: 2056.4586 - mse: 2056.4585 - mae: 31.2521 - val_loss: 3045.7457 - val_mse: 3045.7458 - val_mae: 37.2692\n",
      "Epoch 406/1000\n",
      "16763/16763 [==============================] - 1s 49us/step - loss: 2038.6086 - mse: 2038.6089 - mae: 31.1773 - val_loss: 3102.2323 - val_mse: 3102.2324 - val_mae: 38.3570\n",
      "Epoch 407/1000\n",
      "16763/16763 [==============================] - 1s 46us/step - loss: 2038.5948 - mse: 2038.5955 - mae: 31.1314 - val_loss: 3102.7615 - val_mse: 3102.7622 - val_mae: 37.8053\n",
      "Epoch 408/1000\n",
      "16763/16763 [==============================] - 1s 51us/step - loss: 2036.3930 - mse: 2036.3928 - mae: 31.0604 - val_loss: 3041.5294 - val_mse: 3041.5293 - val_mae: 37.1009\n",
      "Epoch 409/1000\n",
      "16763/16763 [==============================] - 1s 49us/step - loss: 2045.1216 - mse: 2045.1224 - mae: 31.1880 - val_loss: 3232.5466 - val_mse: 3232.5471 - val_mae: 38.5402\n",
      "Epoch 410/1000\n",
      "16763/16763 [==============================] - 1s 53us/step - loss: 2042.5048 - mse: 2042.5048 - mae: 31.2329 - val_loss: 3079.1922 - val_mse: 3079.1926 - val_mae: 37.7222\n",
      "Epoch 411/1000\n",
      "16763/16763 [==============================] - 1s 48us/step - loss: 2039.9291 - mse: 2039.9288 - mae: 31.2215 - val_loss: 3045.9360 - val_mse: 3045.9363 - val_mae: 37.5132\n",
      "Epoch 412/1000\n",
      "16763/16763 [==============================] - 1s 54us/step - loss: 2028.6610 - mse: 2028.6615 - mae: 31.0743 - val_loss: 3052.4747 - val_mse: 3052.4749 - val_mae: 37.2284\n",
      "Epoch 413/1000\n",
      "16763/16763 [==============================] - 1s 55us/step - loss: 2014.0507 - mse: 2014.0511 - mae: 30.9710 - val_loss: 3069.6941 - val_mse: 3069.6941 - val_mae: 37.3769\n",
      "Epoch 414/1000\n",
      "16763/16763 [==============================] - 1s 45us/step - loss: 2033.0218 - mse: 2033.0217 - mae: 31.1854 - val_loss: 3038.1166 - val_mse: 3038.1162 - val_mae: 37.3545\n",
      "Epoch 415/1000\n",
      "16763/16763 [==============================] - 1s 46us/step - loss: 2049.6297 - mse: 2049.6299 - mae: 31.2775 - val_loss: 3041.1589 - val_mse: 3041.1584 - val_mae: 37.5433\n",
      "Epoch 416/1000\n",
      "16763/16763 [==============================] - 1s 38us/step - loss: 2023.5570 - mse: 2023.5571 - mae: 31.0089 - val_loss: 3095.0697 - val_mse: 3095.0696 - val_mae: 37.9851\n",
      "Epoch 417/1000\n",
      "16763/16763 [==============================] - 1s 38us/step - loss: 2037.8309 - mse: 2037.8307 - mae: 31.1886 - val_loss: 3054.5398 - val_mse: 3054.5400 - val_mae: 37.6060\n",
      "Epoch 418/1000\n",
      "16763/16763 [==============================] - 1s 39us/step - loss: 2027.0852 - mse: 2027.0846 - mae: 31.0545 - val_loss: 3158.7983 - val_mse: 3158.7983 - val_mae: 37.7607\n",
      "Epoch 419/1000\n",
      "16763/16763 [==============================] - 1s 47us/step - loss: 2026.8075 - mse: 2026.8074 - mae: 31.0643 - val_loss: 3073.9662 - val_mse: 3073.9656 - val_mae: 37.3116\n",
      "Epoch 420/1000\n",
      "16763/16763 [==============================] - 1s 45us/step - loss: 2020.7662 - mse: 2020.7665 - mae: 31.1176 - val_loss: 3100.3519 - val_mse: 3100.3521 - val_mae: 37.7320\n",
      "Epoch 421/1000\n",
      "16763/16763 [==============================] - 1s 45us/step - loss: 2015.5976 - mse: 2015.5979 - mae: 31.0445 - val_loss: 3014.7175 - val_mse: 3014.7173 - val_mae: 37.1483\n",
      "Epoch 422/1000\n",
      "16763/16763 [==============================] - 1s 45us/step - loss: 2021.3684 - mse: 2021.3680 - mae: 31.0630 - val_loss: 3078.2592 - val_mse: 3078.2588 - val_mae: 37.4932\n",
      "Epoch 423/1000\n",
      "16763/16763 [==============================] - 1s 46us/step - loss: 2015.9321 - mse: 2015.9327 - mae: 30.9920 - val_loss: 3108.1842 - val_mse: 3108.1843 - val_mae: 37.7161\n",
      "Epoch 424/1000\n",
      "16763/16763 [==============================] - 1s 47us/step - loss: 2036.5079 - mse: 2036.5088 - mae: 31.2808 - val_loss: 3132.1780 - val_mse: 3132.1782 - val_mae: 38.0895\n",
      "Epoch 425/1000\n",
      "16763/16763 [==============================] - 1s 74us/step - loss: 2016.9287 - mse: 2016.9287 - mae: 31.0657 - val_loss: 3112.8725 - val_mse: 3112.8730 - val_mae: 37.9556\n",
      "Epoch 426/1000\n",
      "16763/16763 [==============================] - 1s 56us/step - loss: 2007.9745 - mse: 2007.9745 - mae: 30.9425 - val_loss: 3001.1220 - val_mse: 3001.1226 - val_mae: 37.0443\n",
      "Epoch 427/1000\n",
      "16763/16763 [==============================] - 1s 66us/step - loss: 2009.6059 - mse: 2009.6067 - mae: 30.9654 - val_loss: 3026.1816 - val_mse: 3026.1826 - val_mae: 37.3155\n",
      "Epoch 428/1000\n",
      "16763/16763 [==============================] - 1s 45us/step - loss: 2028.5321 - mse: 2028.5316 - mae: 31.1691 - val_loss: 3059.9963 - val_mse: 3059.9968 - val_mae: 37.3758\n",
      "Epoch 429/1000\n",
      "16763/16763 [==============================] - 1s 40us/step - loss: 2007.2477 - mse: 2007.2479 - mae: 30.9459 - val_loss: 3107.7830 - val_mse: 3107.7832 - val_mae: 37.5168\n",
      "Epoch 430/1000\n",
      "16763/16763 [==============================] - 1s 44us/step - loss: 2003.2290 - mse: 2003.2297 - mae: 30.9713 - val_loss: 3121.1657 - val_mse: 3121.1663 - val_mae: 37.9194\n",
      "Epoch 431/1000\n",
      "16763/16763 [==============================] - 1s 40us/step - loss: 1991.8317 - mse: 1991.8309 - mae: 30.8672 - val_loss: 3083.0333 - val_mse: 3083.0330 - val_mae: 37.6091\n",
      "Epoch 432/1000\n",
      "16763/16763 [==============================] - 1s 40us/step - loss: 2011.3459 - mse: 2011.3452 - mae: 30.9841 - val_loss: 3181.9415 - val_mse: 3181.9414 - val_mae: 38.2826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 433/1000\n",
      "16763/16763 [==============================] - 1s 45us/step - loss: 2009.7391 - mse: 2009.7386 - mae: 30.9664 - val_loss: 3016.7139 - val_mse: 3016.7139 - val_mae: 37.2877\n",
      "Epoch 434/1000\n",
      "16763/16763 [==============================] - 1s 49us/step - loss: 2005.1921 - mse: 2005.1914 - mae: 30.9159 - val_loss: 3100.3518 - val_mse: 3100.3518 - val_mae: 37.6953\n",
      "Epoch 435/1000\n",
      "16763/16763 [==============================] - 1s 48us/step - loss: 1992.3913 - mse: 1992.3911 - mae: 30.8641 - val_loss: 3110.8818 - val_mse: 3110.8813 - val_mae: 38.3003\n",
      "Epoch 436/1000\n",
      "16763/16763 [==============================] - 1s 49us/step - loss: 2006.4041 - mse: 2006.4045 - mae: 30.9532 - val_loss: 3024.0038 - val_mse: 3024.0037 - val_mae: 37.3173\n",
      "Epoch 437/1000\n",
      "16763/16763 [==============================] - 1s 51us/step - loss: 2002.1977 - mse: 2002.1978 - mae: 30.8316 - val_loss: 3062.2740 - val_mse: 3062.2734 - val_mae: 37.2818\n",
      "Epoch 438/1000\n",
      "16763/16763 [==============================] - 1s 55us/step - loss: 2022.9336 - mse: 2022.9326 - mae: 31.1410 - val_loss: 3038.5390 - val_mse: 3038.5388 - val_mae: 37.3846\n",
      "Epoch 439/1000\n",
      "16763/16763 [==============================] - 1s 51us/step - loss: 1986.5354 - mse: 1986.5348 - mae: 30.8243 - val_loss: 3021.4014 - val_mse: 3021.4016 - val_mae: 37.0552\n",
      "Epoch 440/1000\n",
      "16763/16763 [==============================] - 1s 42us/step - loss: 1992.3635 - mse: 1992.3632 - mae: 30.8631 - val_loss: 3076.7547 - val_mse: 3076.7544 - val_mae: 37.6377\n",
      "Epoch 441/1000\n",
      "16763/16763 [==============================] - 1s 40us/step - loss: 1981.9777 - mse: 1981.9780 - mae: 30.7082 - val_loss: 3094.7277 - val_mse: 3094.7280 - val_mae: 37.4738\n",
      "Epoch 442/1000\n",
      "16763/16763 [==============================] - 1s 49us/step - loss: 1997.7161 - mse: 1997.7162 - mae: 30.9761 - val_loss: 3023.9525 - val_mse: 3023.9526 - val_mae: 37.4348\n",
      "Epoch 443/1000\n",
      "16763/16763 [==============================] - 1s 37us/step - loss: 1992.2519 - mse: 1992.2516 - mae: 30.8942 - val_loss: 3118.2127 - val_mse: 3118.2126 - val_mae: 38.4088\n",
      "Epoch 444/1000\n",
      "16763/16763 [==============================] - 1s 39us/step - loss: 1993.3535 - mse: 1993.3529 - mae: 30.9698 - val_loss: 3032.7223 - val_mse: 3032.7222 - val_mae: 37.3114\n",
      "Epoch 445/1000\n",
      "16763/16763 [==============================] - 1s 53us/step - loss: 1980.8017 - mse: 1980.8019 - mae: 30.7979 - val_loss: 3138.3622 - val_mse: 3138.3625 - val_mae: 37.9656\n",
      "Epoch 446/1000\n",
      "16763/16763 [==============================] - 1s 54us/step - loss: 1985.4338 - mse: 1985.4336 - mae: 30.7573 - val_loss: 3062.1258 - val_mse: 3062.1257 - val_mae: 37.5141\n",
      "Epoch 447/1000\n",
      "16763/16763 [==============================] - 1s 55us/step - loss: 1982.1302 - mse: 1982.1295 - mae: 30.8324 - val_loss: 3047.3403 - val_mse: 3047.3408 - val_mae: 37.3662\n",
      "Epoch 448/1000\n",
      "16763/16763 [==============================] - 1s 70us/step - loss: 1991.2776 - mse: 1991.2777 - mae: 30.9279 - val_loss: 3094.9777 - val_mse: 3094.9773 - val_mae: 37.4526\n",
      "Epoch 449/1000\n",
      "16763/16763 [==============================] - 1s 56us/step - loss: 2005.1288 - mse: 2005.1292 - mae: 31.0105 - val_loss: 3023.6475 - val_mse: 3023.6472 - val_mae: 37.1342\n",
      "Epoch 450/1000\n",
      "16763/16763 [==============================] - 1s 48us/step - loss: 1970.9821 - mse: 1970.9829 - mae: 30.7616 - val_loss: 3077.0080 - val_mse: 3077.0081 - val_mae: 37.3952\n",
      "Epoch 451/1000\n",
      "16763/16763 [==============================] - 1s 55us/step - loss: 1994.4220 - mse: 1994.4211 - mae: 30.8569 - val_loss: 3129.6207 - val_mse: 3129.6208 - val_mae: 37.9168\n",
      "Epoch 452/1000\n",
      "16763/16763 [==============================] - 1s 56us/step - loss: 1974.4644 - mse: 1974.4641 - mae: 30.7895 - val_loss: 3075.8749 - val_mse: 3075.8750 - val_mae: 37.9970\n",
      "Epoch 453/1000\n",
      "16763/16763 [==============================] - 1s 54us/step - loss: 1968.8609 - mse: 1968.8612 - mae: 30.7107 - val_loss: 3094.2524 - val_mse: 3094.2520 - val_mae: 37.9121\n",
      "Epoch 454/1000\n",
      "16763/16763 [==============================] - 1s 54us/step - loss: 1994.3635 - mse: 1994.3636 - mae: 30.9977 - val_loss: 3188.5876 - val_mse: 3188.5876 - val_mae: 37.7808\n",
      "Epoch 455/1000\n",
      "16763/16763 [==============================] - 1s 43us/step - loss: 1975.1254 - mse: 1975.1259 - mae: 30.7407 - val_loss: 3049.7921 - val_mse: 3049.7920 - val_mae: 37.5159\n",
      "Epoch 456/1000\n",
      "16763/16763 [==============================] - 1s 46us/step - loss: 1981.6220 - mse: 1981.6227 - mae: 30.7836 - val_loss: 3026.9439 - val_mse: 3026.9438 - val_mae: 37.5360\n",
      "Epoch 457/1000\n",
      "16763/16763 [==============================] - 1s 42us/step - loss: 1957.8939 - mse: 1957.8939 - mae: 30.5569 - val_loss: 3014.4252 - val_mse: 3014.4253 - val_mae: 37.1210\n",
      "Epoch 458/1000\n",
      "16763/16763 [==============================] - 1s 46us/step - loss: 1970.3690 - mse: 1970.3691 - mae: 30.7788 - val_loss: 3156.9033 - val_mse: 3156.9026 - val_mae: 38.0704\n",
      "Epoch 459/1000\n",
      "16763/16763 [==============================] - 1s 48us/step - loss: 1958.5252 - mse: 1958.5248 - mae: 30.6203 - val_loss: 3120.9781 - val_mse: 3120.9785 - val_mae: 37.7783\n",
      "Epoch 460/1000\n",
      "16763/16763 [==============================] - 1s 54us/step - loss: 1955.4213 - mse: 1955.4218 - mae: 30.5835 - val_loss: 3107.4866 - val_mse: 3107.4866 - val_mae: 38.2024\n",
      "Epoch 461/1000\n",
      "16763/16763 [==============================] - 1s 59us/step - loss: 1977.8856 - mse: 1977.8856 - mae: 30.8671 - val_loss: 3040.2697 - val_mse: 3040.2708 - val_mae: 37.6154\n",
      "Epoch 462/1000\n",
      "16763/16763 [==============================] - 1s 50us/step - loss: 1958.6731 - mse: 1958.6726 - mae: 30.7508 - val_loss: 3082.2340 - val_mse: 3082.2346 - val_mae: 37.6110\n",
      "Epoch 463/1000\n",
      "16763/16763 [==============================] - 1s 49us/step - loss: 1964.7945 - mse: 1964.7947 - mae: 30.7763 - val_loss: 3106.1106 - val_mse: 3106.1106 - val_mae: 37.8789\n",
      "Epoch 464/1000\n",
      "16763/16763 [==============================] - 1s 46us/step - loss: 1953.8936 - mse: 1953.8937 - mae: 30.6134 - val_loss: 3075.0190 - val_mse: 3075.0193 - val_mae: 37.3152\n",
      "Epoch 465/1000\n",
      "16763/16763 [==============================] - 1s 43us/step - loss: 1942.1627 - mse: 1942.1633 - mae: 30.5764 - val_loss: 3110.1958 - val_mse: 3110.1951 - val_mae: 38.0296\n",
      "Epoch 466/1000\n",
      "16763/16763 [==============================] - 1s 44us/step - loss: 1950.3735 - mse: 1950.3726 - mae: 30.6950 - val_loss: 3057.3323 - val_mse: 3057.3323 - val_mae: 37.8819\n",
      "Epoch 467/1000\n",
      "16763/16763 [==============================] - 1s 42us/step - loss: 1956.3487 - mse: 1956.3495 - mae: 30.6792 - val_loss: 3068.7500 - val_mse: 3068.7500 - val_mae: 37.5528\n",
      "Epoch 468/1000\n",
      "16763/16763 [==============================] - 1s 44us/step - loss: 1950.9076 - mse: 1950.9073 - mae: 30.6733 - val_loss: 3140.8360 - val_mse: 3140.8369 - val_mae: 37.8758\n",
      "Epoch 469/1000\n",
      "16763/16763 [==============================] - 1s 48us/step - loss: 1946.1587 - mse: 1946.1591 - mae: 30.5913 - val_loss: 3032.9361 - val_mse: 3032.9355 - val_mae: 37.2709\n",
      "Epoch 470/1000\n",
      "16763/16763 [==============================] - 1s 57us/step - loss: 1968.9950 - mse: 1968.9949 - mae: 30.7627 - val_loss: 3018.7056 - val_mse: 3018.7051 - val_mae: 37.1746\n",
      "Epoch 471/1000\n",
      "16763/16763 [==============================] - 1s 53us/step - loss: 1970.6861 - mse: 1970.6873 - mae: 30.9047 - val_loss: 3034.6336 - val_mse: 3034.6335 - val_mae: 37.5801\n",
      "Epoch 472/1000\n",
      "16763/16763 [==============================] - 1s 57us/step - loss: 1937.1388 - mse: 1937.1378 - mae: 30.5133 - val_loss: 3072.2757 - val_mse: 3072.2756 - val_mae: 37.7099\n",
      "Epoch 473/1000\n",
      "16763/16763 [==============================] - 1s 64us/step - loss: 1960.1516 - mse: 1960.1520 - mae: 30.8238 - val_loss: 3028.4592 - val_mse: 3028.4585 - val_mae: 37.0215\n",
      "Epoch 474/1000\n",
      "16763/16763 [==============================] - 1s 89us/step - loss: 1944.2795 - mse: 1944.2793 - mae: 30.5870 - val_loss: 3066.9008 - val_mse: 3066.9009 - val_mae: 38.2688\n",
      "Epoch 475/1000\n",
      "16763/16763 [==============================] - 1s 65us/step - loss: 1943.3723 - mse: 1943.3727 - mae: 30.6194 - val_loss: 3084.9406 - val_mse: 3084.9402 - val_mae: 37.6755\n",
      "Epoch 476/1000\n",
      "16763/16763 [==============================] - 1s 45us/step - loss: 1936.6276 - mse: 1936.6282 - mae: 30.6030 - val_loss: 3046.5481 - val_mse: 3046.5481 - val_mae: 37.1287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 477/1000\n",
      "16763/16763 [==============================] - 1s 51us/step - loss: 1945.9820 - mse: 1945.9823 - mae: 30.6628 - val_loss: 3121.5547 - val_mse: 3121.5544 - val_mae: 38.4424\n",
      "Epoch 478/1000\n",
      "16763/16763 [==============================] - 1s 82us/step - loss: 1943.3213 - mse: 1943.3214 - mae: 30.6331 - val_loss: 3033.0617 - val_mse: 3033.0615 - val_mae: 37.3795\n",
      "Epoch 479/1000\n",
      "16763/16763 [==============================] - 1s 68us/step - loss: 1958.5236 - mse: 1958.5231 - mae: 30.8138 - val_loss: 3043.1726 - val_mse: 3043.1731 - val_mae: 37.3486\n",
      "Epoch 480/1000\n",
      "16763/16763 [==============================] - 1s 46us/step - loss: 1932.5111 - mse: 1932.5117 - mae: 30.4685 - val_loss: 3128.4925 - val_mse: 3128.4919 - val_mae: 37.9208\n",
      "Epoch 481/1000\n",
      "16763/16763 [==============================] - 1s 41us/step - loss: 1930.3673 - mse: 1930.3665 - mae: 30.4990 - val_loss: 3061.9161 - val_mse: 3061.9165 - val_mae: 37.6272\n",
      "Epoch 482/1000\n",
      "16763/16763 [==============================] - 1s 38us/step - loss: 1933.7207 - mse: 1933.7211 - mae: 30.5398 - val_loss: 3015.6404 - val_mse: 3015.6406 - val_mae: 37.0309\n",
      "Epoch 483/1000\n",
      "16763/16763 [==============================] - 1s 41us/step - loss: 1934.3690 - mse: 1934.3690 - mae: 30.4957 - val_loss: 3023.9183 - val_mse: 3023.9182 - val_mae: 37.4024\n",
      "Epoch 484/1000\n",
      "16763/16763 [==============================] - 1s 38us/step - loss: 1928.2517 - mse: 1928.2513 - mae: 30.5397 - val_loss: 3111.8669 - val_mse: 3111.8672 - val_mae: 37.9002\n",
      "Epoch 485/1000\n",
      "16763/16763 [==============================] - 1s 46us/step - loss: 1917.6879 - mse: 1917.6874 - mae: 30.3681 - val_loss: 3085.1087 - val_mse: 3085.1091 - val_mae: 38.5127\n",
      "Epoch 486/1000\n",
      "16763/16763 [==============================] - 1s 48us/step - loss: 1938.3164 - mse: 1938.3156 - mae: 30.7281 - val_loss: 3031.1391 - val_mse: 3031.1389 - val_mae: 37.4609\n",
      "Epoch 487/1000\n",
      "16763/16763 [==============================] - 1s 50us/step - loss: 1928.1526 - mse: 1928.1527 - mae: 30.5286 - val_loss: 3088.0102 - val_mse: 3088.0100 - val_mae: 37.6809\n",
      "Epoch 488/1000\n",
      "16763/16763 [==============================] - 1s 51us/step - loss: 1928.0476 - mse: 1928.0482 - mae: 30.5165 - val_loss: 3211.8662 - val_mse: 3211.8660 - val_mae: 38.7015\n",
      "Epoch 489/1000\n",
      "16763/16763 [==============================] - 1s 53us/step - loss: 1929.8016 - mse: 1929.8016 - mae: 30.5307 - val_loss: 3006.9042 - val_mse: 3006.9045 - val_mae: 37.2988\n",
      "Epoch 490/1000\n",
      "16763/16763 [==============================] - 1s 53us/step - loss: 1919.9337 - mse: 1919.9330 - mae: 30.4062 - val_loss: 3110.1096 - val_mse: 3110.1094 - val_mae: 37.7030\n",
      "Epoch 491/1000\n",
      "16763/16763 [==============================] - 1s 56us/step - loss: 1916.9470 - mse: 1916.9469 - mae: 30.4055 - val_loss: 3103.1220 - val_mse: 3103.1221 - val_mae: 37.8635\n",
      "Epoch 492/1000\n",
      "16763/16763 [==============================] - 1s 56us/step - loss: 1932.5535 - mse: 1932.5536 - mae: 30.5617 - val_loss: 3077.7855 - val_mse: 3077.7854 - val_mae: 37.4787\n",
      "Epoch 493/1000\n",
      "16763/16763 [==============================] - 1s 56us/step - loss: 1925.2190 - mse: 1925.2195 - mae: 30.5239 - val_loss: 3078.4286 - val_mse: 3078.4282 - val_mae: 37.5341\n",
      "Epoch 494/1000\n",
      "16763/16763 [==============================] - 1s 71us/step - loss: 1930.8192 - mse: 1930.8185 - mae: 30.4914 - val_loss: 3032.7473 - val_mse: 3032.7473 - val_mae: 37.5008\n",
      "Epoch 495/1000\n",
      "16763/16763 [==============================] - 1s 55us/step - loss: 1918.1414 - mse: 1918.1410 - mae: 30.4070 - val_loss: 3112.7045 - val_mse: 3112.7041 - val_mae: 38.0065\n",
      "Epoch 496/1000\n",
      "16763/16763 [==============================] - 1s 58us/step - loss: 1917.6347 - mse: 1917.6343 - mae: 30.4855 - val_loss: 3044.6709 - val_mse: 3044.6707 - val_mae: 37.2650\n",
      "Epoch 497/1000\n",
      "16763/16763 [==============================] - 1s 53us/step - loss: 1916.8218 - mse: 1916.8225 - mae: 30.4793 - val_loss: 3134.7969 - val_mse: 3134.7964 - val_mae: 38.2262\n",
      "Epoch 498/1000\n",
      "16763/16763 [==============================] - 1s 48us/step - loss: 1912.1475 - mse: 1912.1483 - mae: 30.3743 - val_loss: 3054.7773 - val_mse: 3054.7776 - val_mae: 37.2721\n",
      "Epoch 499/1000\n",
      "16763/16763 [==============================] - 1s 41us/step - loss: 1909.5328 - mse: 1909.5330 - mae: 30.3251 - val_loss: 3060.6847 - val_mse: 3060.6848 - val_mae: 37.8746\n",
      "Epoch 500/1000\n",
      "16763/16763 [==============================] - 1s 43us/step - loss: 1912.3746 - mse: 1912.3748 - mae: 30.4473 - val_loss: 3026.7933 - val_mse: 3026.7937 - val_mae: 37.5331\n",
      "Epoch 501/1000\n",
      "16763/16763 [==============================] - 1s 45us/step - loss: 1916.0471 - mse: 1916.0466 - mae: 30.4558 - val_loss: 3022.2214 - val_mse: 3022.2212 - val_mae: 37.3640\n",
      "Epoch 502/1000\n",
      "16763/16763 [==============================] - 1s 44us/step - loss: 1902.6584 - mse: 1902.6589 - mae: 30.2927 - val_loss: 3014.2410 - val_mse: 3014.2415 - val_mae: 37.1315\n",
      "Epoch 503/1000\n",
      "16763/16763 [==============================] - 1s 57us/step - loss: 1896.5754 - mse: 1896.5754 - mae: 30.1480 - val_loss: 3066.9404 - val_mse: 3066.9399 - val_mae: 37.3114\n",
      "Epoch 504/1000\n",
      "16763/16763 [==============================] - 1s 53us/step - loss: 1910.5966 - mse: 1910.5964 - mae: 30.3691 - val_loss: 2982.4298 - val_mse: 2982.4294 - val_mae: 36.9397\n",
      "Epoch 505/1000\n",
      "16763/16763 [==============================] - 1s 56us/step - loss: 1915.5208 - mse: 1915.5208 - mae: 30.5024 - val_loss: 3054.0171 - val_mse: 3054.0171 - val_mae: 37.2750\n",
      "Epoch 506/1000\n",
      "16763/16763 [==============================] - 1s 57us/step - loss: 1926.8822 - mse: 1926.8823 - mae: 30.6168 - val_loss: 3069.6101 - val_mse: 3069.6099 - val_mae: 37.8976\n",
      "Epoch 507/1000\n",
      "16763/16763 [==============================] - 1s 53us/step - loss: 1894.0616 - mse: 1894.0613 - mae: 30.3185 - val_loss: 3025.2445 - val_mse: 3025.2444 - val_mae: 37.4740\n",
      "Epoch 508/1000\n",
      "16763/16763 [==============================] - 1s 61us/step - loss: 1892.8058 - mse: 1892.8059 - mae: 30.1244 - val_loss: 3038.4446 - val_mse: 3038.4443 - val_mae: 37.2876\n",
      "Epoch 509/1000\n",
      "16763/16763 [==============================] - 1s 64us/step - loss: 1903.2770 - mse: 1903.2762 - mae: 30.4365 - val_loss: 3069.4727 - val_mse: 3069.4731 - val_mae: 37.9124\n",
      "Epoch 510/1000\n",
      "16763/16763 [==============================] - 1s 65us/step - loss: 1909.4392 - mse: 1909.4396 - mae: 30.4629 - val_loss: 3051.9777 - val_mse: 3051.9771 - val_mae: 37.5566\n",
      "Epoch 511/1000\n",
      "16763/16763 [==============================] - 1s 65us/step - loss: 1913.4717 - mse: 1913.4716 - mae: 30.4597 - val_loss: 3053.4936 - val_mse: 3053.4932 - val_mae: 38.0200\n",
      "Epoch 512/1000\n",
      "16763/16763 [==============================] - 1s 60us/step - loss: 1900.6502 - mse: 1900.6504 - mae: 30.3293 - val_loss: 3057.4914 - val_mse: 3057.4915 - val_mae: 37.7416\n",
      "Epoch 513/1000\n",
      "16763/16763 [==============================] - 1s 51us/step - loss: 1887.0937 - mse: 1887.0935 - mae: 30.1986 - val_loss: 3149.4009 - val_mse: 3149.4011 - val_mae: 37.7293\n",
      "Epoch 514/1000\n",
      "16763/16763 [==============================] - 1s 60us/step - loss: 1893.2136 - mse: 1893.2137 - mae: 30.2572 - val_loss: 3017.8399 - val_mse: 3017.8401 - val_mae: 37.1780\n",
      "Epoch 515/1000\n",
      "16763/16763 [==============================] - 1s 51us/step - loss: 1912.9800 - mse: 1912.9803 - mae: 30.4396 - val_loss: 3067.5424 - val_mse: 3067.5425 - val_mae: 37.4505\n",
      "Epoch 516/1000\n",
      "16763/16763 [==============================] - 1s 57us/step - loss: 1889.8275 - mse: 1889.8281 - mae: 30.2211 - val_loss: 3097.0658 - val_mse: 3097.0662 - val_mae: 38.0731\n",
      "Epoch 517/1000\n",
      "16763/16763 [==============================] - 1s 51us/step - loss: 1894.3816 - mse: 1894.3811 - mae: 30.3209 - val_loss: 3040.7816 - val_mse: 3040.7815 - val_mae: 37.6264\n",
      "Epoch 518/1000\n",
      "16763/16763 [==============================] - 1s 57us/step - loss: 1888.8017 - mse: 1888.8022 - mae: 30.3482 - val_loss: 3114.8316 - val_mse: 3114.8323 - val_mae: 37.7092\n",
      "Epoch 519/1000\n",
      "16763/16763 [==============================] - 1s 67us/step - loss: 1898.5545 - mse: 1898.5542 - mae: 30.3378 - val_loss: 3046.6428 - val_mse: 3046.6428 - val_mae: 38.2391\n",
      "Epoch 520/1000\n",
      "16763/16763 [==============================] - 1s 56us/step - loss: 1882.0012 - mse: 1882.0015 - mae: 30.1975 - val_loss: 3074.2538 - val_mse: 3074.2534 - val_mae: 37.4253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 521/1000\n",
      "16763/16763 [==============================] - 1s 48us/step - loss: 1898.1789 - mse: 1898.1786 - mae: 30.2638 - val_loss: 3068.4730 - val_mse: 3068.4729 - val_mae: 37.6822\n",
      "Epoch 522/1000\n",
      "16763/16763 [==============================] - 1s 43us/step - loss: 1891.2590 - mse: 1891.2587 - mae: 30.2959 - val_loss: 3000.4145 - val_mse: 3000.4150 - val_mae: 37.2283\n",
      "Epoch 523/1000\n",
      "16763/16763 [==============================] - 1s 42us/step - loss: 1887.0262 - mse: 1887.0255 - mae: 30.2148 - val_loss: 3061.4970 - val_mse: 3061.4971 - val_mae: 37.2414\n",
      "Epoch 524/1000\n",
      "16763/16763 [==============================] - 1s 46us/step - loss: 1880.9300 - mse: 1880.9303 - mae: 30.2276 - val_loss: 2995.4273 - val_mse: 2995.4265 - val_mae: 36.9870\n",
      "Epoch 525/1000\n",
      "16763/16763 [==============================] - 1s 44us/step - loss: 1887.5105 - mse: 1887.5106 - mae: 30.2106 - val_loss: 3061.9327 - val_mse: 3061.9326 - val_mae: 37.4535\n",
      "Epoch 526/1000\n",
      "16763/16763 [==============================] - 1s 47us/step - loss: 1869.9898 - mse: 1869.9897 - mae: 30.0231 - val_loss: 3080.3279 - val_mse: 3080.3279 - val_mae: 37.8423\n",
      "Epoch 527/1000\n",
      "16763/16763 [==============================] - 1s 60us/step - loss: 1877.3406 - mse: 1877.3413 - mae: 30.1533 - val_loss: 3140.8358 - val_mse: 3140.8357 - val_mae: 38.0144\n",
      "Epoch 528/1000\n",
      "16763/16763 [==============================] - 1s 62us/step - loss: 1885.6447 - mse: 1885.6450 - mae: 30.2466 - val_loss: 3081.7951 - val_mse: 3081.7952 - val_mae: 37.5950\n",
      "Epoch 529/1000\n",
      "16763/16763 [==============================] - 1s 66us/step - loss: 1883.6961 - mse: 1883.6960 - mae: 30.2146 - val_loss: 3004.0501 - val_mse: 3004.0493 - val_mae: 36.9499\n",
      "Epoch 530/1000\n",
      "16763/16763 [==============================] - 1s 63us/step - loss: 1888.0610 - mse: 1888.0619 - mae: 30.2324 - val_loss: 2994.2798 - val_mse: 2994.2800 - val_mae: 37.0523\n",
      "Epoch 531/1000\n",
      "16763/16763 [==============================] - 1s 61us/step - loss: 1863.7436 - mse: 1863.7427 - mae: 30.1055 - val_loss: 3040.5080 - val_mse: 3040.5078 - val_mae: 37.4382\n",
      "Epoch 532/1000\n",
      "16763/16763 [==============================] - 1s 74us/step - loss: 1867.7434 - mse: 1867.7422 - mae: 30.2445 - val_loss: 3076.2167 - val_mse: 3076.2173 - val_mae: 37.5742\n",
      "Epoch 533/1000\n",
      "16763/16763 [==============================] - 1s 70us/step - loss: 1867.6919 - mse: 1867.6921 - mae: 30.0944 - val_loss: 3131.8563 - val_mse: 3131.8562 - val_mae: 38.7685\n",
      "Epoch 534/1000\n",
      "16763/16763 [==============================] - 1s 66us/step - loss: 1872.0256 - mse: 1872.0254 - mae: 30.1702 - val_loss: 3041.0491 - val_mse: 3041.0496 - val_mae: 37.1817\n",
      "Epoch 535/1000\n",
      "16763/16763 [==============================] - 1s 62us/step - loss: 1875.1488 - mse: 1875.1481 - mae: 30.1478 - val_loss: 3069.4743 - val_mse: 3069.4746 - val_mae: 37.4846\n",
      "Epoch 536/1000\n",
      "16763/16763 [==============================] - 1s 54us/step - loss: 1868.7201 - mse: 1868.7201 - mae: 30.1004 - val_loss: 3032.8921 - val_mse: 3032.8918 - val_mae: 37.3858\n",
      "Epoch 537/1000\n",
      "16763/16763 [==============================] - 1s 54us/step - loss: 1874.1481 - mse: 1874.1482 - mae: 30.2076 - val_loss: 3052.9091 - val_mse: 3052.9094 - val_mae: 37.4697\n",
      "Epoch 538/1000\n",
      "16763/16763 [==============================] - 1s 49us/step - loss: 1874.2140 - mse: 1874.2133 - mae: 30.1449 - val_loss: 3054.4343 - val_mse: 3054.4341 - val_mae: 37.2719\n",
      "Epoch 539/1000\n",
      "16763/16763 [==============================] - 1s 45us/step - loss: 1865.1142 - mse: 1865.1146 - mae: 30.1216 - val_loss: 3013.2276 - val_mse: 3013.2280 - val_mae: 37.2378\n",
      "Epoch 540/1000\n",
      "16763/16763 [==============================] - 1s 44us/step - loss: 1862.5364 - mse: 1862.5360 - mae: 30.0426 - val_loss: 3046.7729 - val_mse: 3046.7727 - val_mae: 37.9889\n",
      "Epoch 541/1000\n",
      "16763/16763 [==============================] - 1s 52us/step - loss: 1854.8773 - mse: 1854.8777 - mae: 30.0542 - val_loss: 3078.4574 - val_mse: 3078.4575 - val_mae: 37.7950\n",
      "Epoch 542/1000\n",
      "16763/16763 [==============================] - 1s 46us/step - loss: 1866.0489 - mse: 1866.0490 - mae: 30.0632 - val_loss: 3041.0565 - val_mse: 3041.0571 - val_mae: 37.4385\n",
      "Epoch 543/1000\n",
      "16763/16763 [==============================] - 1s 64us/step - loss: 1862.0492 - mse: 1862.0493 - mae: 30.0977 - val_loss: 3100.2147 - val_mse: 3100.2158 - val_mae: 38.3489\n",
      "Epoch 544/1000\n",
      "16763/16763 [==============================] - 1s 61us/step - loss: 1863.5196 - mse: 1863.5194 - mae: 30.1036 - val_loss: 3022.2301 - val_mse: 3022.2295 - val_mae: 37.1438\n",
      "Epoch 545/1000\n",
      "16763/16763 [==============================] - 1s 51us/step - loss: 1845.1509 - mse: 1845.1508 - mae: 29.9176 - val_loss: 3021.6626 - val_mse: 3021.6631 - val_mae: 37.2430\n",
      "Epoch 546/1000\n",
      "16763/16763 [==============================] - 1s 47us/step - loss: 1862.3087 - mse: 1862.3091 - mae: 30.1129 - val_loss: 3015.3735 - val_mse: 3015.3738 - val_mae: 37.3099\n",
      "Epoch 547/1000\n",
      "16763/16763 [==============================] - 1s 45us/step - loss: 1838.5949 - mse: 1838.5951 - mae: 29.8145 - val_loss: 3151.1362 - val_mse: 3151.1370 - val_mae: 37.7416\n",
      "Epoch 548/1000\n",
      "16763/16763 [==============================] - 1s 47us/step - loss: 1847.8417 - mse: 1847.8416 - mae: 29.9162 - val_loss: 3056.3241 - val_mse: 3056.3235 - val_mae: 37.5616\n",
      "Epoch 549/1000\n",
      "16763/16763 [==============================] - 1s 46us/step - loss: 1850.1586 - mse: 1850.1588 - mae: 29.9705 - val_loss: 3067.8213 - val_mse: 3067.8215 - val_mae: 37.6633\n",
      "Epoch 550/1000\n",
      "16763/16763 [==============================] - 1s 46us/step - loss: 1863.1077 - mse: 1863.1088 - mae: 30.1125 - val_loss: 3023.7596 - val_mse: 3023.7598 - val_mae: 37.3836\n",
      "Epoch 551/1000\n",
      "16763/16763 [==============================] - 1s 48us/step - loss: 1846.6610 - mse: 1846.6608 - mae: 29.9478 - val_loss: 3141.4731 - val_mse: 3141.4727 - val_mae: 37.9639\n",
      "Epoch 552/1000\n",
      "16763/16763 [==============================] - 1s 55us/step - loss: 1843.7468 - mse: 1843.7474 - mae: 29.9192 - val_loss: 3057.3531 - val_mse: 3057.3530 - val_mae: 37.7161\n",
      "Epoch 553/1000\n",
      "16763/16763 [==============================] - 1s 65us/step - loss: 1844.9597 - mse: 1844.9595 - mae: 29.9368 - val_loss: 3091.8258 - val_mse: 3091.8257 - val_mae: 37.6395\n",
      "Epoch 554/1000\n",
      "16763/16763 [==============================] - 1s 58us/step - loss: 1846.9175 - mse: 1846.9170 - mae: 29.9182 - val_loss: 3083.7444 - val_mse: 3083.7449 - val_mae: 37.4035\n",
      "Epoch 555/1000\n",
      "16763/16763 [==============================] - 1s 46us/step - loss: 1828.2462 - mse: 1828.2456 - mae: 29.7355 - val_loss: 2968.5870 - val_mse: 2968.5867 - val_mae: 36.9831\n",
      "Epoch 556/1000\n",
      "16763/16763 [==============================] - 1s 48us/step - loss: 1830.5446 - mse: 1830.5447 - mae: 29.8717 - val_loss: 2998.9059 - val_mse: 2998.9060 - val_mae: 37.2881\n",
      "Epoch 557/1000\n",
      "16763/16763 [==============================] - 1s 64us/step - loss: 1835.9951 - mse: 1835.9948 - mae: 29.8104 - val_loss: 3059.4005 - val_mse: 3059.4001 - val_mae: 37.2062\n",
      "Epoch 558/1000\n",
      "16763/16763 [==============================] - 1s 53us/step - loss: 1846.2755 - mse: 1846.2754 - mae: 30.0428 - val_loss: 3009.7190 - val_mse: 3009.7192 - val_mae: 37.3098\n",
      "Epoch 559/1000\n",
      "16763/16763 [==============================] - 1s 38us/step - loss: 1835.8238 - mse: 1835.8240 - mae: 29.9016 - val_loss: 3013.2312 - val_mse: 3013.2314 - val_mae: 37.1338\n",
      "Epoch 560/1000\n",
      "16763/16763 [==============================] - 1s 39us/step - loss: 1841.7854 - mse: 1841.7859 - mae: 29.9061 - val_loss: 3155.9788 - val_mse: 3155.9780 - val_mae: 38.9501\n",
      "Epoch 561/1000\n",
      "16763/16763 [==============================] - 1s 40us/step - loss: 1839.7747 - mse: 1839.7753 - mae: 29.9179 - val_loss: 3069.9214 - val_mse: 3069.9214 - val_mae: 37.6968\n",
      "Epoch 562/1000\n",
      "16763/16763 [==============================] - 1s 40us/step - loss: 1839.9129 - mse: 1839.9131 - mae: 29.9011 - val_loss: 3038.7698 - val_mse: 3038.7695 - val_mae: 37.3430\n",
      "Epoch 563/1000\n",
      "16763/16763 [==============================] - 1s 44us/step - loss: 1836.8495 - mse: 1836.8506 - mae: 30.0074 - val_loss: 3038.0994 - val_mse: 3038.0999 - val_mae: 37.2613\n",
      "Epoch 564/1000\n",
      "16763/16763 [==============================] - 1s 46us/step - loss: 1842.6614 - mse: 1842.6615 - mae: 29.9264 - val_loss: 3022.7814 - val_mse: 3022.7820 - val_mae: 37.2234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 565/1000\n",
      "16763/16763 [==============================] - 1s 45us/step - loss: 1848.8622 - mse: 1848.8627 - mae: 29.9790 - val_loss: 3000.8086 - val_mse: 3000.8086 - val_mae: 37.4417\n",
      "Epoch 566/1000\n",
      "16763/16763 [==============================] - 1s 47us/step - loss: 1827.2590 - mse: 1827.2596 - mae: 29.7822 - val_loss: 3035.7780 - val_mse: 3035.7773 - val_mae: 37.3641\n",
      "Epoch 567/1000\n",
      "16763/16763 [==============================] - 1s 54us/step - loss: 1834.6741 - mse: 1834.6738 - mae: 30.0296 - val_loss: 3021.6188 - val_mse: 3021.6187 - val_mae: 37.3110\n",
      "Epoch 568/1000\n",
      "16763/16763 [==============================] - 1s 59us/step - loss: 1851.4489 - mse: 1851.4486 - mae: 30.1520 - val_loss: 2987.6299 - val_mse: 2987.6301 - val_mae: 36.9019\n",
      "Epoch 569/1000\n",
      "16763/16763 [==============================] - 1s 39us/step - loss: 1836.7161 - mse: 1836.7172 - mae: 29.8619 - val_loss: 3046.5467 - val_mse: 3046.5469 - val_mae: 37.7615\n",
      "Epoch 570/1000\n",
      "16763/16763 [==============================] - 1s 38us/step - loss: 1826.5364 - mse: 1826.5369 - mae: 29.8259 - val_loss: 3061.2944 - val_mse: 3061.2949 - val_mae: 37.4901\n",
      "Epoch 571/1000\n",
      "16763/16763 [==============================] - 1s 37us/step - loss: 1827.1222 - mse: 1827.1229 - mae: 29.8291 - val_loss: 3032.5464 - val_mse: 3032.5461 - val_mae: 37.4729\n",
      "Epoch 572/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 1840.4006 - mse: 1840.4000 - mae: 30.0632 - val_loss: 2984.1000 - val_mse: 2984.1001 - val_mae: 37.1105\n",
      "Epoch 573/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 1821.8901 - mse: 1821.8904 - mae: 29.8569 - val_loss: 3146.5723 - val_mse: 3146.5718 - val_mae: 38.5201\n",
      "Epoch 574/1000\n",
      "16763/16763 [==============================] - 1s 37us/step - loss: 1827.4547 - mse: 1827.4547 - mae: 29.8557 - val_loss: 3025.9984 - val_mse: 3025.9978 - val_mae: 37.5978\n",
      "Epoch 575/1000\n",
      "16763/16763 [==============================] - 1s 55us/step - loss: 1833.1588 - mse: 1833.1586 - mae: 29.8911 - val_loss: 3002.6721 - val_mse: 3002.6724 - val_mae: 37.2335\n",
      "Epoch 576/1000\n",
      "16763/16763 [==============================] - 1s 53us/step - loss: 1816.3956 - mse: 1816.3960 - mae: 29.7328 - val_loss: 3140.3711 - val_mse: 3140.3706 - val_mae: 38.4878\n",
      "Epoch 577/1000\n",
      "16763/16763 [==============================] - 1s 53us/step - loss: 1825.2608 - mse: 1825.2612 - mae: 29.8847 - val_loss: 2968.4859 - val_mse: 2968.4861 - val_mae: 36.7796\n",
      "Epoch 578/1000\n",
      "16763/16763 [==============================] - 1s 47us/step - loss: 1801.5755 - mse: 1801.5757 - mae: 29.6224 - val_loss: 3071.1231 - val_mse: 3071.1233 - val_mae: 37.3847\n",
      "Epoch 579/1000\n",
      "16763/16763 [==============================] - 1s 47us/step - loss: 1811.1283 - mse: 1811.1288 - mae: 29.7359 - val_loss: 2988.8058 - val_mse: 2988.8057 - val_mae: 37.0251\n",
      "Epoch 580/1000\n",
      "16763/16763 [==============================] - 1s 46us/step - loss: 1814.9086 - mse: 1814.9082 - mae: 29.7961 - val_loss: 3028.9035 - val_mse: 3028.9026 - val_mae: 37.1773\n",
      "Epoch 581/1000\n",
      "16763/16763 [==============================] - 1s 46us/step - loss: 1822.3252 - mse: 1822.3246 - mae: 29.8557 - val_loss: 2986.9848 - val_mse: 2986.9851 - val_mae: 37.3544\n",
      "Epoch 582/1000\n",
      "16763/16763 [==============================] - 1s 49us/step - loss: 1835.8103 - mse: 1835.8096 - mae: 29.9011 - val_loss: 3129.7646 - val_mse: 3129.7646 - val_mae: 38.1029\n",
      "Epoch 583/1000\n",
      "16763/16763 [==============================] - 1s 47us/step - loss: 1837.4405 - mse: 1837.4408 - mae: 30.0847 - val_loss: 3068.9416 - val_mse: 3068.9421 - val_mae: 37.8467\n",
      "Epoch 584/1000\n",
      "16763/16763 [==============================] - 1s 47us/step - loss: 1810.3969 - mse: 1810.3976 - mae: 29.7415 - val_loss: 2988.1039 - val_mse: 2988.1042 - val_mae: 37.3456\n",
      "Epoch 585/1000\n",
      "16763/16763 [==============================] - 1s 48us/step - loss: 1830.1158 - mse: 1830.1155 - mae: 29.9051 - val_loss: 3029.0242 - val_mse: 3029.0247 - val_mae: 37.1496\n",
      "Epoch 586/1000\n",
      "16763/16763 [==============================] - 1s 48us/step - loss: 1823.2149 - mse: 1823.2148 - mae: 29.8675 - val_loss: 3035.0848 - val_mse: 3035.0842 - val_mae: 37.2548\n",
      "Epoch 587/1000\n",
      "16763/16763 [==============================] - 1s 48us/step - loss: 1803.3274 - mse: 1803.3275 - mae: 29.6417 - val_loss: 3096.2829 - val_mse: 3096.2830 - val_mae: 38.0053\n",
      "Epoch 588/1000\n",
      "16763/16763 [==============================] - 1s 45us/step - loss: 1807.7528 - mse: 1807.7521 - mae: 29.7309 - val_loss: 3029.5653 - val_mse: 3029.5659 - val_mae: 37.1885\n",
      "Epoch 589/1000\n",
      "16763/16763 [==============================] - 1s 40us/step - loss: 1802.2706 - mse: 1802.2704 - mae: 29.7105 - val_loss: 3099.5519 - val_mse: 3099.5522 - val_mae: 37.9388\n",
      "Epoch 590/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 1827.2762 - mse: 1827.2760 - mae: 29.9489 - val_loss: 3075.1174 - val_mse: 3075.1174 - val_mae: 37.6695\n",
      "Epoch 591/1000\n",
      "16763/16763 [==============================] - 1s 37us/step - loss: 1819.6129 - mse: 1819.6128 - mae: 29.7322 - val_loss: 3080.2452 - val_mse: 3080.2451 - val_mae: 37.6759\n",
      "Epoch 592/1000\n",
      "16763/16763 [==============================] - 1s 37us/step - loss: 1799.8779 - mse: 1799.8767 - mae: 29.6879 - val_loss: 3020.2609 - val_mse: 3020.2610 - val_mae: 37.2655\n",
      "Epoch 593/1000\n",
      "16763/16763 [==============================] - 1s 37us/step - loss: 1814.8263 - mse: 1814.8260 - mae: 29.7497 - val_loss: 2976.4797 - val_mse: 2976.4795 - val_mae: 36.9262\n",
      "Epoch 594/1000\n",
      "16763/16763 [==============================] - 1s 44us/step - loss: 1798.2848 - mse: 1798.2843 - mae: 29.6800 - val_loss: 2990.8211 - val_mse: 2990.8213 - val_mae: 37.0031\n",
      "Epoch 595/1000\n",
      "16763/16763 [==============================] - 1s 68us/step - loss: 1799.2498 - mse: 1799.2505 - mae: 29.7373 - val_loss: 3038.8564 - val_mse: 3038.8560 - val_mae: 37.4235\n",
      "Epoch 596/1000\n",
      "16763/16763 [==============================] - 1s 74us/step - loss: 1797.0338 - mse: 1797.0336 - mae: 29.6804 - val_loss: 3042.2072 - val_mse: 3042.2070 - val_mae: 37.3771\n",
      "Epoch 597/1000\n",
      "16763/16763 [==============================] - 1s 52us/step - loss: 1800.7760 - mse: 1800.7765 - mae: 29.7320 - val_loss: 3094.0336 - val_mse: 3094.0330 - val_mae: 37.6746\n",
      "Epoch 598/1000\n",
      "16763/16763 [==============================] - 1s 53us/step - loss: 1814.9360 - mse: 1814.9362 - mae: 29.8393 - val_loss: 3002.3852 - val_mse: 3002.3850 - val_mae: 37.0957\n",
      "Epoch 599/1000\n",
      "16763/16763 [==============================] - 1s 71us/step - loss: 1787.5478 - mse: 1787.5481 - mae: 29.6601 - val_loss: 3213.3098 - val_mse: 3213.3093 - val_mae: 38.6603\n",
      "Epoch 600/1000\n",
      "16763/16763 [==============================] - 1s 69us/step - loss: 1804.9002 - mse: 1804.8998 - mae: 29.7452 - val_loss: 3023.7213 - val_mse: 3023.7214 - val_mae: 37.1236\n",
      "Epoch 601/1000\n",
      "16763/16763 [==============================] - 1s 69us/step - loss: 1791.0918 - mse: 1791.0914 - mae: 29.6906 - val_loss: 2994.1259 - val_mse: 2994.1257 - val_mae: 37.1638\n",
      "Epoch 602/1000\n",
      "16763/16763 [==============================] - 1s 57us/step - loss: 1798.5907 - mse: 1798.5908 - mae: 29.6524 - val_loss: 3082.8180 - val_mse: 3082.8179 - val_mae: 37.5645\n",
      "Epoch 603/1000\n",
      "16763/16763 [==============================] - 1s 54us/step - loss: 1801.5431 - mse: 1801.5432 - mae: 29.6852 - val_loss: 3012.4762 - val_mse: 3012.4756 - val_mae: 37.0049\n",
      "Epoch 604/1000\n",
      "16763/16763 [==============================] - 1s 58us/step - loss: 1812.0399 - mse: 1812.0391 - mae: 29.7405 - val_loss: 2976.0889 - val_mse: 2976.0891 - val_mae: 36.8593\n",
      "Epoch 605/1000\n",
      "16763/16763 [==============================] - 1s 69us/step - loss: 1802.5499 - mse: 1802.5498 - mae: 29.8044 - val_loss: 3031.1996 - val_mse: 3031.1990 - val_mae: 37.3375\n",
      "Epoch 606/1000\n",
      "16763/16763 [==============================] - 1s 61us/step - loss: 1785.6693 - mse: 1785.6689 - mae: 29.5244 - val_loss: 3169.2219 - val_mse: 3169.2224 - val_mae: 38.4847\n",
      "Epoch 607/1000\n",
      "16763/16763 [==============================] - 1s 45us/step - loss: 1797.3373 - mse: 1797.3373 - mae: 29.6451 - val_loss: 3140.2722 - val_mse: 3140.2722 - val_mae: 37.7033\n",
      "Epoch 608/1000\n",
      "16763/16763 [==============================] - 1s 38us/step - loss: 1802.5098 - mse: 1802.5099 - mae: 29.7573 - val_loss: 3091.4901 - val_mse: 3091.4897 - val_mae: 37.6436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 609/1000\n",
      "16763/16763 [==============================] - 1s 33us/step - loss: 1790.5975 - mse: 1790.5978 - mae: 29.6135 - val_loss: 3037.9815 - val_mse: 3037.9817 - val_mae: 37.2042\n",
      "Epoch 610/1000\n",
      "16763/16763 [==============================] - 1s 35us/step - loss: 1781.3733 - mse: 1781.3735 - mae: 29.5442 - val_loss: 3023.6237 - val_mse: 3023.6235 - val_mae: 37.2850\n",
      "Epoch 611/1000\n",
      "16763/16763 [==============================] - 1s 33us/step - loss: 1792.7017 - mse: 1792.7013 - mae: 29.6152 - val_loss: 2957.9283 - val_mse: 2957.9277 - val_mae: 36.8941\n",
      "Epoch 612/1000\n",
      "16763/16763 [==============================] - 1s 33us/step - loss: 1787.0078 - mse: 1787.0079 - mae: 29.6754 - val_loss: 3132.0931 - val_mse: 3132.0930 - val_mae: 37.8872\n",
      "Epoch 613/1000\n",
      "16763/16763 [==============================] - 1s 38us/step - loss: 1799.5991 - mse: 1799.6003 - mae: 29.6505 - val_loss: 2973.8372 - val_mse: 2973.8369 - val_mae: 37.0358\n",
      "Epoch 614/1000\n",
      "16763/16763 [==============================] - 1s 47us/step - loss: 1809.6729 - mse: 1809.6724 - mae: 29.7516 - val_loss: 3007.2430 - val_mse: 3007.2427 - val_mae: 37.4414\n",
      "Epoch 615/1000\n",
      "16763/16763 [==============================] - 1s 45us/step - loss: 1806.7273 - mse: 1806.7275 - mae: 29.8062 - val_loss: 3108.8250 - val_mse: 3108.8250 - val_mae: 38.6215\n",
      "Epoch 616/1000\n",
      "16763/16763 [==============================] - 1s 45us/step - loss: 1780.6092 - mse: 1780.6093 - mae: 29.5686 - val_loss: 3003.3483 - val_mse: 3003.3484 - val_mae: 37.2649\n",
      "Epoch 617/1000\n",
      "16763/16763 [==============================] - 1s 53us/step - loss: 1769.3625 - mse: 1769.3632 - mae: 29.5348 - val_loss: 3038.6907 - val_mse: 3038.6902 - val_mae: 37.5646\n",
      "Epoch 618/1000\n",
      "16763/16763 [==============================] - 1s 38us/step - loss: 1777.1975 - mse: 1777.1986 - mae: 29.4837 - val_loss: 3005.4158 - val_mse: 3005.4158 - val_mae: 37.2805\n",
      "Epoch 619/1000\n",
      "16763/16763 [==============================] - 1s 39us/step - loss: 1797.2893 - mse: 1797.2903 - mae: 29.7366 - val_loss: 3055.4062 - val_mse: 3055.4060 - val_mae: 37.9996\n",
      "Epoch 620/1000\n",
      "16763/16763 [==============================] - 1s 33us/step - loss: 1785.7546 - mse: 1785.7552 - mae: 29.5398 - val_loss: 2977.2747 - val_mse: 2977.2751 - val_mae: 37.0336\n",
      "Epoch 621/1000\n",
      "16763/16763 [==============================] - 1s 32us/step - loss: 1772.0365 - mse: 1772.0372 - mae: 29.3970 - val_loss: 3116.7686 - val_mse: 3116.7683 - val_mae: 37.8616\n",
      "Epoch 622/1000\n",
      "16763/16763 [==============================] - 1s 34us/step - loss: 1766.2314 - mse: 1766.2321 - mae: 29.4916 - val_loss: 2971.1402 - val_mse: 2971.1401 - val_mae: 36.8213\n",
      "Epoch 623/1000\n",
      "16763/16763 [==============================] - 1s 35us/step - loss: 1767.2017 - mse: 1767.2024 - mae: 29.4065 - val_loss: 2962.2390 - val_mse: 2962.2385 - val_mae: 36.9225\n",
      "Epoch 624/1000\n",
      "16763/16763 [==============================] - 1s 50us/step - loss: 1775.9939 - mse: 1775.9937 - mae: 29.5513 - val_loss: 3070.6211 - val_mse: 3070.6213 - val_mae: 37.6328\n",
      "Epoch 625/1000\n",
      "16763/16763 [==============================] - 1s 42us/step - loss: 1779.6722 - mse: 1779.6716 - mae: 29.5317 - val_loss: 3036.6792 - val_mse: 3036.6794 - val_mae: 37.6458\n",
      "Epoch 626/1000\n",
      "16763/16763 [==============================] - 1s 46us/step - loss: 1785.6208 - mse: 1785.6207 - mae: 29.6157 - val_loss: 3041.5000 - val_mse: 3041.5000 - val_mae: 37.5670\n",
      "Epoch 627/1000\n",
      "16763/16763 [==============================] - 1s 41us/step - loss: 1767.3270 - mse: 1767.3270 - mae: 29.3537 - val_loss: 3056.8442 - val_mse: 3056.8442 - val_mae: 37.6780\n",
      "Epoch 628/1000\n",
      "16763/16763 [==============================] - 1s 45us/step - loss: 1765.6183 - mse: 1765.6188 - mae: 29.3966 - val_loss: 3065.4990 - val_mse: 3065.4985 - val_mae: 38.1159\n",
      "Epoch 629/1000\n",
      "16763/16763 [==============================] - 1s 45us/step - loss: 1779.3376 - mse: 1779.3383 - mae: 29.5379 - val_loss: 3027.8109 - val_mse: 3027.8113 - val_mae: 37.3106\n",
      "Epoch 630/1000\n",
      "16763/16763 [==============================] - 1s 34us/step - loss: 1770.6970 - mse: 1770.6979 - mae: 29.4625 - val_loss: 3071.5190 - val_mse: 3071.5193 - val_mae: 37.3606\n",
      "Epoch 631/1000\n",
      "16763/16763 [==============================] - 1s 37us/step - loss: 1771.5169 - mse: 1771.5171 - mae: 29.5688 - val_loss: 3100.5011 - val_mse: 3100.5010 - val_mae: 38.1730\n",
      "Epoch 632/1000\n",
      "16763/16763 [==============================] - 1s 35us/step - loss: 1775.7581 - mse: 1775.7573 - mae: 29.6047 - val_loss: 3035.3381 - val_mse: 3035.3386 - val_mae: 37.1867\n",
      "Epoch 633/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 1760.6671 - mse: 1760.6664 - mae: 29.4536 - val_loss: 3033.6859 - val_mse: 3033.6855 - val_mae: 37.3546\n",
      "Epoch 634/1000\n",
      "16763/16763 [==============================] - 1s 39us/step - loss: 1756.5459 - mse: 1756.5460 - mae: 29.4028 - val_loss: 3015.5175 - val_mse: 3015.5178 - val_mae: 37.4910\n",
      "Epoch 635/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 1778.3024 - mse: 1778.3026 - mae: 29.5233 - val_loss: 3013.3879 - val_mse: 3013.3877 - val_mae: 37.2304\n",
      "Epoch 636/1000\n",
      "16763/16763 [==============================] - 1s 42us/step - loss: 1758.4668 - mse: 1758.4666 - mae: 29.4022 - val_loss: 2990.3110 - val_mse: 2990.3108 - val_mae: 37.1286\n",
      "Epoch 637/1000\n",
      "16763/16763 [==============================] - 1s 45us/step - loss: 1757.0893 - mse: 1757.0891 - mae: 29.3752 - val_loss: 3029.5965 - val_mse: 3029.5967 - val_mae: 37.7528\n",
      "Epoch 638/1000\n",
      "16763/16763 [==============================] - 1s 43us/step - loss: 1756.5494 - mse: 1756.5492 - mae: 29.3708 - val_loss: 2995.9038 - val_mse: 2995.9036 - val_mae: 37.1198\n",
      "Epoch 639/1000\n",
      "16763/16763 [==============================] - 1s 44us/step - loss: 1749.0503 - mse: 1749.0500 - mae: 29.2954 - val_loss: 2969.6452 - val_mse: 2969.6453 - val_mae: 36.8989\n",
      "Epoch 640/1000\n",
      "16763/16763 [==============================] - 1s 54us/step - loss: 1770.6939 - mse: 1770.6942 - mae: 29.5187 - val_loss: 3136.2525 - val_mse: 3136.2529 - val_mae: 38.3751\n",
      "Epoch 641/1000\n",
      "16763/16763 [==============================] - 1s 42us/step - loss: 1770.3641 - mse: 1770.3636 - mae: 29.5507 - val_loss: 3049.2212 - val_mse: 3049.2214 - val_mae: 38.0208\n",
      "Epoch 642/1000\n",
      "16763/16763 [==============================] - 1s 44us/step - loss: 1753.2174 - mse: 1753.2166 - mae: 29.3398 - val_loss: 3008.5737 - val_mse: 3008.5740 - val_mae: 36.9609\n",
      "Epoch 643/1000\n",
      "16763/16763 [==============================] - 1s 40us/step - loss: 1756.5477 - mse: 1756.5476 - mae: 29.3509 - val_loss: 2975.4220 - val_mse: 2975.4219 - val_mae: 36.9399\n",
      "Epoch 644/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 1756.1975 - mse: 1756.1968 - mae: 29.3604 - val_loss: 3038.3065 - val_mse: 3038.3071 - val_mae: 37.6373\n",
      "Epoch 645/1000\n",
      "16763/16763 [==============================] - 1s 40us/step - loss: 1747.4108 - mse: 1747.4103 - mae: 29.3161 - val_loss: 3123.6312 - val_mse: 3123.6318 - val_mae: 37.8604\n",
      "Epoch 646/1000\n",
      "16763/16763 [==============================] - 1s 34us/step - loss: 1746.2135 - mse: 1746.2134 - mae: 29.3552 - val_loss: 3065.6106 - val_mse: 3065.6111 - val_mae: 37.5907\n",
      "Epoch 647/1000\n",
      "16763/16763 [==============================] - 1s 34us/step - loss: 1740.8641 - mse: 1740.8646 - mae: 29.2808 - val_loss: 3118.6140 - val_mse: 3118.6143 - val_mae: 37.7177\n",
      "Epoch 648/1000\n",
      "16763/16763 [==============================] - 1s 33us/step - loss: 1755.3197 - mse: 1755.3185 - mae: 29.3756 - val_loss: 3038.4132 - val_mse: 3038.4138 - val_mae: 37.5797\n",
      "Epoch 649/1000\n",
      "16763/16763 [==============================] - 1s 40us/step - loss: 1740.1934 - mse: 1740.1936 - mae: 29.3875 - val_loss: 2999.3117 - val_mse: 2999.3110 - val_mae: 37.2147\n",
      "Epoch 650/1000\n",
      "16763/16763 [==============================] - 1s 41us/step - loss: 1758.6255 - mse: 1758.6254 - mae: 29.5903 - val_loss: 3090.5149 - val_mse: 3090.5151 - val_mae: 37.6954\n",
      "Epoch 651/1000\n",
      "16763/16763 [==============================] - 1s 45us/step - loss: 1757.5332 - mse: 1757.5332 - mae: 29.4101 - val_loss: 3039.5427 - val_mse: 3039.5422 - val_mae: 37.2597\n",
      "Epoch 652/1000\n",
      "16763/16763 [==============================] - 1s 43us/step - loss: 1750.5014 - mse: 1750.5010 - mae: 29.3037 - val_loss: 2995.1562 - val_mse: 2995.1570 - val_mae: 37.2864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 653/1000\n",
      "16763/16763 [==============================] - 1s 45us/step - loss: 1730.4307 - mse: 1730.4308 - mae: 29.2389 - val_loss: 3008.4054 - val_mse: 3008.4055 - val_mae: 37.3289\n",
      "Epoch 654/1000\n",
      "16763/16763 [==============================] - 1s 43us/step - loss: 1756.2292 - mse: 1756.2297 - mae: 29.4333 - val_loss: 3131.6841 - val_mse: 3131.6846 - val_mae: 38.0482\n",
      "Epoch 655/1000\n",
      "16763/16763 [==============================] - 1s 41us/step - loss: 1741.7637 - mse: 1741.7635 - mae: 29.3311 - val_loss: 3009.1756 - val_mse: 3009.1758 - val_mae: 37.1517\n",
      "Epoch 656/1000\n",
      "16763/16763 [==============================] - 1s 37us/step - loss: 1762.5279 - mse: 1762.5277 - mae: 29.6312 - val_loss: 3028.6990 - val_mse: 3028.6995 - val_mae: 37.2766\n",
      "Epoch 657/1000\n",
      "16763/16763 [==============================] - 1s 39us/step - loss: 1747.6402 - mse: 1747.6400 - mae: 29.3563 - val_loss: 2988.4421 - val_mse: 2988.4426 - val_mae: 37.2236\n",
      "Epoch 658/1000\n",
      "16763/16763 [==============================] - 1s 40us/step - loss: 1723.1647 - mse: 1723.1654 - mae: 29.1411 - val_loss: 3044.6109 - val_mse: 3044.6108 - val_mae: 37.4803\n",
      "Epoch 659/1000\n",
      "16763/16763 [==============================] - 1s 35us/step - loss: 1732.3344 - mse: 1732.3348 - mae: 29.2035 - val_loss: 3008.9544 - val_mse: 3008.9539 - val_mae: 37.4401\n",
      "Epoch 660/1000\n",
      "16763/16763 [==============================] - 1s 38us/step - loss: 1739.7698 - mse: 1739.7703 - mae: 29.4827 - val_loss: 2979.7453 - val_mse: 2979.7456 - val_mae: 36.7889\n",
      "Epoch 661/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 1732.7363 - mse: 1732.7362 - mae: 29.2463 - val_loss: 3004.5891 - val_mse: 3004.5889 - val_mae: 37.9538\n",
      "Epoch 662/1000\n",
      "16763/16763 [==============================] - 1s 34us/step - loss: 1734.8301 - mse: 1734.8298 - mae: 29.3019 - val_loss: 3072.5327 - val_mse: 3072.5330 - val_mae: 37.4388\n",
      "Epoch 663/1000\n",
      "16763/16763 [==============================] - 1s 44us/step - loss: 1737.7808 - mse: 1737.7817 - mae: 29.2526 - val_loss: 3030.9885 - val_mse: 3030.9885 - val_mae: 37.3253\n",
      "Epoch 664/1000\n",
      "16763/16763 [==============================] - 1s 44us/step - loss: 1752.4753 - mse: 1752.4753 - mae: 29.4802 - val_loss: 3028.7362 - val_mse: 3028.7361 - val_mae: 37.1366\n",
      "Epoch 665/1000\n",
      "16763/16763 [==============================] - 1s 46us/step - loss: 1722.0937 - mse: 1722.0929 - mae: 29.0601 - val_loss: 3115.5368 - val_mse: 3115.5366 - val_mae: 37.5693\n",
      "Epoch 666/1000\n",
      "16763/16763 [==============================] - 1s 50us/step - loss: 1730.8892 - mse: 1730.8895 - mae: 29.1446 - val_loss: 3059.0176 - val_mse: 3059.0173 - val_mae: 37.3913\n",
      "Epoch 667/1000\n",
      "16763/16763 [==============================] - 1s 49us/step - loss: 1729.8918 - mse: 1729.8920 - mae: 29.3153 - val_loss: 3025.5330 - val_mse: 3025.5325 - val_mae: 37.7110\n",
      "Epoch 668/1000\n",
      "16763/16763 [==============================] - 1s 45us/step - loss: 1737.2578 - mse: 1737.2577 - mae: 29.2515 - val_loss: 3036.2870 - val_mse: 3036.2866 - val_mae: 37.3231\n",
      "Epoch 669/1000\n",
      "16763/16763 [==============================] - 1s 41us/step - loss: 1730.0397 - mse: 1730.0394 - mae: 29.2464 - val_loss: 2965.1361 - val_mse: 2965.1353 - val_mae: 36.9826\n",
      "Epoch 670/1000\n",
      "16763/16763 [==============================] - 1s 47us/step - loss: 1735.0529 - mse: 1735.0526 - mae: 29.3826 - val_loss: 3058.5949 - val_mse: 3058.5947 - val_mae: 37.4620\n",
      "Epoch 671/1000\n",
      "16763/16763 [==============================] - 1s 47us/step - loss: 1723.5305 - mse: 1723.5304 - mae: 29.1920 - val_loss: 3056.7280 - val_mse: 3056.7275 - val_mae: 37.8013\n",
      "Epoch 672/1000\n",
      "16763/16763 [==============================] - 1s 50us/step - loss: 1722.1653 - mse: 1722.1650 - mae: 29.1610 - val_loss: 2992.8668 - val_mse: 2992.8672 - val_mae: 37.4924\n",
      "Epoch 673/1000\n",
      "16763/16763 [==============================] - 1s 42us/step - loss: 1720.1269 - mse: 1720.1270 - mae: 29.2301 - val_loss: 3075.8064 - val_mse: 3075.8059 - val_mae: 38.0516\n",
      "Epoch 674/1000\n",
      "16763/16763 [==============================] - 1s 46us/step - loss: 1723.2355 - mse: 1723.2357 - mae: 29.1244 - val_loss: 3042.0711 - val_mse: 3042.0710 - val_mae: 37.7380\n",
      "Epoch 675/1000\n",
      "16763/16763 [==============================] - 1s 33us/step - loss: 1716.8020 - mse: 1716.8021 - mae: 29.2090 - val_loss: 2950.7646 - val_mse: 2950.7634 - val_mae: 36.8498\n",
      "Epoch 676/1000\n",
      "16763/16763 [==============================] - 0s 29us/step - loss: 1719.8902 - mse: 1719.8901 - mae: 29.1829 - val_loss: 3003.9824 - val_mse: 3003.9827 - val_mae: 37.4985\n",
      "Epoch 677/1000\n",
      "16763/16763 [==============================] - 0s 30us/step - loss: 1734.8040 - mse: 1734.8035 - mae: 29.3388 - val_loss: 3019.4387 - val_mse: 3019.4382 - val_mae: 37.5861\n",
      "Epoch 678/1000\n",
      "16763/16763 [==============================] - 0s 29us/step - loss: 1723.4366 - mse: 1723.4368 - mae: 29.2083 - val_loss: 3103.2606 - val_mse: 3103.2605 - val_mae: 38.3462\n",
      "Epoch 679/1000\n",
      "16763/16763 [==============================] - 1s 30us/step - loss: 1730.5228 - mse: 1730.5233 - mae: 29.2535 - val_loss: 2980.9941 - val_mse: 2980.9944 - val_mae: 37.0119\n",
      "Epoch 680/1000\n",
      "16763/16763 [==============================] - 0s 30us/step - loss: 1731.9191 - mse: 1731.9197 - mae: 29.3121 - val_loss: 2969.2302 - val_mse: 2969.2305 - val_mae: 36.9836\n",
      "Epoch 681/1000\n",
      "16763/16763 [==============================] - 1s 32us/step - loss: 1709.3276 - mse: 1709.3281 - mae: 29.0375 - val_loss: 2990.4657 - val_mse: 2990.4653 - val_mae: 37.8424\n",
      "Epoch 682/1000\n",
      "16763/16763 [==============================] - 1s 37us/step - loss: 1714.8179 - mse: 1714.8186 - mae: 29.2027 - val_loss: 3029.4243 - val_mse: 3029.4246 - val_mae: 37.2700\n",
      "Epoch 683/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 1715.4599 - mse: 1715.4596 - mae: 29.1127 - val_loss: 2963.3128 - val_mse: 2963.3130 - val_mae: 37.2080\n",
      "Epoch 684/1000\n",
      "16763/16763 [==============================] - 1s 37us/step - loss: 1721.3396 - mse: 1721.3400 - mae: 29.1727 - val_loss: 3009.2549 - val_mse: 3009.2554 - val_mae: 37.1217\n",
      "Epoch 685/1000\n",
      "16763/16763 [==============================] - 1s 37us/step - loss: 1718.8440 - mse: 1718.8442 - mae: 29.1773 - val_loss: 3017.4376 - val_mse: 3017.4373 - val_mae: 37.2520\n",
      "Epoch 686/1000\n",
      "16763/16763 [==============================] - 1s 37us/step - loss: 1731.3589 - mse: 1731.3586 - mae: 29.2830 - val_loss: 3209.9155 - val_mse: 3209.9150 - val_mae: 40.5076\n",
      "Epoch 687/1000\n",
      "16763/16763 [==============================] - 1s 33us/step - loss: 1714.2955 - mse: 1714.2953 - mae: 29.2090 - val_loss: 2994.2576 - val_mse: 2994.2578 - val_mae: 37.0792\n",
      "Epoch 688/1000\n",
      "16763/16763 [==============================] - 1s 30us/step - loss: 1704.3156 - mse: 1704.3157 - mae: 29.1319 - val_loss: 3040.4920 - val_mse: 3040.4922 - val_mae: 37.6665\n",
      "Epoch 689/1000\n",
      "16763/16763 [==============================] - 0s 29us/step - loss: 1709.5944 - mse: 1709.5948 - mae: 29.1104 - val_loss: 3128.4168 - val_mse: 3128.4163 - val_mae: 38.6977\n",
      "Epoch 690/1000\n",
      "16763/16763 [==============================] - 1s 30us/step - loss: 1701.1969 - mse: 1701.1963 - mae: 29.0243 - val_loss: 3005.1247 - val_mse: 3005.1243 - val_mae: 37.0111\n",
      "Epoch 691/1000\n",
      "16763/16763 [==============================] - 0s 30us/step - loss: 1726.1368 - mse: 1726.1370 - mae: 29.2960 - val_loss: 2988.0714 - val_mse: 2988.0713 - val_mae: 37.1896\n",
      "Epoch 692/1000\n",
      "16763/16763 [==============================] - 1s 30us/step - loss: 1724.2795 - mse: 1724.2797 - mae: 29.2632 - val_loss: 2988.8393 - val_mse: 2988.8394 - val_mae: 37.1251\n",
      "Epoch 693/1000\n",
      "16763/16763 [==============================] - 0s 30us/step - loss: 1700.6513 - mse: 1700.6514 - mae: 29.0646 - val_loss: 2973.0920 - val_mse: 2973.0923 - val_mae: 37.2484\n",
      "Epoch 694/1000\n",
      "16763/16763 [==============================] - 1s 35us/step - loss: 1709.9160 - mse: 1709.9155 - mae: 29.0863 - val_loss: 3028.8665 - val_mse: 3028.8667 - val_mae: 37.0991\n",
      "Epoch 695/1000\n",
      "16763/16763 [==============================] - 1s 37us/step - loss: 1702.5059 - mse: 1702.5051 - mae: 29.0721 - val_loss: 3005.0677 - val_mse: 3005.0679 - val_mae: 37.5806\n",
      "Epoch 696/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 1709.4092 - mse: 1709.4084 - mae: 29.1038 - val_loss: 3025.2329 - val_mse: 3025.2336 - val_mae: 37.0881\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 697/1000\n",
      "16763/16763 [==============================] - 1s 38us/step - loss: 1701.0128 - mse: 1701.0131 - mae: 29.0623 - val_loss: 2960.5149 - val_mse: 2960.5149 - val_mae: 36.9297\n",
      "Epoch 698/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 1722.2051 - mse: 1722.2051 - mae: 29.2307 - val_loss: 2996.0511 - val_mse: 2996.0505 - val_mae: 37.0415\n",
      "Epoch 699/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 1702.6866 - mse: 1702.6863 - mae: 28.9923 - val_loss: 3041.5052 - val_mse: 3041.5049 - val_mae: 37.4008\n",
      "Epoch 700/1000\n",
      "16763/16763 [==============================] - 0s 30us/step - loss: 1689.6533 - mse: 1689.6536 - mae: 28.9779 - val_loss: 2979.6478 - val_mse: 2979.6479 - val_mae: 36.9397\n",
      "Epoch 701/1000\n",
      "16763/16763 [==============================] - 0s 30us/step - loss: 1698.8101 - mse: 1698.8105 - mae: 29.0372 - val_loss: 2989.9420 - val_mse: 2989.9426 - val_mae: 37.1455\n",
      "Epoch 702/1000\n",
      "16763/16763 [==============================] - 1s 31us/step - loss: 1716.3013 - mse: 1716.3016 - mae: 29.3010 - val_loss: 2963.1766 - val_mse: 2963.1765 - val_mae: 36.9184\n",
      "Epoch 703/1000\n",
      "16763/16763 [==============================] - 1s 32us/step - loss: 1691.3593 - mse: 1691.3593 - mae: 29.0569 - val_loss: 2970.6152 - val_mse: 2970.6157 - val_mae: 37.3265\n",
      "Epoch 704/1000\n",
      "16763/16763 [==============================] - 0s 30us/step - loss: 1709.7319 - mse: 1709.7319 - mae: 29.1254 - val_loss: 3008.3339 - val_mse: 3008.3333 - val_mae: 37.0220\n",
      "Epoch 705/1000\n",
      "16763/16763 [==============================] - 0s 30us/step - loss: 1691.4346 - mse: 1691.4332 - mae: 28.9755 - val_loss: 2988.4025 - val_mse: 2988.4028 - val_mae: 36.9696\n",
      "Epoch 706/1000\n",
      "16763/16763 [==============================] - 1s 33us/step - loss: 1699.1389 - mse: 1699.1385 - mae: 29.0350 - val_loss: 3078.5801 - val_mse: 3078.5801 - val_mae: 37.9269\n",
      "Epoch 707/1000\n",
      "16763/16763 [==============================] - 1s 37us/step - loss: 1700.0469 - mse: 1700.0471 - mae: 29.0127 - val_loss: 2981.5789 - val_mse: 2981.5786 - val_mae: 37.0186\n",
      "Epoch 708/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 1687.5393 - mse: 1687.5396 - mae: 29.0282 - val_loss: 3037.2403 - val_mse: 3037.2410 - val_mae: 37.5140\n",
      "Epoch 709/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 1686.4725 - mse: 1686.4720 - mae: 29.0348 - val_loss: 2972.7562 - val_mse: 2972.7556 - val_mae: 36.8152\n",
      "Epoch 710/1000\n",
      "16763/16763 [==============================] - 1s 38us/step - loss: 1688.1946 - mse: 1688.1941 - mae: 28.9764 - val_loss: 2994.1078 - val_mse: 2994.1082 - val_mae: 36.8604\n",
      "Epoch 711/1000\n",
      "16763/16763 [==============================] - 1s 37us/step - loss: 1684.3330 - mse: 1684.3328 - mae: 28.9598 - val_loss: 3002.6348 - val_mse: 3002.6348 - val_mae: 37.4642\n",
      "Epoch 712/1000\n",
      "16763/16763 [==============================] - 1s 31us/step - loss: 1689.9976 - mse: 1689.9984 - mae: 28.9202 - val_loss: 2996.4418 - val_mse: 2996.4417 - val_mae: 37.2800\n",
      "Epoch 713/1000\n",
      "16763/16763 [==============================] - 1s 31us/step - loss: 1718.8909 - mse: 1718.8906 - mae: 29.3497 - val_loss: 3032.6612 - val_mse: 3032.6606 - val_mae: 37.7364\n",
      "Epoch 714/1000\n",
      "16763/16763 [==============================] - 0s 29us/step - loss: 1694.0549 - mse: 1694.0549 - mae: 28.9985 - val_loss: 3022.9074 - val_mse: 3022.9077 - val_mae: 37.1233\n",
      "Epoch 715/1000\n",
      "16763/16763 [==============================] - 0s 30us/step - loss: 1674.5361 - mse: 1674.5354 - mae: 28.8095 - val_loss: 3160.1239 - val_mse: 3160.1240 - val_mae: 39.1616\n",
      "Epoch 716/1000\n",
      "16763/16763 [==============================] - 0s 29us/step - loss: 1683.0460 - mse: 1683.0457 - mae: 29.0343 - val_loss: 2954.2325 - val_mse: 2954.2329 - val_mae: 37.0352\n",
      "Epoch 717/1000\n",
      "16763/16763 [==============================] - 1s 30us/step - loss: 1692.5237 - mse: 1692.5238 - mae: 29.0800 - val_loss: 2955.7900 - val_mse: 2955.7900 - val_mae: 36.8513\n",
      "Epoch 718/1000\n",
      "16763/16763 [==============================] - 1s 31us/step - loss: 1691.1320 - mse: 1691.1315 - mae: 28.9561 - val_loss: 2990.1780 - val_mse: 2990.1782 - val_mae: 37.0406\n",
      "Epoch 719/1000\n",
      "16763/16763 [==============================] - 1s 37us/step - loss: 1679.3788 - mse: 1679.3790 - mae: 28.9080 - val_loss: 3072.2077 - val_mse: 3072.2075 - val_mae: 37.3971\n",
      "Epoch 720/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 1697.3119 - mse: 1697.3121 - mae: 29.1667 - val_loss: 2951.2379 - val_mse: 2951.2373 - val_mae: 36.8155\n",
      "Epoch 721/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 1688.9825 - mse: 1688.9822 - mae: 29.0360 - val_loss: 3034.3763 - val_mse: 3034.3762 - val_mae: 37.1919\n",
      "Epoch 722/1000\n",
      "16763/16763 [==============================] - 1s 40us/step - loss: 1675.4809 - mse: 1675.4811 - mae: 28.8902 - val_loss: 2923.9327 - val_mse: 2923.9321 - val_mae: 36.8057\n",
      "Epoch 723/1000\n",
      "16763/16763 [==============================] - 1s 41us/step - loss: 1684.1910 - mse: 1684.1906 - mae: 29.0033 - val_loss: 3055.4728 - val_mse: 3055.4724 - val_mae: 38.3740\n",
      "Epoch 724/1000\n",
      "16763/16763 [==============================] - 1s 53us/step - loss: 1672.1046 - mse: 1672.1041 - mae: 28.8788 - val_loss: 2974.1194 - val_mse: 2974.1191 - val_mae: 36.9682\n",
      "Epoch 725/1000\n",
      "16763/16763 [==============================] - 1s 44us/step - loss: 1685.3054 - mse: 1685.3055 - mae: 28.8357 - val_loss: 3026.1481 - val_mse: 3026.1487 - val_mae: 37.9293\n",
      "Epoch 726/1000\n",
      "16763/16763 [==============================] - 1s 44us/step - loss: 1678.1911 - mse: 1678.1914 - mae: 28.9291 - val_loss: 2948.4500 - val_mse: 2948.4495 - val_mae: 36.7586\n",
      "Epoch 727/1000\n",
      "16763/16763 [==============================] - 1s 52us/step - loss: 1667.4891 - mse: 1667.4900 - mae: 28.7998 - val_loss: 3010.4804 - val_mse: 3010.4807 - val_mae: 37.1252\n",
      "Epoch 728/1000\n",
      "16763/16763 [==============================] - 1s 41us/step - loss: 1670.9862 - mse: 1670.9860 - mae: 28.8080 - val_loss: 2994.7124 - val_mse: 2994.7124 - val_mae: 37.1543\n",
      "Epoch 729/1000\n",
      "16763/16763 [==============================] - 1s 39us/step - loss: 1678.7850 - mse: 1678.7847 - mae: 28.9771 - val_loss: 3017.8340 - val_mse: 3017.8340 - val_mae: 37.5186\n",
      "Epoch 730/1000\n",
      "16763/16763 [==============================] - 1s 39us/step - loss: 1687.9382 - mse: 1687.9386 - mae: 28.9516 - val_loss: 3021.0099 - val_mse: 3021.0100 - val_mae: 37.4481\n",
      "Epoch 731/1000\n",
      "16763/16763 [==============================] - 1s 37us/step - loss: 1662.7210 - mse: 1662.7207 - mae: 28.7409 - val_loss: 2948.0861 - val_mse: 2948.0859 - val_mae: 37.1392\n",
      "Epoch 732/1000\n",
      "16763/16763 [==============================] - 1s 34us/step - loss: 1672.9117 - mse: 1672.9116 - mae: 28.8529 - val_loss: 2951.3936 - val_mse: 2951.3940 - val_mae: 36.7307\n",
      "Epoch 733/1000\n",
      "16763/16763 [==============================] - 1s 37us/step - loss: 1664.3352 - mse: 1664.3356 - mae: 28.8094 - val_loss: 3051.7446 - val_mse: 3051.7444 - val_mae: 37.3442\n",
      "Epoch 734/1000\n",
      "16763/16763 [==============================] - 1s 30us/step - loss: 1674.8551 - mse: 1674.8552 - mae: 28.8838 - val_loss: 2987.3406 - val_mse: 2987.3406 - val_mae: 37.4179\n",
      "Epoch 735/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 1666.3274 - mse: 1666.3278 - mae: 28.9804 - val_loss: 3016.6719 - val_mse: 3016.6721 - val_mae: 37.2374\n",
      "Epoch 736/1000\n",
      "16763/16763 [==============================] - 1s 31us/step - loss: 1670.1138 - mse: 1670.1143 - mae: 29.0001 - val_loss: 2962.2540 - val_mse: 2962.2539 - val_mae: 36.8569\n",
      "Epoch 737/1000\n",
      "16763/16763 [==============================] - 1s 35us/step - loss: 1658.9291 - mse: 1658.9297 - mae: 28.7184 - val_loss: 2970.1937 - val_mse: 2970.1943 - val_mae: 36.9402\n",
      "Epoch 738/1000\n",
      "16763/16763 [==============================] - 1s 50us/step - loss: 1663.0697 - mse: 1663.0701 - mae: 28.7723 - val_loss: 3073.2495 - val_mse: 3073.2498 - val_mae: 37.7244\n",
      "Epoch 739/1000\n",
      "16763/16763 [==============================] - 1s 42us/step - loss: 1663.2152 - mse: 1663.2155 - mae: 28.8460 - val_loss: 2994.4377 - val_mse: 2994.4377 - val_mae: 37.2597\n",
      "Epoch 740/1000\n",
      "16763/16763 [==============================] - 1s 45us/step - loss: 1668.1274 - mse: 1668.1274 - mae: 28.8563 - val_loss: 3074.7952 - val_mse: 3074.7952 - val_mae: 37.6659\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 741/1000\n",
      "16763/16763 [==============================] - 1s 38us/step - loss: 1673.1220 - mse: 1673.1221 - mae: 28.8545 - val_loss: 2966.7976 - val_mse: 2966.7981 - val_mae: 36.8742\n",
      "Epoch 742/1000\n",
      "16763/16763 [==============================] - 1s 40us/step - loss: 1659.1616 - mse: 1659.1625 - mae: 28.7525 - val_loss: 3017.8718 - val_mse: 3017.8721 - val_mae: 37.2470\n",
      "Epoch 743/1000\n",
      "16763/16763 [==============================] - 0s 30us/step - loss: 1660.3209 - mse: 1660.3206 - mae: 28.8184 - val_loss: 2980.2430 - val_mse: 2980.2432 - val_mae: 36.9892\n",
      "Epoch 744/1000\n",
      "16763/16763 [==============================] - 1s 32us/step - loss: 1682.1464 - mse: 1682.1465 - mae: 29.0109 - val_loss: 3012.0568 - val_mse: 3012.0566 - val_mae: 37.1892\n",
      "Epoch 745/1000\n",
      "16763/16763 [==============================] - 1s 37us/step - loss: 1653.0819 - mse: 1653.0819 - mae: 28.7186 - val_loss: 3067.9004 - val_mse: 3067.9009 - val_mae: 37.5814\n",
      "Epoch 746/1000\n",
      "16763/16763 [==============================] - 1s 38us/step - loss: 1678.7632 - mse: 1678.7639 - mae: 29.0838 - val_loss: 2955.9818 - val_mse: 2955.9819 - val_mae: 36.7164\n",
      "Epoch 747/1000\n",
      "16763/16763 [==============================] - 1s 33us/step - loss: 1658.7366 - mse: 1658.7365 - mae: 28.8270 - val_loss: 3049.5368 - val_mse: 3049.5371 - val_mae: 37.9950\n",
      "Epoch 748/1000\n",
      "16763/16763 [==============================] - 1s 35us/step - loss: 1678.5978 - mse: 1678.5977 - mae: 29.0013 - val_loss: 3069.7658 - val_mse: 3069.7654 - val_mae: 37.4764\n",
      "Epoch 749/1000\n",
      "16763/16763 [==============================] - 1s 42us/step - loss: 1645.0450 - mse: 1645.0441 - mae: 28.6290 - val_loss: 2960.1624 - val_mse: 2960.1631 - val_mae: 37.1268\n",
      "Epoch 750/1000\n",
      "16763/16763 [==============================] - 1s 38us/step - loss: 1670.0005 - mse: 1670.0004 - mae: 28.8812 - val_loss: 2958.6002 - val_mse: 2958.5996 - val_mae: 36.9194\n",
      "Epoch 751/1000\n",
      "16763/16763 [==============================] - 1s 41us/step - loss: 1658.9262 - mse: 1658.9265 - mae: 28.7199 - val_loss: 3042.5571 - val_mse: 3042.5569 - val_mae: 37.3278\n",
      "Epoch 752/1000\n",
      "16763/16763 [==============================] - 1s 39us/step - loss: 1639.8777 - mse: 1639.8770 - mae: 28.6452 - val_loss: 3063.1061 - val_mse: 3063.1062 - val_mae: 38.4392\n",
      "Epoch 753/1000\n",
      "16763/16763 [==============================] - 1s 37us/step - loss: 1668.6095 - mse: 1668.6097 - mae: 28.9518 - val_loss: 2992.6773 - val_mse: 2992.6772 - val_mae: 37.0169\n",
      "Epoch 754/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 1642.6200 - mse: 1642.6202 - mae: 28.6660 - val_loss: 2952.6635 - val_mse: 2952.6633 - val_mae: 37.1923\n",
      "Epoch 755/1000\n",
      "16763/16763 [==============================] - 1s 37us/step - loss: 1660.0840 - mse: 1660.0842 - mae: 28.7749 - val_loss: 3041.1785 - val_mse: 3041.1785 - val_mae: 37.9885\n",
      "Epoch 756/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 1656.9558 - mse: 1656.9554 - mae: 28.7113 - val_loss: 2984.2436 - val_mse: 2984.2432 - val_mae: 37.0013\n",
      "Epoch 757/1000\n",
      "16763/16763 [==============================] - 1s 35us/step - loss: 1664.5532 - mse: 1664.5531 - mae: 28.7979 - val_loss: 2968.6961 - val_mse: 2968.6963 - val_mae: 37.2522\n",
      "Epoch 758/1000\n",
      "16763/16763 [==============================] - 0s 29us/step - loss: 1648.2743 - mse: 1648.2744 - mae: 28.7443 - val_loss: 3023.0781 - val_mse: 3023.0779 - val_mae: 37.3846\n",
      "Epoch 759/1000\n",
      "16763/16763 [==============================] - 0s 30us/step - loss: 1649.2375 - mse: 1649.2383 - mae: 28.6769 - val_loss: 2985.1048 - val_mse: 2985.1052 - val_mae: 37.2005\n",
      "Epoch 760/1000\n",
      "16763/16763 [==============================] - 0s 29us/step - loss: 1654.6252 - mse: 1654.6249 - mae: 28.7482 - val_loss: 3001.6036 - val_mse: 3001.6033 - val_mae: 37.1918\n",
      "Epoch 761/1000\n",
      "16763/16763 [==============================] - 0s 29us/step - loss: 1673.8793 - mse: 1673.8792 - mae: 29.0407 - val_loss: 3009.5498 - val_mse: 3009.5500 - val_mae: 37.2399\n",
      "Epoch 762/1000\n",
      "16763/16763 [==============================] - 0s 29us/step - loss: 1653.8632 - mse: 1653.8624 - mae: 28.7166 - val_loss: 2990.1270 - val_mse: 2990.1277 - val_mae: 37.0930\n",
      "Epoch 763/1000\n",
      "16763/16763 [==============================] - 0s 29us/step - loss: 1655.3681 - mse: 1655.3683 - mae: 28.6963 - val_loss: 2966.8589 - val_mse: 2966.8586 - val_mae: 36.9300\n",
      "Epoch 764/1000\n",
      "16763/16763 [==============================] - 0s 30us/step - loss: 1648.4379 - mse: 1648.4379 - mae: 28.7054 - val_loss: 2975.6723 - val_mse: 2975.6719 - val_mae: 37.1201\n",
      "Epoch 765/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 1651.0213 - mse: 1651.0208 - mae: 28.6605 - val_loss: 3056.7660 - val_mse: 3056.7659 - val_mae: 37.8726\n",
      "Epoch 766/1000\n",
      "16763/16763 [==============================] - 1s 35us/step - loss: 1652.5100 - mse: 1652.5100 - mae: 28.7201 - val_loss: 2991.0409 - val_mse: 2991.0413 - val_mae: 37.4517\n",
      "Epoch 767/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 1651.0183 - mse: 1651.0177 - mae: 28.7379 - val_loss: 2951.5494 - val_mse: 2951.5488 - val_mae: 36.7406\n",
      "Epoch 768/1000\n",
      "16763/16763 [==============================] - 1s 35us/step - loss: 1641.4872 - mse: 1641.4873 - mae: 28.6528 - val_loss: 3082.5981 - val_mse: 3082.5981 - val_mae: 37.4462\n",
      "Epoch 769/1000\n",
      "16763/16763 [==============================] - 1s 37us/step - loss: 1643.6100 - mse: 1643.6101 - mae: 28.7313 - val_loss: 2967.0252 - val_mse: 2967.0259 - val_mae: 36.9644\n",
      "Epoch 770/1000\n",
      "16763/16763 [==============================] - 1s 35us/step - loss: 1646.8153 - mse: 1646.8147 - mae: 28.7253 - val_loss: 2946.7606 - val_mse: 2946.7607 - val_mae: 36.9999\n",
      "Epoch 771/1000\n",
      "16763/16763 [==============================] - 1s 35us/step - loss: 1648.7276 - mse: 1648.7280 - mae: 28.6939 - val_loss: 3026.2665 - val_mse: 3026.2661 - val_mae: 37.1596\n",
      "Epoch 772/1000\n",
      "16763/16763 [==============================] - 1s 33us/step - loss: 1649.8834 - mse: 1649.8835 - mae: 28.7271 - val_loss: 2929.9683 - val_mse: 2929.9678 - val_mae: 36.8235\n",
      "Epoch 773/1000\n",
      "16763/16763 [==============================] - 0s 29us/step - loss: 1639.8261 - mse: 1639.8263 - mae: 28.6350 - val_loss: 3100.1547 - val_mse: 3100.1543 - val_mae: 38.6201\n",
      "Epoch 774/1000\n",
      "16763/16763 [==============================] - 0s 29us/step - loss: 1645.3456 - mse: 1645.3455 - mae: 28.7372 - val_loss: 2987.0772 - val_mse: 2987.0781 - val_mae: 37.5516\n",
      "Epoch 775/1000\n",
      "16763/16763 [==============================] - 0s 29us/step - loss: 1641.3448 - mse: 1641.3451 - mae: 28.6093 - val_loss: 3159.1062 - val_mse: 3159.1067 - val_mae: 38.1027\n",
      "Epoch 776/1000\n",
      "16763/16763 [==============================] - 0s 29us/step - loss: 1665.7876 - mse: 1665.7878 - mae: 28.9329 - val_loss: 3047.0078 - val_mse: 3047.0076 - val_mae: 37.3100\n",
      "Epoch 777/1000\n",
      "16763/16763 [==============================] - 0s 29us/step - loss: 1640.9148 - mse: 1640.9141 - mae: 28.6774 - val_loss: 2985.1936 - val_mse: 2985.1941 - val_mae: 37.1616\n",
      "Epoch 778/1000\n",
      "16763/16763 [==============================] - 0s 29us/step - loss: 1652.5429 - mse: 1652.5428 - mae: 28.7263 - val_loss: 2964.8079 - val_mse: 2964.8074 - val_mae: 37.0100\n",
      "Epoch 779/1000\n",
      "16763/16763 [==============================] - 1s 31us/step - loss: 1664.0299 - mse: 1664.0295 - mae: 28.9355 - val_loss: 3001.1158 - val_mse: 3001.1155 - val_mae: 37.0375\n",
      "Epoch 780/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 1625.5698 - mse: 1625.5696 - mae: 28.5373 - val_loss: 2994.1016 - val_mse: 2994.1013 - val_mae: 36.9740\n",
      "Epoch 781/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 1644.7636 - mse: 1644.7633 - mae: 28.7765 - val_loss: 3042.9157 - val_mse: 3042.9158 - val_mae: 37.7469\n",
      "Epoch 782/1000\n",
      "16763/16763 [==============================] - 1s 35us/step - loss: 1632.2677 - mse: 1632.2676 - mae: 28.5660 - val_loss: 3035.4502 - val_mse: 3035.4502 - val_mae: 37.8010\n",
      "Epoch 783/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 1629.2892 - mse: 1629.2893 - mae: 28.5693 - val_loss: 2927.2555 - val_mse: 2927.2559 - val_mae: 36.9080\n",
      "Epoch 784/1000\n",
      "16763/16763 [==============================] - 1s 35us/step - loss: 1628.7884 - mse: 1628.7886 - mae: 28.5929 - val_loss: 2961.5831 - val_mse: 2961.5833 - val_mae: 36.8036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 785/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 1629.2770 - mse: 1629.2771 - mae: 28.5090 - val_loss: 3020.2463 - val_mse: 3020.2471 - val_mae: 37.3709\n",
      "Epoch 786/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 1632.7395 - mse: 1632.7393 - mae: 28.5949 - val_loss: 3052.8765 - val_mse: 3052.8765 - val_mae: 37.5949\n",
      "Epoch 787/1000\n",
      "16763/16763 [==============================] - 1s 31us/step - loss: 1624.4184 - mse: 1624.4186 - mae: 28.4769 - val_loss: 3014.9325 - val_mse: 3014.9324 - val_mae: 37.5765\n",
      "Epoch 788/1000\n",
      "16763/16763 [==============================] - 0s 29us/step - loss: 1650.9385 - mse: 1650.9392 - mae: 28.8421 - val_loss: 2988.5916 - val_mse: 2988.5918 - val_mae: 37.2358\n",
      "Epoch 789/1000\n",
      "16763/16763 [==============================] - 0s 29us/step - loss: 1649.3728 - mse: 1649.3721 - mae: 28.7316 - val_loss: 3028.1631 - val_mse: 3028.1628 - val_mae: 38.0644\n",
      "Epoch 790/1000\n",
      "16763/16763 [==============================] - 0s 29us/step - loss: 1635.3563 - mse: 1635.3562 - mae: 28.6521 - val_loss: 2972.6022 - val_mse: 2972.6023 - val_mae: 36.9975\n",
      "Epoch 791/1000\n",
      "16763/16763 [==============================] - 0s 29us/step - loss: 1622.5062 - mse: 1622.5059 - mae: 28.4451 - val_loss: 3029.4575 - val_mse: 3029.4573 - val_mae: 37.4603\n",
      "Epoch 792/1000\n",
      "16763/16763 [==============================] - 0s 29us/step - loss: 1627.2766 - mse: 1627.2764 - mae: 28.5569 - val_loss: 2974.5550 - val_mse: 2974.5552 - val_mae: 36.8126\n",
      "Epoch 793/1000\n",
      "16763/16763 [==============================] - 0s 29us/step - loss: 1628.6340 - mse: 1628.6343 - mae: 28.6235 - val_loss: 3043.3845 - val_mse: 3043.3845 - val_mae: 37.4722\n",
      "Epoch 794/1000\n",
      "16763/16763 [==============================] - 1s 34us/step - loss: 1616.9228 - mse: 1616.9230 - mae: 28.5125 - val_loss: 2966.7010 - val_mse: 2966.7012 - val_mae: 36.6555\n",
      "Epoch 795/1000\n",
      "16763/16763 [==============================] - 1s 35us/step - loss: 1622.5516 - mse: 1622.5520 - mae: 28.4811 - val_loss: 2976.5186 - val_mse: 2976.5190 - val_mae: 37.3265\n",
      "Epoch 796/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 1618.8657 - mse: 1618.8662 - mae: 28.6509 - val_loss: 2993.3033 - val_mse: 2993.3030 - val_mae: 37.1116\n",
      "Epoch 797/1000\n",
      "16763/16763 [==============================] - 1s 35us/step - loss: 1649.3983 - mse: 1649.3989 - mae: 28.7309 - val_loss: 3176.6971 - val_mse: 3176.6973 - val_mae: 37.9351\n",
      "Epoch 798/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 1623.8263 - mse: 1623.8263 - mae: 28.4942 - val_loss: 2966.5148 - val_mse: 2966.5149 - val_mae: 36.9349\n",
      "Epoch 799/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 1613.1951 - mse: 1613.1948 - mae: 28.4581 - val_loss: 2964.0476 - val_mse: 2964.0479 - val_mae: 36.5849\n",
      "Epoch 800/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 1630.0527 - mse: 1630.0530 - mae: 28.6116 - val_loss: 2953.9831 - val_mse: 2953.9827 - val_mae: 36.8996\n",
      "Epoch 801/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 1609.6745 - mse: 1609.6742 - mae: 28.4063 - val_loss: 2982.8544 - val_mse: 2982.8547 - val_mae: 36.9371\n",
      "Epoch 802/1000\n",
      "16763/16763 [==============================] - 1s 35us/step - loss: 1620.1326 - mse: 1620.1326 - mae: 28.6055 - val_loss: 3059.3137 - val_mse: 3059.3132 - val_mae: 38.4932\n",
      "Epoch 803/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 1647.3760 - mse: 1647.3770 - mae: 28.6494 - val_loss: 3079.9306 - val_mse: 3079.9304 - val_mae: 37.5356\n",
      "Epoch 804/1000\n",
      "16763/16763 [==============================] - 1s 37us/step - loss: 1628.2570 - mse: 1628.2565 - mae: 28.6315 - val_loss: 2927.5131 - val_mse: 2927.5137 - val_mae: 36.7380\n",
      "Epoch 805/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 1604.2772 - mse: 1604.2770 - mae: 28.4113 - val_loss: 2954.8777 - val_mse: 2954.8782 - val_mae: 37.1806\n",
      "Epoch 806/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 1615.7451 - mse: 1615.7443 - mae: 28.3883 - val_loss: 3101.0524 - val_mse: 3101.0525 - val_mae: 39.1974\n",
      "Epoch 807/1000\n",
      "16763/16763 [==============================] - 1s 33us/step - loss: 1615.4726 - mse: 1615.4738 - mae: 28.5380 - val_loss: 2944.9286 - val_mse: 2944.9290 - val_mae: 36.8193\n",
      "Epoch 808/1000\n",
      "16763/16763 [==============================] - 0s 29us/step - loss: 1610.7237 - mse: 1610.7235 - mae: 28.4329 - val_loss: 3003.4013 - val_mse: 3003.4016 - val_mae: 37.0990\n",
      "Epoch 809/1000\n",
      "16763/16763 [==============================] - 0s 29us/step - loss: 1619.5059 - mse: 1619.5063 - mae: 28.6077 - val_loss: 3003.3167 - val_mse: 3003.3169 - val_mae: 37.2104\n",
      "Epoch 810/1000\n",
      "16763/16763 [==============================] - 0s 29us/step - loss: 1614.5590 - mse: 1614.5586 - mae: 28.4209 - val_loss: 3018.5402 - val_mse: 3018.5398 - val_mae: 37.5208\n",
      "Epoch 811/1000\n",
      "16763/16763 [==============================] - 0s 29us/step - loss: 1623.8734 - mse: 1623.8729 - mae: 28.4753 - val_loss: 3038.2911 - val_mse: 3038.2913 - val_mae: 37.1796\n",
      "Epoch 812/1000\n",
      "16763/16763 [==============================] - 0s 29us/step - loss: 1618.2142 - mse: 1618.2144 - mae: 28.4807 - val_loss: 3001.7799 - val_mse: 3001.7805 - val_mae: 37.6978\n",
      "Epoch 813/1000\n",
      "16763/16763 [==============================] - 0s 29us/step - loss: 1626.9657 - mse: 1626.9661 - mae: 28.6632 - val_loss: 2971.8499 - val_mse: 2971.8501 - val_mae: 36.8785\n",
      "Epoch 814/1000\n",
      "16763/16763 [==============================] - 1s 31us/step - loss: 1605.6729 - mse: 1605.6729 - mae: 28.3178 - val_loss: 2968.0675 - val_mse: 2968.0671 - val_mae: 37.2259\n",
      "Epoch 815/1000\n",
      "16763/16763 [==============================] - 1s 35us/step - loss: 1618.6495 - mse: 1618.6494 - mae: 28.5517 - val_loss: 3035.3354 - val_mse: 3035.3354 - val_mae: 37.0804\n",
      "Epoch 816/1000\n",
      "16763/16763 [==============================] - 1s 39us/step - loss: 1607.2795 - mse: 1607.2797 - mae: 28.3595 - val_loss: 3002.1814 - val_mse: 3002.1819 - val_mae: 37.4222\n",
      "Epoch 817/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 1613.4337 - mse: 1613.4335 - mae: 28.5110 - val_loss: 2959.2584 - val_mse: 2959.2581 - val_mae: 36.9755\n",
      "Epoch 818/1000\n",
      "16763/16763 [==============================] - 1s 35us/step - loss: 1610.8825 - mse: 1610.8828 - mae: 28.4524 - val_loss: 2998.6465 - val_mse: 2998.6472 - val_mae: 37.1019\n",
      "Epoch 819/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 1595.9044 - mse: 1595.9043 - mae: 28.2610 - val_loss: 2974.1983 - val_mse: 2974.1990 - val_mae: 37.1877\n",
      "Epoch 820/1000\n",
      "16763/16763 [==============================] - 1s 31us/step - loss: 1604.6367 - mse: 1604.6377 - mae: 28.4286 - val_loss: 3009.6467 - val_mse: 3009.6470 - val_mae: 37.0599\n",
      "Epoch 821/1000\n",
      "16763/16763 [==============================] - 0s 29us/step - loss: 1620.1555 - mse: 1620.1558 - mae: 28.6290 - val_loss: 2903.7942 - val_mse: 2903.7942 - val_mae: 36.5756\n",
      "Epoch 822/1000\n",
      "16763/16763 [==============================] - 0s 29us/step - loss: 1606.8219 - mse: 1606.8220 - mae: 28.4610 - val_loss: 2947.5632 - val_mse: 2947.5630 - val_mae: 36.8252\n",
      "Epoch 823/1000\n",
      "16763/16763 [==============================] - 0s 30us/step - loss: 1614.0843 - mse: 1614.0844 - mae: 28.4537 - val_loss: 3029.7843 - val_mse: 3029.7842 - val_mae: 37.9738\n",
      "Epoch 824/1000\n",
      "16763/16763 [==============================] - 0s 29us/step - loss: 1603.2052 - mse: 1603.2048 - mae: 28.3230 - val_loss: 2955.6730 - val_mse: 2955.6729 - val_mae: 36.8352\n",
      "Epoch 825/1000\n",
      "16763/16763 [==============================] - 0s 29us/step - loss: 1599.1063 - mse: 1599.1066 - mae: 28.3099 - val_loss: 2967.5002 - val_mse: 2967.5002 - val_mae: 36.8766\n",
      "Epoch 826/1000\n",
      "16763/16763 [==============================] - 0s 29us/step - loss: 1601.0851 - mse: 1601.0848 - mae: 28.4099 - val_loss: 2947.2965 - val_mse: 2947.2964 - val_mae: 36.8910\n",
      "Epoch 827/1000\n",
      "16763/16763 [==============================] - 1s 33us/step - loss: 1605.9872 - mse: 1605.9872 - mae: 28.4095 - val_loss: 3015.2487 - val_mse: 3015.2493 - val_mae: 37.0100\n",
      "Epoch 828/1000\n",
      "16763/16763 [==============================] - 1s 35us/step - loss: 1598.1883 - mse: 1598.1884 - mae: 28.2461 - val_loss: 2968.5491 - val_mse: 2968.5493 - val_mae: 36.6854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 829/1000\n",
      "16763/16763 [==============================] - 1s 35us/step - loss: 1597.8915 - mse: 1597.8916 - mae: 28.3639 - val_loss: 3159.0656 - val_mse: 3159.0652 - val_mae: 38.1483\n",
      "Epoch 830/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 1601.1080 - mse: 1601.1083 - mae: 28.4100 - val_loss: 2995.5606 - val_mse: 2995.5608 - val_mae: 37.2332\n",
      "Epoch 831/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 1601.4045 - mse: 1601.4045 - mae: 28.3200 - val_loss: 3045.8057 - val_mse: 3045.8059 - val_mae: 38.1306\n",
      "Epoch 832/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 1594.7942 - mse: 1594.7949 - mae: 28.2974 - val_loss: 2980.0116 - val_mse: 2980.0110 - val_mae: 37.2135\n",
      "Epoch 833/1000\n",
      "16763/16763 [==============================] - 1s 30us/step - loss: 1593.7759 - mse: 1593.7761 - mae: 28.2493 - val_loss: 3050.9399 - val_mse: 3050.9414 - val_mae: 37.3258\n",
      "Epoch 834/1000\n",
      "16763/16763 [==============================] - 0s 29us/step - loss: 1599.6599 - mse: 1599.6599 - mae: 28.3447 - val_loss: 2988.2707 - val_mse: 2988.2710 - val_mae: 37.5537\n",
      "Epoch 835/1000\n",
      "16763/16763 [==============================] - 0s 29us/step - loss: 1606.7583 - mse: 1606.7583 - mae: 28.4336 - val_loss: 3001.7701 - val_mse: 3001.7703 - val_mae: 37.2567\n",
      "Epoch 836/1000\n",
      "16763/16763 [==============================] - 0s 29us/step - loss: 1581.8303 - mse: 1581.8304 - mae: 28.2986 - val_loss: 2912.9162 - val_mse: 2912.9163 - val_mae: 36.5275\n",
      "Epoch 837/1000\n",
      "16763/16763 [==============================] - 0s 29us/step - loss: 1592.5214 - mse: 1592.5215 - mae: 28.3587 - val_loss: 2964.3911 - val_mse: 2964.3911 - val_mae: 36.9128\n",
      "Epoch 838/1000\n",
      "16763/16763 [==============================] - 0s 29us/step - loss: 1599.7109 - mse: 1599.7114 - mae: 28.3104 - val_loss: 3036.8665 - val_mse: 3036.8665 - val_mae: 37.7429\n",
      "Epoch 839/1000\n",
      "16763/16763 [==============================] - 0s 29us/step - loss: 1612.4148 - mse: 1612.4148 - mae: 28.4742 - val_loss: 3007.4901 - val_mse: 3007.4902 - val_mae: 37.2133\n",
      "Epoch 840/1000\n",
      "16763/16763 [==============================] - 1s 35us/step - loss: 1601.9835 - mse: 1601.9830 - mae: 28.3678 - val_loss: 2973.9359 - val_mse: 2973.9360 - val_mae: 37.1910\n",
      "Epoch 841/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 1626.6614 - mse: 1626.6613 - mae: 28.6013 - val_loss: 3038.2220 - val_mse: 3038.2219 - val_mae: 37.5053\n",
      "Epoch 842/1000\n",
      "16763/16763 [==============================] - 1s 35us/step - loss: 1586.2818 - mse: 1586.2819 - mae: 28.1611 - val_loss: 2983.2790 - val_mse: 2983.2791 - val_mae: 36.9820\n",
      "Epoch 843/1000\n",
      "16763/16763 [==============================] - 1s 35us/step - loss: 1606.1055 - mse: 1606.1049 - mae: 28.4521 - val_loss: 2991.5190 - val_mse: 2991.5195 - val_mae: 37.3151\n",
      "Epoch 844/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 1595.5631 - mse: 1595.5634 - mae: 28.3503 - val_loss: 3019.2158 - val_mse: 3019.2158 - val_mae: 37.2137\n",
      "Epoch 845/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 1596.4947 - mse: 1596.4946 - mae: 28.3056 - val_loss: 2966.7421 - val_mse: 2966.7422 - val_mae: 36.8954\n",
      "Epoch 846/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 1595.9137 - mse: 1595.9135 - mae: 28.3493 - val_loss: 3120.3137 - val_mse: 3120.3135 - val_mae: 37.8223\n",
      "Epoch 847/1000\n",
      "16763/16763 [==============================] - 1s 35us/step - loss: 1582.4920 - mse: 1582.4926 - mae: 28.1798 - val_loss: 2952.5391 - val_mse: 2952.5391 - val_mae: 36.8286\n",
      "Epoch 848/1000\n",
      "16763/16763 [==============================] - 0s 29us/step - loss: 1594.5220 - mse: 1594.5221 - mae: 28.3785 - val_loss: 3039.5459 - val_mse: 3039.5449 - val_mae: 37.6323\n",
      "Epoch 849/1000\n",
      "16763/16763 [==============================] - 0s 29us/step - loss: 1601.5217 - mse: 1601.5222 - mae: 28.3929 - val_loss: 2988.9479 - val_mse: 2988.9480 - val_mae: 37.4401\n",
      "Epoch 850/1000\n",
      "16763/16763 [==============================] - 1s 31us/step - loss: 1584.9920 - mse: 1584.9916 - mae: 28.2327 - val_loss: 3028.4581 - val_mse: 3028.4583 - val_mae: 37.4283\n",
      "Epoch 851/1000\n",
      "16763/16763 [==============================] - 1s 37us/step - loss: 1600.0551 - mse: 1600.0553 - mae: 28.4059 - val_loss: 3032.5521 - val_mse: 3032.5525 - val_mae: 37.3790\n",
      "Epoch 852/1000\n",
      "16763/16763 [==============================] - 1s 34us/step - loss: 1603.3540 - mse: 1603.3535 - mae: 28.3199 - val_loss: 3004.9380 - val_mse: 3004.9382 - val_mae: 37.2914\n",
      "Epoch 853/1000\n",
      "16763/16763 [==============================] - 1s 33us/step - loss: 1598.0814 - mse: 1598.0823 - mae: 28.3486 - val_loss: 3007.6432 - val_mse: 3007.6426 - val_mae: 37.1667\n",
      "Epoch 854/1000\n",
      "16763/16763 [==============================] - 1s 39us/step - loss: 1584.8509 - mse: 1584.8517 - mae: 28.2337 - val_loss: 3015.6313 - val_mse: 3015.6309 - val_mae: 38.0753\n",
      "Epoch 855/1000\n",
      "16763/16763 [==============================] - 1s 37us/step - loss: 1593.0266 - mse: 1593.0254 - mae: 28.2857 - val_loss: 2976.3194 - val_mse: 2976.3201 - val_mae: 36.7667\n",
      "Epoch 856/1000\n",
      "16763/16763 [==============================] - 1s 35us/step - loss: 1587.9610 - mse: 1587.9606 - mae: 28.2510 - val_loss: 2942.1961 - val_mse: 2942.1963 - val_mae: 36.8596\n",
      "Epoch 857/1000\n",
      "16763/16763 [==============================] - 1s 39us/step - loss: 1581.0219 - mse: 1581.0216 - mae: 28.2023 - val_loss: 2986.0590 - val_mse: 2986.0588 - val_mae: 36.8401\n",
      "Epoch 858/1000\n",
      "16763/16763 [==============================] - 1s 40us/step - loss: 1586.0264 - mse: 1586.0262 - mae: 28.3077 - val_loss: 2978.8058 - val_mse: 2978.8054 - val_mae: 37.5552\n",
      "Epoch 859/1000\n",
      "16763/16763 [==============================] - 1s 39us/step - loss: 1578.8193 - mse: 1578.8196 - mae: 28.1667 - val_loss: 3027.7651 - val_mse: 3027.7644 - val_mae: 37.1124\n",
      "Epoch 860/1000\n",
      "16763/16763 [==============================] - 1s 39us/step - loss: 1591.4791 - mse: 1591.4794 - mae: 28.3142 - val_loss: 3103.6172 - val_mse: 3103.6169 - val_mae: 38.3694\n",
      "Epoch 861/1000\n",
      "16763/16763 [==============================] - 1s 47us/step - loss: 1603.7340 - mse: 1603.7344 - mae: 28.4832 - val_loss: 2966.7113 - val_mse: 2966.7109 - val_mae: 36.8860\n",
      "Epoch 862/1000\n",
      "16763/16763 [==============================] - 1s 37us/step - loss: 1603.8177 - mse: 1603.8181 - mae: 28.4634 - val_loss: 3017.8084 - val_mse: 3017.8083 - val_mae: 37.1915\n",
      "Epoch 863/1000\n",
      "16763/16763 [==============================] - 1s 34us/step - loss: 1582.6269 - mse: 1582.6276 - mae: 28.2410 - val_loss: 3027.6204 - val_mse: 3027.6206 - val_mae: 38.1953\n",
      "Epoch 864/1000\n",
      "16763/16763 [==============================] - 1s 33us/step - loss: 1574.1583 - mse: 1574.1578 - mae: 28.1184 - val_loss: 3073.7156 - val_mse: 3073.7158 - val_mae: 37.3495\n",
      "Epoch 865/1000\n",
      "16763/16763 [==============================] - 1s 32us/step - loss: 1564.4402 - mse: 1564.4412 - mae: 28.0920 - val_loss: 2914.5039 - val_mse: 2914.5039 - val_mae: 36.6018\n",
      "Epoch 866/1000\n",
      "16763/16763 [==============================] - 0s 29us/step - loss: 1588.6858 - mse: 1588.6854 - mae: 28.3638 - val_loss: 3069.9191 - val_mse: 3069.9189 - val_mae: 38.2222\n",
      "Epoch 867/1000\n",
      "16763/16763 [==============================] - 0s 30us/step - loss: 1582.4647 - mse: 1582.4648 - mae: 28.2372 - val_loss: 3003.4916 - val_mse: 3003.4917 - val_mae: 37.2010\n",
      "Epoch 868/1000\n",
      "16763/16763 [==============================] - 0s 29us/step - loss: 1573.2055 - mse: 1573.2046 - mae: 28.1267 - val_loss: 3087.3805 - val_mse: 3087.3811 - val_mae: 38.8299\n",
      "Epoch 869/1000\n",
      "16763/16763 [==============================] - 1s 33us/step - loss: 1589.6466 - mse: 1589.6466 - mae: 28.3524 - val_loss: 2932.2492 - val_mse: 2932.2488 - val_mae: 36.7102\n",
      "Epoch 870/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 1577.9526 - mse: 1577.9526 - mae: 28.2113 - val_loss: 2985.0637 - val_mse: 2985.0637 - val_mae: 37.4691\n",
      "Epoch 871/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 1589.9102 - mse: 1589.9099 - mae: 28.2541 - val_loss: 2967.1448 - val_mse: 2967.1450 - val_mae: 36.6982\n",
      "Epoch 872/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 1568.1840 - mse: 1568.1847 - mae: 28.1221 - val_loss: 2960.2337 - val_mse: 2960.2344 - val_mae: 37.3963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 873/1000\n",
      "16763/16763 [==============================] - 1s 35us/step - loss: 1578.9672 - mse: 1578.9680 - mae: 28.1544 - val_loss: 2967.1104 - val_mse: 2967.1113 - val_mae: 36.8654\n",
      "Epoch 874/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 1564.5645 - mse: 1564.5653 - mae: 28.0603 - val_loss: 2917.2068 - val_mse: 2917.2070 - val_mae: 36.7315\n",
      "Epoch 875/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 1578.7221 - mse: 1578.7216 - mae: 28.2086 - val_loss: 2985.4318 - val_mse: 2985.4316 - val_mae: 37.6054\n",
      "Epoch 876/1000\n",
      "16763/16763 [==============================] - 1s 35us/step - loss: 1586.1186 - mse: 1586.1185 - mae: 28.2883 - val_loss: 2972.9400 - val_mse: 2972.9399 - val_mae: 37.0704\n",
      "Epoch 877/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 1580.4252 - mse: 1580.4265 - mae: 28.2300 - val_loss: 2938.8184 - val_mse: 2938.8179 - val_mae: 36.9068\n",
      "Epoch 878/1000\n",
      "16763/16763 [==============================] - 1s 35us/step - loss: 1590.8193 - mse: 1590.8193 - mae: 28.3172 - val_loss: 2950.7832 - val_mse: 2950.7832 - val_mae: 36.6832\n",
      "Epoch 879/1000\n",
      "16763/16763 [==============================] - 1s 30us/step - loss: 1569.2050 - mse: 1569.2045 - mae: 28.1077 - val_loss: 3204.1865 - val_mse: 3204.1873 - val_mae: 38.3434\n",
      "Epoch 880/1000\n",
      "16763/16763 [==============================] - 0s 29us/step - loss: 1576.5018 - mse: 1576.5023 - mae: 28.1944 - val_loss: 2972.8344 - val_mse: 2972.8350 - val_mae: 36.9711\n",
      "Epoch 881/1000\n",
      "16763/16763 [==============================] - 0s 29us/step - loss: 1571.8031 - mse: 1571.8030 - mae: 28.1348 - val_loss: 3068.1637 - val_mse: 3068.1636 - val_mae: 37.4068\n",
      "Epoch 882/1000\n",
      "16763/16763 [==============================] - 0s 29us/step - loss: 1588.4306 - mse: 1588.4310 - mae: 28.2797 - val_loss: 3011.6742 - val_mse: 3011.6738 - val_mae: 36.9063\n",
      "Epoch 883/1000\n",
      "16763/16763 [==============================] - 0s 29us/step - loss: 1570.8594 - mse: 1570.8593 - mae: 28.2360 - val_loss: 3155.9014 - val_mse: 3155.9019 - val_mae: 39.1363\n",
      "Epoch 884/1000\n",
      "16763/16763 [==============================] - 0s 29us/step - loss: 1583.6783 - mse: 1583.6780 - mae: 28.2572 - val_loss: 3016.9773 - val_mse: 3016.9778 - val_mae: 37.6440\n",
      "Epoch 885/1000\n",
      "16763/16763 [==============================] - 0s 29us/step - loss: 1563.7074 - mse: 1563.7079 - mae: 28.0373 - val_loss: 3021.1971 - val_mse: 3021.1963 - val_mae: 37.5406\n",
      "Epoch 886/1000\n",
      "16763/16763 [==============================] - 1s 34us/step - loss: 1586.4200 - mse: 1586.4204 - mae: 28.2820 - val_loss: 3009.7285 - val_mse: 3009.7280 - val_mae: 37.1462\n",
      "Epoch 887/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 1579.6066 - mse: 1579.6071 - mae: 28.2765 - val_loss: 3061.1649 - val_mse: 3061.1653 - val_mae: 38.3916\n",
      "Epoch 888/1000\n",
      "16763/16763 [==============================] - 1s 35us/step - loss: 1563.1269 - mse: 1563.1273 - mae: 28.0568 - val_loss: 2956.2024 - val_mse: 2956.2031 - val_mae: 37.0162\n",
      "Epoch 889/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 1571.7293 - mse: 1571.7289 - mae: 28.1646 - val_loss: 2983.3032 - val_mse: 2983.3035 - val_mae: 36.9660\n",
      "Epoch 890/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 1559.9504 - mse: 1559.9508 - mae: 28.0615 - val_loss: 3012.2118 - val_mse: 3012.2117 - val_mae: 37.1528\n",
      "Epoch 891/1000\n",
      "16763/16763 [==============================] - 1s 35us/step - loss: 1568.1447 - mse: 1568.1443 - mae: 28.1600 - val_loss: 2985.2022 - val_mse: 2985.2021 - val_mae: 36.9354\n",
      "Epoch 892/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 1574.9066 - mse: 1574.9069 - mae: 28.2459 - val_loss: 2912.6239 - val_mse: 2912.6248 - val_mae: 36.7466\n",
      "Epoch 893/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 1559.7815 - mse: 1559.7815 - mae: 28.1297 - val_loss: 2918.6610 - val_mse: 2918.6611 - val_mae: 36.5926\n",
      "Epoch 894/1000\n",
      "16763/16763 [==============================] - 1s 35us/step - loss: 1557.1319 - mse: 1557.1316 - mae: 27.9735 - val_loss: 2935.0023 - val_mse: 2935.0020 - val_mae: 36.7338\n",
      "Epoch 895/1000\n",
      "16763/16763 [==============================] - 1s 37us/step - loss: 1560.1226 - mse: 1560.1217 - mae: 28.1121 - val_loss: 2959.4294 - val_mse: 2959.4297 - val_mae: 37.2515\n",
      "Epoch 896/1000\n",
      "16763/16763 [==============================] - 1s 42us/step - loss: 1553.3771 - mse: 1553.3774 - mae: 27.9378 - val_loss: 2992.7844 - val_mse: 2992.7847 - val_mae: 37.5184\n",
      "Epoch 897/1000\n",
      "16763/16763 [==============================] - 1s 39us/step - loss: 1559.5305 - mse: 1559.5305 - mae: 28.1283 - val_loss: 2995.6137 - val_mse: 2995.6143 - val_mae: 37.1131\n",
      "Epoch 898/1000\n",
      "16763/16763 [==============================] - 1s 35us/step - loss: 1575.9505 - mse: 1575.9504 - mae: 28.2323 - val_loss: 2960.9086 - val_mse: 2960.9089 - val_mae: 37.1251\n",
      "Epoch 899/1000\n",
      "16763/16763 [==============================] - 1s 31us/step - loss: 1568.1944 - mse: 1568.1951 - mae: 28.2167 - val_loss: 2971.3907 - val_mse: 2971.3906 - val_mae: 36.8398\n",
      "Epoch 900/1000\n",
      "16763/16763 [==============================] - 0s 29us/step - loss: 1572.7995 - mse: 1572.8002 - mae: 28.1228 - val_loss: 3065.7787 - val_mse: 3065.7788 - val_mae: 37.5609\n",
      "Epoch 901/1000\n",
      "16763/16763 [==============================] - 0s 29us/step - loss: 1569.5115 - mse: 1569.5114 - mae: 28.1728 - val_loss: 3001.1664 - val_mse: 3001.1667 - val_mae: 37.2332\n",
      "Epoch 902/1000\n",
      "16763/16763 [==============================] - 0s 29us/step - loss: 1559.2531 - mse: 1559.2539 - mae: 28.0815 - val_loss: 2966.0883 - val_mse: 2966.0886 - val_mae: 37.1863\n",
      "Epoch 903/1000\n",
      "16763/16763 [==============================] - 1s 31us/step - loss: 1560.9282 - mse: 1560.9275 - mae: 28.0596 - val_loss: 2991.0482 - val_mse: 2991.0486 - val_mae: 37.2274\n",
      "Epoch 904/1000\n",
      "16763/16763 [==============================] - 0s 29us/step - loss: 1564.1725 - mse: 1564.1722 - mae: 28.1106 - val_loss: 3000.7729 - val_mse: 3000.7729 - val_mae: 36.9795\n",
      "Epoch 905/1000\n",
      "16763/16763 [==============================] - 0s 29us/step - loss: 1552.6974 - mse: 1552.6968 - mae: 27.9705 - val_loss: 2974.0836 - val_mse: 2974.0840 - val_mae: 36.8288\n",
      "Epoch 906/1000\n",
      "16763/16763 [==============================] - 1s 33us/step - loss: 1551.7035 - mse: 1551.7039 - mae: 28.0505 - val_loss: 2972.3646 - val_mse: 2972.3640 - val_mae: 37.0147\n",
      "Epoch 907/1000\n",
      "16763/16763 [==============================] - 1s 35us/step - loss: 1576.1211 - mse: 1576.1210 - mae: 28.3753 - val_loss: 2942.9320 - val_mse: 2942.9309 - val_mae: 36.7948\n",
      "Epoch 908/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 1555.3392 - mse: 1555.3389 - mae: 27.9627 - val_loss: 3141.8975 - val_mse: 3141.8975 - val_mae: 38.1504\n",
      "Epoch 909/1000\n",
      "16763/16763 [==============================] - 1s 45us/step - loss: 1570.1877 - mse: 1570.1881 - mae: 28.1974 - val_loss: 2978.7662 - val_mse: 2978.7671 - val_mae: 37.3650\n",
      "Epoch 910/1000\n",
      "16763/16763 [==============================] - 1s 40us/step - loss: 1565.8290 - mse: 1565.8281 - mae: 28.1473 - val_loss: 2979.1627 - val_mse: 2979.1626 - val_mae: 36.8909\n",
      "Epoch 911/1000\n",
      "16763/16763 [==============================] - 1s 40us/step - loss: 1535.3198 - mse: 1535.3187 - mae: 27.8320 - val_loss: 3016.7983 - val_mse: 3016.7981 - val_mae: 37.0424\n",
      "Epoch 912/1000\n",
      "16763/16763 [==============================] - 1s 40us/step - loss: 1554.1936 - mse: 1554.1946 - mae: 28.0976 - val_loss: 2955.8062 - val_mse: 2955.8059 - val_mae: 36.7171\n",
      "Epoch 913/1000\n",
      "16763/16763 [==============================] - 1s 31us/step - loss: 1543.3525 - mse: 1543.3531 - mae: 27.8955 - val_loss: 3001.5617 - val_mse: 3001.5620 - val_mae: 37.4462\n",
      "Epoch 914/1000\n",
      "16763/16763 [==============================] - 1s 33us/step - loss: 1557.5322 - mse: 1557.5325 - mae: 28.1325 - val_loss: 2936.1382 - val_mse: 2936.1384 - val_mae: 36.5765\n",
      "Epoch 915/1000\n",
      "16763/16763 [==============================] - 0s 29us/step - loss: 1575.5739 - mse: 1575.5743 - mae: 28.1938 - val_loss: 3040.4047 - val_mse: 3040.4055 - val_mae: 37.4647\n",
      "Epoch 916/1000\n",
      "16763/16763 [==============================] - 1s 30us/step - loss: 1554.1244 - mse: 1554.1240 - mae: 28.1151 - val_loss: 2952.0597 - val_mse: 2952.0596 - val_mae: 37.3800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 917/1000\n",
      "16763/16763 [==============================] - 1s 32us/step - loss: 1551.4164 - mse: 1551.4169 - mae: 27.9872 - val_loss: 3019.1159 - val_mse: 3019.1160 - val_mae: 37.2965\n",
      "Epoch 918/1000\n",
      "16763/16763 [==============================] - 1s 34us/step - loss: 1562.9936 - mse: 1562.9930 - mae: 28.0547 - val_loss: 3000.6093 - val_mse: 3000.6091 - val_mae: 37.8420\n",
      "Epoch 919/1000\n",
      "16763/16763 [==============================] - 1s 41us/step - loss: 1546.9161 - mse: 1546.9158 - mae: 27.9441 - val_loss: 2946.4197 - val_mse: 2946.4197 - val_mae: 36.8127\n",
      "Epoch 920/1000\n",
      "16763/16763 [==============================] - 1s 51us/step - loss: 1560.4549 - mse: 1560.4558 - mae: 28.0923 - val_loss: 3002.3581 - val_mse: 3002.3582 - val_mae: 37.1194\n",
      "Epoch 921/1000\n",
      "16763/16763 [==============================] - 1s 43us/step - loss: 1544.2929 - mse: 1544.2931 - mae: 27.9789 - val_loss: 2939.5659 - val_mse: 2939.5662 - val_mae: 36.8682\n",
      "Epoch 922/1000\n",
      "16763/16763 [==============================] - 1s 50us/step - loss: 1545.2181 - mse: 1545.2183 - mae: 27.8738 - val_loss: 3006.1696 - val_mse: 3006.1694 - val_mae: 37.2342\n",
      "Epoch 923/1000\n",
      "16763/16763 [==============================] - 1s 44us/step - loss: 1549.5374 - mse: 1549.5374 - mae: 27.9703 - val_loss: 3064.4359 - val_mse: 3064.4355 - val_mae: 37.6596\n",
      "Epoch 924/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 1557.6626 - mse: 1557.6624 - mae: 28.1476 - val_loss: 2950.2593 - val_mse: 2950.2583 - val_mae: 37.0637\n",
      "Epoch 925/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 1556.2779 - mse: 1556.2778 - mae: 28.0956 - val_loss: 2985.9767 - val_mse: 2985.9771 - val_mae: 36.8180\n",
      "Epoch 926/1000\n",
      "16763/16763 [==============================] - 1s 41us/step - loss: 1541.1806 - mse: 1541.1804 - mae: 27.9085 - val_loss: 2965.3351 - val_mse: 2965.3350 - val_mae: 37.1821\n",
      "Epoch 927/1000\n",
      "16763/16763 [==============================] - 1s 41us/step - loss: 1554.4698 - mse: 1554.4697 - mae: 28.0095 - val_loss: 3005.9137 - val_mse: 3005.9141 - val_mae: 37.5338\n",
      "Epoch 928/1000\n",
      "16763/16763 [==============================] - 1s 41us/step - loss: 1557.9872 - mse: 1557.9873 - mae: 28.0682 - val_loss: 3074.7305 - val_mse: 3074.7305 - val_mae: 38.7707\n",
      "Epoch 929/1000\n",
      "16763/16763 [==============================] - 1s 49us/step - loss: 1548.2817 - mse: 1548.2817 - mae: 27.9845 - val_loss: 2969.6292 - val_mse: 2969.6284 - val_mae: 36.9138\n",
      "Epoch 930/1000\n",
      "16763/16763 [==============================] - 1s 47us/step - loss: 1547.9410 - mse: 1547.9408 - mae: 27.9254 - val_loss: 2980.7938 - val_mse: 2980.7930 - val_mae: 36.8427\n",
      "Epoch 931/1000\n",
      "16763/16763 [==============================] - 1s 52us/step - loss: 1536.3302 - mse: 1536.3301 - mae: 27.8352 - val_loss: 3101.9972 - val_mse: 3101.9973 - val_mae: 37.6204\n",
      "Epoch 932/1000\n",
      "16763/16763 [==============================] - 1s 52us/step - loss: 1538.7231 - mse: 1538.7230 - mae: 27.8977 - val_loss: 2964.1075 - val_mse: 2964.1074 - val_mae: 37.1509\n",
      "Epoch 933/1000\n",
      "16763/16763 [==============================] - 1s 52us/step - loss: 1538.6665 - mse: 1538.6666 - mae: 27.8696 - val_loss: 3037.6836 - val_mse: 3037.6836 - val_mae: 37.9169\n",
      "Epoch 934/1000\n",
      "16763/16763 [==============================] - 1s 51us/step - loss: 1547.7414 - mse: 1547.7412 - mae: 28.0077 - val_loss: 2988.9022 - val_mse: 2988.9023 - val_mae: 37.8913\n",
      "Epoch 935/1000\n",
      "16763/16763 [==============================] - 1s 49us/step - loss: 1551.8674 - mse: 1551.8677 - mae: 28.0877 - val_loss: 2987.7332 - val_mse: 2987.7332 - val_mae: 37.0355\n",
      "Epoch 936/1000\n",
      "16763/16763 [==============================] - 1s 50us/step - loss: 1540.2409 - mse: 1540.2416 - mae: 27.9682 - val_loss: 2999.5804 - val_mse: 2999.5803 - val_mae: 37.1167\n",
      "Epoch 937/1000\n",
      "16763/16763 [==============================] - 1s 45us/step - loss: 1529.9668 - mse: 1529.9668 - mae: 27.8357 - val_loss: 2956.9304 - val_mse: 2956.9304 - val_mae: 36.7972\n",
      "Epoch 938/1000\n",
      "16763/16763 [==============================] - 1s 48us/step - loss: 1541.7986 - mse: 1541.7990 - mae: 27.9101 - val_loss: 2965.4690 - val_mse: 2965.4690 - val_mae: 36.8792\n",
      "Epoch 939/1000\n",
      "16763/16763 [==============================] - 1s 44us/step - loss: 1536.3960 - mse: 1536.3961 - mae: 27.8732 - val_loss: 3015.5584 - val_mse: 3015.5583 - val_mae: 37.2432\n",
      "Epoch 940/1000\n",
      "16763/16763 [==============================] - 1s 44us/step - loss: 1544.8979 - mse: 1544.8979 - mae: 28.0203 - val_loss: 2964.5510 - val_mse: 2964.5518 - val_mae: 36.9428\n",
      "Epoch 941/1000\n",
      "16763/16763 [==============================] - 1s 42us/step - loss: 1531.8726 - mse: 1531.8724 - mae: 27.7919 - val_loss: 3076.8345 - val_mse: 3076.8350 - val_mae: 37.8142\n",
      "Epoch 942/1000\n",
      "16763/16763 [==============================] - 1s 40us/step - loss: 1545.0984 - mse: 1545.0980 - mae: 27.9548 - val_loss: 3007.1477 - val_mse: 3007.1482 - val_mae: 37.1655\n",
      "Epoch 943/1000\n",
      "16763/16763 [==============================] - 1s 45us/step - loss: 1539.0864 - mse: 1539.0872 - mae: 27.9735 - val_loss: 2978.9469 - val_mse: 2978.9473 - val_mae: 37.0359\n",
      "Epoch 944/1000\n",
      "16763/16763 [==============================] - 1s 51us/step - loss: 1520.2616 - mse: 1520.2612 - mae: 27.7656 - val_loss: 3000.1665 - val_mse: 3000.1660 - val_mae: 37.4325\n",
      "Epoch 945/1000\n",
      "16763/16763 [==============================] - 1s 66us/step - loss: 1524.6408 - mse: 1524.6411 - mae: 27.8215 - val_loss: 2920.8204 - val_mse: 2920.8203 - val_mae: 36.7212\n",
      "Epoch 946/1000\n",
      "16763/16763 [==============================] - 1s 73us/step - loss: 1534.1875 - mse: 1534.1874 - mae: 27.9299 - val_loss: 2938.4309 - val_mse: 2938.4309 - val_mae: 36.5511\n",
      "Epoch 947/1000\n",
      "16763/16763 [==============================] - 1s 41us/step - loss: 1542.5012 - mse: 1542.5006 - mae: 27.9292 - val_loss: 2915.6798 - val_mse: 2915.6799 - val_mae: 36.7296\n",
      "Epoch 948/1000\n",
      "16763/16763 [==============================] - 1s 37us/step - loss: 1536.4410 - mse: 1536.4408 - mae: 27.9658 - val_loss: 2962.5883 - val_mse: 2962.5879 - val_mae: 37.1215\n",
      "Epoch 949/1000\n",
      "16763/16763 [==============================] - 1s 46us/step - loss: 1538.4354 - mse: 1538.4353 - mae: 27.8966 - val_loss: 3064.7594 - val_mse: 3064.7593 - val_mae: 38.1103\n",
      "Epoch 950/1000\n",
      "16763/16763 [==============================] - 1s 41us/step - loss: 1543.3882 - mse: 1543.3879 - mae: 27.9426 - val_loss: 3161.1664 - val_mse: 3161.1655 - val_mae: 38.1981\n",
      "Epoch 951/1000\n",
      "16763/16763 [==============================] - 1s 38us/step - loss: 1527.4589 - mse: 1527.4591 - mae: 27.8959 - val_loss: 3005.6356 - val_mse: 3005.6367 - val_mae: 36.9329\n",
      "Epoch 952/1000\n",
      "16763/16763 [==============================] - 1s 35us/step - loss: 1552.9131 - mse: 1552.9132 - mae: 28.0242 - val_loss: 2936.5198 - val_mse: 2936.5200 - val_mae: 36.6568\n",
      "Epoch 953/1000\n",
      "16763/16763 [==============================] - 1s 33us/step - loss: 1528.6154 - mse: 1528.6155 - mae: 27.7670 - val_loss: 3006.4952 - val_mse: 3006.4956 - val_mae: 37.1212\n",
      "Epoch 954/1000\n",
      "16763/16763 [==============================] - 1s 32us/step - loss: 1542.1743 - mse: 1542.1742 - mae: 28.0167 - val_loss: 3187.2850 - val_mse: 3187.2849 - val_mae: 39.4849\n",
      "Epoch 955/1000\n",
      "16763/16763 [==============================] - 1s 36us/step - loss: 1548.8174 - mse: 1548.8170 - mae: 28.0780 - val_loss: 2972.9168 - val_mse: 2972.9177 - val_mae: 37.3830\n",
      "Epoch 956/1000\n",
      "16763/16763 [==============================] - 1s 40us/step - loss: 1535.1467 - mse: 1535.1461 - mae: 27.9415 - val_loss: 2975.6831 - val_mse: 2975.6831 - val_mae: 37.0563\n",
      "Epoch 957/1000\n",
      "16763/16763 [==============================] - 1s 40us/step - loss: 1532.9460 - mse: 1532.9457 - mae: 27.8435 - val_loss: 2993.5610 - val_mse: 2993.5608 - val_mae: 36.9829\n",
      "Epoch 958/1000\n",
      "16763/16763 [==============================] - 1s 43us/step - loss: 1528.9039 - mse: 1528.9034 - mae: 27.7636 - val_loss: 3000.3156 - val_mse: 3000.3159 - val_mae: 37.1036\n",
      "Epoch 959/1000\n",
      "16763/16763 [==============================] - 1s 43us/step - loss: 1513.8471 - mse: 1513.8468 - mae: 27.6803 - val_loss: 2919.3034 - val_mse: 2919.3040 - val_mae: 36.5399\n",
      "Epoch 960/1000\n",
      "16763/16763 [==============================] - 1s 41us/step - loss: 1528.3857 - mse: 1528.3860 - mae: 27.9068 - val_loss: 2977.4088 - val_mse: 2977.4087 - val_mae: 36.9398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 961/1000\n",
      "16763/16763 [==============================] - 1s 39us/step - loss: 1511.4240 - mse: 1511.4243 - mae: 27.7086 - val_loss: 2904.5136 - val_mse: 2904.5132 - val_mae: 36.6053\n",
      "Epoch 962/1000\n",
      "16763/16763 [==============================] - 1s 42us/step - loss: 1536.1359 - mse: 1536.1367 - mae: 27.9943 - val_loss: 2959.9449 - val_mse: 2959.9448 - val_mae: 36.9211\n",
      "Epoch 963/1000\n",
      "16763/16763 [==============================] - 1s 46us/step - loss: 1527.4502 - mse: 1527.4501 - mae: 27.9526 - val_loss: 3036.1429 - val_mse: 3036.1431 - val_mae: 37.5754\n",
      "Epoch 964/1000\n",
      "16763/16763 [==============================] - 1s 41us/step - loss: 1524.5711 - mse: 1524.5714 - mae: 27.8143 - val_loss: 3043.3725 - val_mse: 3043.3733 - val_mae: 37.5615\n",
      "Epoch 965/1000\n",
      "16763/16763 [==============================] - 1s 43us/step - loss: 1512.4542 - mse: 1512.4535 - mae: 27.8146 - val_loss: 3118.0269 - val_mse: 3118.0273 - val_mae: 38.4840\n",
      "Epoch 966/1000\n",
      "16763/16763 [==============================] - 1s 42us/step - loss: 1542.7716 - mse: 1542.7714 - mae: 28.0273 - val_loss: 2963.6896 - val_mse: 2963.6887 - val_mae: 36.8716\n",
      "Epoch 967/1000\n",
      "16763/16763 [==============================] - 1s 44us/step - loss: 1513.2797 - mse: 1513.2797 - mae: 27.7301 - val_loss: 2964.2909 - val_mse: 2964.2905 - val_mae: 37.0023\n",
      "Epoch 968/1000\n",
      "16763/16763 [==============================] - 1s 41us/step - loss: 1535.6889 - mse: 1535.6891 - mae: 27.9026 - val_loss: 2958.1076 - val_mse: 2958.1072 - val_mae: 36.6736\n",
      "Epoch 969/1000\n",
      "16763/16763 [==============================] - 1s 44us/step - loss: 1540.9953 - mse: 1540.9968 - mae: 28.0445 - val_loss: 2952.5459 - val_mse: 2952.5454 - val_mae: 36.9977\n",
      "Epoch 970/1000\n",
      "16763/16763 [==============================] - 1s 41us/step - loss: 1511.4004 - mse: 1511.3995 - mae: 27.6397 - val_loss: 3014.5038 - val_mse: 3014.5037 - val_mae: 37.9514\n",
      "Epoch 971/1000\n",
      "16763/16763 [==============================] - 1s 42us/step - loss: 1522.6244 - mse: 1522.6243 - mae: 27.8041 - val_loss: 2934.2751 - val_mse: 2934.2754 - val_mae: 36.6961\n",
      "Epoch 972/1000\n",
      "16763/16763 [==============================] - 1s 49us/step - loss: 1525.0110 - mse: 1525.0106 - mae: 27.8080 - val_loss: 2975.7989 - val_mse: 2975.7996 - val_mae: 36.9317\n",
      "Epoch 973/1000\n",
      "16763/16763 [==============================] - 1s 45us/step - loss: 1534.0860 - mse: 1534.0854 - mae: 27.9430 - val_loss: 2944.5109 - val_mse: 2944.5112 - val_mae: 36.8090\n",
      "Epoch 974/1000\n",
      "16763/16763 [==============================] - 1s 52us/step - loss: 1518.1939 - mse: 1518.1929 - mae: 27.7754 - val_loss: 2964.2358 - val_mse: 2964.2361 - val_mae: 37.1294\n",
      "Epoch 975/1000\n",
      "16763/16763 [==============================] - 1s 45us/step - loss: 1525.9560 - mse: 1525.9558 - mae: 27.7911 - val_loss: 3019.3111 - val_mse: 3019.3115 - val_mae: 37.1933\n",
      "Epoch 976/1000\n",
      "16763/16763 [==============================] - 1s 45us/step - loss: 1524.2712 - mse: 1524.2708 - mae: 27.7894 - val_loss: 2956.8024 - val_mse: 2956.8025 - val_mae: 36.7308\n",
      "Epoch 977/1000\n",
      "16763/16763 [==============================] - 1s 43us/step - loss: 1521.1878 - mse: 1521.1875 - mae: 27.7906 - val_loss: 2922.1387 - val_mse: 2922.1396 - val_mae: 36.5382\n",
      "Epoch 978/1000\n",
      "16763/16763 [==============================] - 1s 48us/step - loss: 1527.5444 - mse: 1527.5447 - mae: 27.8985 - val_loss: 3018.5475 - val_mse: 3018.5479 - val_mae: 37.1307\n",
      "Epoch 979/1000\n",
      "16763/16763 [==============================] - 1s 51us/step - loss: 1522.4115 - mse: 1522.4115 - mae: 27.8224 - val_loss: 2925.3334 - val_mse: 2925.3340 - val_mae: 36.5920\n",
      "Epoch 980/1000\n",
      "16763/16763 [==============================] - 1s 52us/step - loss: 1518.1449 - mse: 1518.1447 - mae: 27.7197 - val_loss: 2965.8395 - val_mse: 2965.8396 - val_mae: 36.9745\n",
      "Epoch 981/1000\n",
      "16763/16763 [==============================] - 1s 47us/step - loss: 1525.1507 - mse: 1525.1500 - mae: 27.7874 - val_loss: 2980.4157 - val_mse: 2980.4155 - val_mae: 37.2331\n",
      "Epoch 982/1000\n",
      "16763/16763 [==============================] - 1s 48us/step - loss: 1536.8101 - mse: 1536.8097 - mae: 27.9640 - val_loss: 2980.0091 - val_mse: 2980.0098 - val_mae: 37.0700\n",
      "Epoch 983/1000\n",
      "16763/16763 [==============================] - 1s 48us/step - loss: 1530.1777 - mse: 1530.1780 - mae: 27.8679 - val_loss: 2926.4973 - val_mse: 2926.4973 - val_mae: 36.7423\n",
      "Epoch 984/1000\n",
      "16763/16763 [==============================] - 1s 51us/step - loss: 1527.0832 - mse: 1527.0833 - mae: 27.8962 - val_loss: 2993.7273 - val_mse: 2993.7271 - val_mae: 37.7613\n",
      "Epoch 985/1000\n",
      "16763/16763 [==============================] - 1s 47us/step - loss: 1509.5437 - mse: 1509.5433 - mae: 27.6701 - val_loss: 2938.3067 - val_mse: 2938.3074 - val_mae: 36.7099\n",
      "Epoch 986/1000\n",
      "16763/16763 [==============================] - 1s 40us/step - loss: 1510.9620 - mse: 1510.9620 - mae: 27.6695 - val_loss: 3013.9559 - val_mse: 3013.9556 - val_mae: 36.9879\n",
      "Epoch 987/1000\n",
      "16763/16763 [==============================] - 1s 42us/step - loss: 1509.5489 - mse: 1509.5490 - mae: 27.6428 - val_loss: 2962.0592 - val_mse: 2962.0591 - val_mae: 37.2601\n",
      "Epoch 988/1000\n",
      "16763/16763 [==============================] - 1s 42us/step - loss: 1526.5418 - mse: 1526.5416 - mae: 27.8099 - val_loss: 3033.5511 - val_mse: 3033.5513 - val_mae: 37.7588\n",
      "Epoch 989/1000\n",
      "16763/16763 [==============================] - 1s 42us/step - loss: 1508.6454 - mse: 1508.6454 - mae: 27.7563 - val_loss: 2919.4345 - val_mse: 2919.4338 - val_mae: 36.9911\n",
      "Epoch 990/1000\n",
      "16763/16763 [==============================] - 1s 41us/step - loss: 1522.6403 - mse: 1522.6407 - mae: 27.8209 - val_loss: 3029.3790 - val_mse: 3029.3794 - val_mae: 37.6759\n",
      "Epoch 991/1000\n",
      "16763/16763 [==============================] - 1s 41us/step - loss: 1499.5285 - mse: 1499.5286 - mae: 27.6442 - val_loss: 2920.1155 - val_mse: 2920.1157 - val_mae: 36.7181\n",
      "Epoch 992/1000\n",
      "16763/16763 [==============================] - 1s 44us/step - loss: 1506.6869 - mse: 1506.6870 - mae: 27.6125 - val_loss: 2954.9453 - val_mse: 2954.9453 - val_mae: 36.6324\n",
      "Epoch 993/1000\n",
      "16763/16763 [==============================] - 1s 41us/step - loss: 1527.6534 - mse: 1527.6528 - mae: 27.9324 - val_loss: 2891.2192 - val_mse: 2891.2190 - val_mae: 36.6863\n",
      "Epoch 994/1000\n",
      "16763/16763 [==============================] - 1s 42us/step - loss: 1541.4539 - mse: 1541.4534 - mae: 28.0480 - val_loss: 2931.9060 - val_mse: 2931.9060 - val_mae: 37.1487\n",
      "Epoch 995/1000\n",
      "16763/16763 [==============================] - 1s 43us/step - loss: 1498.9754 - mse: 1498.9745 - mae: 27.6091 - val_loss: 2911.3745 - val_mse: 2911.3745 - val_mae: 36.5832\n",
      "Epoch 996/1000\n",
      "16763/16763 [==============================] - 1s 43us/step - loss: 1493.2396 - mse: 1493.2389 - mae: 27.4704 - val_loss: 2979.1938 - val_mse: 2979.1931 - val_mae: 36.7515\n",
      "Epoch 997/1000\n",
      "16763/16763 [==============================] - 1s 42us/step - loss: 1503.2848 - mse: 1503.2853 - mae: 27.7070 - val_loss: 2962.3924 - val_mse: 2962.3921 - val_mae: 36.9149\n",
      "Epoch 998/1000\n",
      "16763/16763 [==============================] - 1s 42us/step - loss: 1512.5695 - mse: 1512.5698 - mae: 27.7352 - val_loss: 2934.6364 - val_mse: 2934.6362 - val_mae: 36.8127\n",
      "Epoch 999/1000\n",
      "16763/16763 [==============================] - 1s 44us/step - loss: 1519.9395 - mse: 1519.9392 - mae: 27.8679 - val_loss: 3071.9570 - val_mse: 3071.9568 - val_mae: 37.3187\n",
      "Epoch 1000/1000\n",
      "16763/16763 [==============================] - 1s 43us/step - loss: 1518.4622 - mse: 1518.4622 - mae: 27.7811 - val_loss: 3008.0825 - val_mse: 3008.0818 - val_mae: 37.3156\n",
      "Executing op __inference_keras_scratch_graph_3891820 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "53.284839166583005\n",
      "42.003333147782065\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "with tf.device('/CPU:0'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, input_dim=13, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.summary()\n",
    "    #Fit\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['mse','mae'])\n",
    "    history = model.fit(scaler.transform(Xtrain), ytrain, epochs=1000, batch_size=50,  verbose=1, validation_split=0.2)\n",
    "    #Print Accuracy\n",
    "    testPred = model.predict(scaler.transform(Xtest))\n",
    "    trainPred = model.predict(scaler.transform(Xtrain))\n",
    "    print(mean_squared_error(testPred, ytest,squared=False))\n",
    "    print(mean_squared_error(trainPred, ytrain,squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
