{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RKP = \"DL031\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "sns.set_theme(style=\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\confusement\\miniconda3\\envs\\mlc\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3263: DtypeWarning: Columns (15) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['StationId', 'Datetime', 'PM2.5', 'PM10', 'NO', 'NO2', 'NOx', 'NH3',\n",
      "       'CO', 'SO2', 'O3', 'Benzene', 'Toluene', 'Xylene', 'AQI', 'AQI_Bucket'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Load datasets and rename columns, load all aqi data but specify metro data name\n",
    "def loadcsv(city=\"./data/rkpuram.csv\"):\n",
    "    met = pd.read_csv(city,delimiter=';',skiprows=24)\n",
    "    aqi = pd.read_csv('./data/station_hour.csv')\n",
    "    print(aqi.columns)\n",
    "    met.rename(columns={'# Date': 'Date',}, inplace=True)\n",
    "    met.rename(columns={'UT time': 'Time',}, inplace=True)\n",
    "    aqi['Time'] = aqi['Datetime'].str[-8:-3]\n",
    "    aqi['Date'] = aqi['Datetime'].str[0:10]\n",
    "    stations = [\"DL\"+str(x).zfill(3) for x in range(1,39)]\n",
    "    split_aqi = {}\n",
    "    for i in range(len(stations)):\n",
    "        split_aqi[stations[i]] = (aqi[aqi['StationId'] == stations[i]])\n",
    "    return met,aqi,split_aqi\n",
    "met,aqi,split_aqi = loadcsv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged Dataset Size 44035\n",
      "Size before roll 44035\n",
      "Size after roll 12790\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PM2.5</th>\n",
       "      <th>PM10</th>\n",
       "      <th>NO</th>\n",
       "      <th>NO2</th>\n",
       "      <th>NOx</th>\n",
       "      <th>NH3</th>\n",
       "      <th>CO</th>\n",
       "      <th>SO2</th>\n",
       "      <th>O3</th>\n",
       "      <th>AQI</th>\n",
       "      <th>...</th>\n",
       "      <th>isWeekend_t-19</th>\n",
       "      <th>isWeekend_t+19</th>\n",
       "      <th>isWeekend_t-20</th>\n",
       "      <th>isWeekend_t+20</th>\n",
       "      <th>isWeekend_t-21</th>\n",
       "      <th>isWeekend_t+21</th>\n",
       "      <th>isWeekend_t-22</th>\n",
       "      <th>isWeekend_t+22</th>\n",
       "      <th>isWeekend_t-23</th>\n",
       "      <th>isWeekend_t+23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>33.00</td>\n",
       "      <td>102.17</td>\n",
       "      <td>6.33</td>\n",
       "      <td>14.45</td>\n",
       "      <td>22.22</td>\n",
       "      <td>23.65</td>\n",
       "      <td>0.17</td>\n",
       "      <td>10.27</td>\n",
       "      <td>18.08</td>\n",
       "      <td>429.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>95.17</td>\n",
       "      <td>184.83</td>\n",
       "      <td>6.72</td>\n",
       "      <td>15.63</td>\n",
       "      <td>23.86</td>\n",
       "      <td>22.06</td>\n",
       "      <td>20.00</td>\n",
       "      <td>11.97</td>\n",
       "      <td>18.58</td>\n",
       "      <td>429.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>77.50</td>\n",
       "      <td>164.67</td>\n",
       "      <td>8.15</td>\n",
       "      <td>32.28</td>\n",
       "      <td>41.18</td>\n",
       "      <td>26.91</td>\n",
       "      <td>8.00</td>\n",
       "      <td>17.28</td>\n",
       "      <td>20.33</td>\n",
       "      <td>318.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>65.50</td>\n",
       "      <td>154.33</td>\n",
       "      <td>7.64</td>\n",
       "      <td>90.45</td>\n",
       "      <td>94.00</td>\n",
       "      <td>30.74</td>\n",
       "      <td>17.00</td>\n",
       "      <td>18.81</td>\n",
       "      <td>29.83</td>\n",
       "      <td>318.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>60.50</td>\n",
       "      <td>310.17</td>\n",
       "      <td>14.19</td>\n",
       "      <td>116.38</td>\n",
       "      <td>121.56</td>\n",
       "      <td>28.73</td>\n",
       "      <td>15.00</td>\n",
       "      <td>15.16</td>\n",
       "      <td>33.33</td>\n",
       "      <td>318.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43839</th>\n",
       "      <td>18.00</td>\n",
       "      <td>53.00</td>\n",
       "      <td>2.80</td>\n",
       "      <td>20.73</td>\n",
       "      <td>13.30</td>\n",
       "      <td>22.40</td>\n",
       "      <td>0.83</td>\n",
       "      <td>15.35</td>\n",
       "      <td>13.27</td>\n",
       "      <td>79.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43840</th>\n",
       "      <td>14.00</td>\n",
       "      <td>48.00</td>\n",
       "      <td>2.55</td>\n",
       "      <td>41.08</td>\n",
       "      <td>23.93</td>\n",
       "      <td>16.75</td>\n",
       "      <td>0.72</td>\n",
       "      <td>12.18</td>\n",
       "      <td>15.43</td>\n",
       "      <td>77.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43841</th>\n",
       "      <td>16.00</td>\n",
       "      <td>46.00</td>\n",
       "      <td>2.85</td>\n",
       "      <td>18.00</td>\n",
       "      <td>11.90</td>\n",
       "      <td>25.40</td>\n",
       "      <td>0.62</td>\n",
       "      <td>12.35</td>\n",
       "      <td>17.43</td>\n",
       "      <td>72.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43842</th>\n",
       "      <td>9.00</td>\n",
       "      <td>45.00</td>\n",
       "      <td>1.10</td>\n",
       "      <td>17.65</td>\n",
       "      <td>10.30</td>\n",
       "      <td>36.00</td>\n",
       "      <td>0.70</td>\n",
       "      <td>10.35</td>\n",
       "      <td>13.00</td>\n",
       "      <td>69.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43843</th>\n",
       "      <td>13.00</td>\n",
       "      <td>48.00</td>\n",
       "      <td>1.73</td>\n",
       "      <td>17.75</td>\n",
       "      <td>10.85</td>\n",
       "      <td>39.80</td>\n",
       "      <td>0.60</td>\n",
       "      <td>9.70</td>\n",
       "      <td>18.52</td>\n",
       "      <td>68.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12790 rows × 977 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       PM2.5    PM10     NO     NO2     NOx    NH3     CO    SO2     O3  \\\n",
       "165    33.00  102.17   6.33   14.45   22.22  23.65   0.17  10.27  18.08   \n",
       "166    95.17  184.83   6.72   15.63   23.86  22.06  20.00  11.97  18.58   \n",
       "167    77.50  164.67   8.15   32.28   41.18  26.91   8.00  17.28  20.33   \n",
       "168    65.50  154.33   7.64   90.45   94.00  30.74  17.00  18.81  29.83   \n",
       "170    60.50  310.17  14.19  116.38  121.56  28.73  15.00  15.16  33.33   \n",
       "...      ...     ...    ...     ...     ...    ...    ...    ...    ...   \n",
       "43839  18.00   53.00   2.80   20.73   13.30  22.40   0.83  15.35  13.27   \n",
       "43840  14.00   48.00   2.55   41.08   23.93  16.75   0.72  12.18  15.43   \n",
       "43841  16.00   46.00   2.85   18.00   11.90  25.40   0.62  12.35  17.43   \n",
       "43842   9.00   45.00   1.10   17.65   10.30  36.00   0.70  10.35  13.00   \n",
       "43843  13.00   48.00   1.73   17.75   10.85  39.80   0.60   9.70  18.52   \n",
       "\n",
       "         AQI  ...  isWeekend_t-19  isWeekend_t+19  isWeekend_t-20  \\\n",
       "165    429.0  ...             0.0             1.0             0.0   \n",
       "166    429.0  ...             0.0             1.0             0.0   \n",
       "167    318.0  ...             0.0             1.0             0.0   \n",
       "168    318.0  ...             0.0             1.0             0.0   \n",
       "170    318.0  ...             1.0             1.0             0.0   \n",
       "...      ...  ...             ...             ...             ...   \n",
       "43839   79.0  ...             1.0             0.0             1.0   \n",
       "43840   77.0  ...             1.0             0.0             1.0   \n",
       "43841   72.0  ...             1.0             0.0             1.0   \n",
       "43842   69.0  ...             1.0             0.0             1.0   \n",
       "43843   68.0  ...             1.0             0.0             1.0   \n",
       "\n",
       "       isWeekend_t+20  isWeekend_t-21  isWeekend_t+21  isWeekend_t-22  \\\n",
       "165               1.0             0.0             1.0             0.0   \n",
       "166               1.0             0.0             1.0             0.0   \n",
       "167               1.0             0.0             1.0             0.0   \n",
       "168               1.0             0.0             1.0             0.0   \n",
       "170               0.0             0.0             0.0             0.0   \n",
       "...               ...             ...             ...             ...   \n",
       "43839             0.0             1.0             0.0             1.0   \n",
       "43840             0.0             1.0             0.0             1.0   \n",
       "43841             0.0             1.0             0.0             1.0   \n",
       "43842             0.0             1.0             0.0             1.0   \n",
       "43843             0.0             1.0             0.0             1.0   \n",
       "\n",
       "       isWeekend_t+22 isWeekend_t-23  isWeekend_t+23  \n",
       "165               1.0            0.0             1.0  \n",
       "166               1.0            0.0             1.0  \n",
       "167               1.0            0.0             0.0  \n",
       "168               0.0            0.0             0.0  \n",
       "170               0.0            0.0             0.0  \n",
       "...               ...            ...             ...  \n",
       "43839             0.0            1.0             0.0  \n",
       "43840             0.0            1.0             0.0  \n",
       "43841             0.0            1.0             0.0  \n",
       "43842             0.0            1.0             0.0  \n",
       "43843             0.0            1.0             0.0  \n",
       "\n",
       "[12790 rows x 977 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pre - processing and loading data\n",
    "class dataset:\n",
    "    def __init__(self,met,aqi,split_aqi):\n",
    "            self.metro_data = met\n",
    "            self.aqi_data = aqi\n",
    "            self.split_aqi = split_aqi\n",
    "    def mergedData(self,station,rlist=['PM2.5','PM10','NO','NO2','NOx','NH3','CO','SO2','O3'],roll=48,shift=168):\n",
    "        df_aqi = self.getdf(station)\n",
    "        df = pd.merge(df_aqi, self.metro_data, how='inner', on=['Date', 'Time'])\n",
    "        print(\"Merged Dataset Size\",len(df))\n",
    "        \n",
    "        #Pre Processing merged Data\n",
    "        df['Year'] = df['Date'].str[0:4]\n",
    "        df['Month'] = df['Date'].str[5:7].astype(np.float64)\n",
    "        df['Day'] = df['Date'].str[8:10].astype(np.float64)\n",
    "        df['Hour'] = df['Time'].str[0:2]\n",
    "        \n",
    "        # TRIG TRANSFORMATIONS\n",
    "        df['windX'] = np.cos(np.deg2rad(df['Wind direction'])) * df['Wind speed']\n",
    "        df['windY'] = np.sin(np.deg2rad(df['Wind direction'])) * df['Wind speed']\n",
    "        df['hourX'] = np.cos((df['Hour'].astype(np.float64)-1)*np.pi/24)\n",
    "        df['hourY'] = np.sin((df['Hour'].astype(np.float64)-1)*np.pi/24)\n",
    "        df['MonthX'] = np.cos((df['Month'].astype(np.float64)-1)*np.pi/12)\n",
    "        df['MonthY'] = np.sin((df['Month'].astype(np.float64)-1)*np.pi/12)\n",
    "        \n",
    "        import datetime\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "        df['isWeekend'] =  (df['Date'].dt.dayofweek>=5).astype(int)\n",
    "        \n",
    "        df.interpolate(method='linear', limit=5,inplace=True)\n",
    "        \n",
    "        # Drop Additional columns\n",
    "        df.drop('Benzene', axis=1, inplace=True)\n",
    "        df.drop('Toluene',axis=1, inplace=True)\n",
    "        df.drop('Xylene', axis=1,inplace=True)\n",
    "        df.drop('AQI_Bucket',axis=1,inplace=True)\n",
    "        df.drop('Datetime',axis=1,inplace=True)\n",
    "        df.drop('StationId',axis=1,inplace=True)\n",
    "        df.drop('Short-wave irradiation',axis=1,inplace=True)\n",
    "        df.drop('Date',axis=1,inplace=True)\n",
    "        df.drop('Time',axis=1,inplace=True)\n",
    "        \n",
    "        # Rolling and shifting \n",
    "        print(\"Size before roll\",len(df))\n",
    "        for i in rlist:\n",
    "            df[i+'_lag1'] = df[i].shift(24)\n",
    "            df[i+'_lag2'] = df[i].shift(48)\n",
    "        for i in rlist:\n",
    "            df[i+\"_pred1\"] = df[i].shift(-24)\n",
    "            df[i+\"_pred2\"] = df[i].shift(-48)\n",
    "        newlist = rlist + ['Temperature','Relative Humidity','windX','windY','Year','MonthX','MonthY','hourX','hourY','isWeekend']\n",
    "        for i in newlist:\n",
    "            for j in range(24):\n",
    "                df[i+\"_t-\"+str(j)] = df[i].shift(j)\n",
    "                df[i+\"_t+\"+str(j)] = df[i].shift(-j-shift)\n",
    "        df.dropna(inplace=True)\n",
    "        print(\"Size after roll\",len(df))\n",
    "        \n",
    "        return df.copy()\n",
    "    def getdf(self,station):\n",
    "        return self.split_aqi[station]\n",
    "    def plot(self,station):\n",
    "        df = self.getdf(station)\n",
    "    def stats(self):\n",
    "        pass\n",
    "dat = dataset(met,aqi,split_aqi)\n",
    "df = dat.mergedData('DL031')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "(8569, 24, 12) (4221, 24, 12)\n"
     ]
    }
   ],
   "source": [
    "# CNN Model Testing as well\n",
    "TIME_SERIES_LENGTH = 24\n",
    "features = []\n",
    "rlist=['PM2.5','PM10','NO','NO2','CO']\n",
    "newlist = rlist + ['Temperature','Relative Humidity','windX','windY','Year','MonthX','MonthY']\n",
    "print(len(newlist))\n",
    "for j in range(24):\n",
    "    for i in newlist:\n",
    "        features.append(i+'_t-'+str(j))\n",
    "predVector = []\n",
    "for j in range(24):\n",
    "    predVector.append('PM2.5_t+'+str(j))\n",
    "X = df[features]\n",
    "y = df[predVector]\n",
    "# print(X)\n",
    "X.shape\n",
    "X = np.array(X).reshape(X.shape[0],TIME_SERIES_LENGTH,len(newlist))\n",
    "scaler = StandardScaler()\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "Xtrain = scaler.fit_transform(Xtrain.reshape(Xtrain.shape[0],TIME_SERIES_LENGTH*len(newlist)))\n",
    "Xtrain = Xtrain.reshape(Xtrain.shape[0],TIME_SERIES_LENGTH,len(newlist))\n",
    "Xtest = scaler.transform(Xtest.reshape(Xtest.shape[0],TIME_SERIES_LENGTH*len(newlist)))\n",
    "Xtest = Xtest.reshape(Xtest.shape[0],TIME_SERIES_LENGTH,len(newlist))\n",
    "print(Xtrain.shape,Xtest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_31 (Conv1D)           (None, 22, 64)            2368      \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 22, 64)            256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 11, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_16 (Flatten)         (None, 704)               0         \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 200)               141000    \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 200)               800       \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "batch_normalization_27 (Batc (None, 200)               800       \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 24)                4824      \n",
      "=================================================================\n",
      "Total params: 190,248\n",
      "Trainable params: 189,320\n",
      "Non-trainable params: 928\n",
      "_________________________________________________________________\n",
      "Train on 6855 samples, validate on 1714 samples\n",
      "Epoch 1/200\n",
      "6855/6855 [==============================] - 2s 246us/step - loss: 15793.8088 - mse: 15793.8076 - mae: 100.6667 - val_loss: 16677.1578 - val_mse: 16677.1602 - val_mae: 109.6470\n",
      "Epoch 2/200\n",
      "6855/6855 [==============================] - 1s 165us/step - loss: 11974.2779 - mse: 11974.2773 - mae: 90.7629 - val_loss: 11444.5953 - val_mse: 11444.5977 - val_mae: 90.4432\n",
      "Epoch 3/200\n",
      "6855/6855 [==============================] - 1s 162us/step - loss: 8477.8367 - mse: 8477.8379 - mae: 74.4255 - val_loss: 7489.1205 - val_mse: 7489.1201 - val_mae: 67.2873\n",
      "Epoch 4/200\n",
      "6855/6855 [==============================] - 1s 164us/step - loss: 5412.3167 - mse: 5412.3159 - mae: 54.2539 - val_loss: 4111.9416 - val_mse: 4111.9419 - val_mae: 45.4850\n",
      "Epoch 5/200\n",
      "6855/6855 [==============================] - 1s 193us/step - loss: 3571.3070 - mse: 3571.3071 - mae: 40.7828 - val_loss: 2983.4365 - val_mse: 2983.4365 - val_mae: 36.7334\n",
      "Epoch 6/200\n",
      "6855/6855 [==============================] - 1s 201us/step - loss: 2911.5504 - mse: 2911.5503 - mae: 36.4391 - val_loss: 2778.6043 - val_mse: 2778.6047 - val_mae: 35.5540\n",
      "Epoch 7/200\n",
      "6855/6855 [==============================] - 1s 188us/step - loss: 2692.9125 - mse: 2692.9128 - mae: 35.2486 - val_loss: 2664.5173 - val_mse: 2664.5171 - val_mae: 35.2247\n",
      "Epoch 8/200\n",
      "6855/6855 [==============================] - 1s 181us/step - loss: 2618.5493 - mse: 2618.5500 - mae: 35.0646 - val_loss: 2660.4202 - val_mse: 2660.4204 - val_mae: 34.6659\n",
      "Epoch 9/200\n",
      "6855/6855 [==============================] - 1s 175us/step - loss: 2454.9903 - mse: 2454.9905 - mae: 33.9429 - val_loss: 2434.3514 - val_mse: 2434.3513 - val_mae: 33.9179\n",
      "Epoch 10/200\n",
      "6855/6855 [==============================] - 1s 203us/step - loss: 2348.8027 - mse: 2348.8030 - mae: 33.2517 - val_loss: 2323.1668 - val_mse: 2323.1670 - val_mae: 32.8842\n",
      "Epoch 11/200\n",
      "6855/6855 [==============================] - 1s 203us/step - loss: 2251.0305 - mse: 2251.0305 - mae: 32.6891 - val_loss: 2322.6790 - val_mse: 2322.6792 - val_mae: 33.1005\n",
      "Epoch 12/200\n",
      "6855/6855 [==============================] - 1s 204us/step - loss: 2175.8021 - mse: 2175.8022 - mae: 32.1779 - val_loss: 2172.9659 - val_mse: 2172.9661 - val_mae: 31.8866\n",
      "Epoch 13/200\n",
      "6855/6855 [==============================] - 1s 202us/step - loss: 2120.5516 - mse: 2120.5510 - mae: 31.9280 - val_loss: 2144.8182 - val_mse: 2144.8184 - val_mae: 31.8268\n",
      "Epoch 14/200\n",
      "6855/6855 [==============================] - 1s 200us/step - loss: 2036.4195 - mse: 2036.4194 - mae: 31.2539 - val_loss: 2107.5099 - val_mse: 2107.5098 - val_mae: 31.3597\n",
      "Epoch 15/200\n",
      "6855/6855 [==============================] - 1s 201us/step - loss: 1935.8752 - mse: 1935.8754 - mae: 30.5538 - val_loss: 2076.6832 - val_mse: 2076.6831 - val_mae: 31.3639\n",
      "Epoch 16/200\n",
      "6855/6855 [==============================] - 1s 199us/step - loss: 1907.5885 - mse: 1907.5886 - mae: 30.4226 - val_loss: 1957.1476 - val_mse: 1957.1476 - val_mae: 30.4066\n",
      "Epoch 17/200\n",
      "6855/6855 [==============================] - 1s 198us/step - loss: 1830.4696 - mse: 1830.4695 - mae: 29.8638 - val_loss: 1997.7885 - val_mse: 1997.7885 - val_mae: 30.6218\n",
      "Epoch 18/200\n",
      "6855/6855 [==============================] - 1s 201us/step - loss: 1777.1334 - mse: 1777.1332 - mae: 29.3205 - val_loss: 1898.0594 - val_mse: 1898.0593 - val_mae: 29.8541\n",
      "Epoch 19/200\n",
      "6855/6855 [==============================] - 1s 202us/step - loss: 1673.2516 - mse: 1673.2509 - mae: 28.6187 - val_loss: 1773.4516 - val_mse: 1773.4515 - val_mae: 29.4976\n",
      "Epoch 20/200\n",
      "6855/6855 [==============================] - 1s 207us/step - loss: 1651.2571 - mse: 1651.2568 - mae: 28.4978 - val_loss: 1746.3082 - val_mse: 1746.3082 - val_mae: 29.1008\n",
      "Epoch 21/200\n",
      "6855/6855 [==============================] - 1s 201us/step - loss: 1669.9901 - mse: 1669.9907 - mae: 28.7465 - val_loss: 1729.8875 - val_mse: 1729.8876 - val_mae: 28.8642\n",
      "Epoch 22/200\n",
      "6855/6855 [==============================] - 1s 199us/step - loss: 1597.7744 - mse: 1597.7749 - mae: 28.1904 - val_loss: 1800.6859 - val_mse: 1800.6858 - val_mae: 29.2025\n",
      "Epoch 23/200\n",
      "6855/6855 [==============================] - 1s 204us/step - loss: 1631.1047 - mse: 1631.1046 - mae: 28.4977 - val_loss: 1673.2877 - val_mse: 1673.2877 - val_mae: 28.4891\n",
      "Epoch 24/200\n",
      "6855/6855 [==============================] - 1s 203us/step - loss: 1653.1251 - mse: 1653.1251 - mae: 28.7999 - val_loss: 1681.0140 - val_mse: 1681.0142 - val_mae: 28.4175\n",
      "Epoch 25/200\n",
      "6855/6855 [==============================] - 1s 203us/step - loss: 1500.8739 - mse: 1500.8739 - mae: 27.3610 - val_loss: 1685.8617 - val_mse: 1685.8617 - val_mae: 28.5370\n",
      "Epoch 26/200\n",
      "6855/6855 [==============================] - 1s 210us/step - loss: 1575.3772 - mse: 1575.3774 - mae: 28.0587 - val_loss: 1680.9265 - val_mse: 1680.9265 - val_mae: 28.4225\n",
      "Epoch 27/200\n",
      "6855/6855 [==============================] - 1s 201us/step - loss: 1592.9343 - mse: 1592.9347 - mae: 28.3499 - val_loss: 1635.5578 - val_mse: 1635.5577 - val_mae: 28.2428\n",
      "Epoch 28/200\n",
      "6855/6855 [==============================] - 1s 204us/step - loss: 1488.9539 - mse: 1488.9536 - mae: 27.3247 - val_loss: 1637.6390 - val_mse: 1637.6389 - val_mae: 28.0358\n",
      "Epoch 29/200\n",
      "6855/6855 [==============================] - 1s 203us/step - loss: 1352.6928 - mse: 1352.6926 - mae: 26.0271 - val_loss: 1554.0992 - val_mse: 1554.0990 - val_mae: 27.8295\n",
      "Epoch 30/200\n",
      "6855/6855 [==============================] - 1s 204us/step - loss: 1417.3832 - mse: 1417.3829 - mae: 26.7414 - val_loss: 1566.2324 - val_mse: 1566.2323 - val_mae: 27.4174\n",
      "Epoch 31/200\n",
      "6855/6855 [==============================] - 1s 200us/step - loss: 1403.0731 - mse: 1403.0730 - mae: 26.5660 - val_loss: 1584.4229 - val_mse: 1584.4231 - val_mae: 27.6412\n",
      "Epoch 32/200\n",
      "6855/6855 [==============================] - 1s 202us/step - loss: 1424.3617 - mse: 1424.3613 - mae: 26.8460 - val_loss: 1571.9649 - val_mse: 1571.9648 - val_mae: 27.7216\n",
      "Epoch 33/200\n",
      "6855/6855 [==============================] - 1s 210us/step - loss: 1329.2532 - mse: 1329.2534 - mae: 25.7944 - val_loss: 1544.7563 - val_mse: 1544.7562 - val_mae: 27.2348\n",
      "Epoch 34/200\n",
      "6855/6855 [==============================] - 1s 202us/step - loss: 1409.9321 - mse: 1409.9327 - mae: 26.7733 - val_loss: 1562.8907 - val_mse: 1562.8909 - val_mae: 27.5791\n",
      "Epoch 35/200\n",
      "6855/6855 [==============================] - 1s 204us/step - loss: 1333.1414 - mse: 1333.1415 - mae: 25.9106 - val_loss: 1508.4963 - val_mse: 1508.4965 - val_mae: 26.8724\n",
      "Epoch 36/200\n",
      "6855/6855 [==============================] - 1s 201us/step - loss: 1277.8472 - mse: 1277.8472 - mae: 25.5048 - val_loss: 1538.4747 - val_mse: 1538.4749 - val_mae: 27.4482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/200\n",
      "6855/6855 [==============================] - 1s 193us/step - loss: 1267.3486 - mse: 1267.3488 - mae: 25.3643 - val_loss: 1454.3611 - val_mse: 1454.3615 - val_mae: 26.7104\n",
      "Epoch 38/200\n",
      "6855/6855 [==============================] - 1s 193us/step - loss: 1266.6346 - mse: 1266.6351 - mae: 25.3151 - val_loss: 1438.3384 - val_mse: 1438.3384 - val_mae: 26.2752\n",
      "Epoch 39/200\n",
      "6855/6855 [==============================] - 1s 194us/step - loss: 1228.6741 - mse: 1228.6740 - mae: 24.9759 - val_loss: 1407.7496 - val_mse: 1407.7494 - val_mae: 26.0765\n",
      "Epoch 40/200\n",
      "6855/6855 [==============================] - 1s 193us/step - loss: 1250.6974 - mse: 1250.6976 - mae: 25.2180 - val_loss: 1418.7466 - val_mse: 1418.7467 - val_mae: 26.1956\n",
      "Epoch 41/200\n",
      "6855/6855 [==============================] - 1s 193us/step - loss: 1192.7698 - mse: 1192.7698 - mae: 24.6660 - val_loss: 1417.5425 - val_mse: 1417.5426 - val_mae: 26.1900\n",
      "Epoch 42/200\n",
      "6855/6855 [==============================] - 1s 193us/step - loss: 1161.5732 - mse: 1161.5731 - mae: 24.3594 - val_loss: 1407.4953 - val_mse: 1407.4954 - val_mae: 26.1828\n",
      "Epoch 43/200\n",
      "6855/6855 [==============================] - 1s 192us/step - loss: 1159.5860 - mse: 1159.5858 - mae: 24.2606 - val_loss: 1450.4027 - val_mse: 1450.4027 - val_mae: 26.5746\n",
      "Epoch 44/200\n",
      "6855/6855 [==============================] - 1s 194us/step - loss: 1139.3577 - mse: 1139.3578 - mae: 24.1862 - val_loss: 1450.0788 - val_mse: 1450.0786 - val_mae: 26.1095\n",
      "Epoch 45/200\n",
      "6855/6855 [==============================] - 1s 206us/step - loss: 1134.7790 - mse: 1134.7794 - mae: 24.1665 - val_loss: 1430.3832 - val_mse: 1430.3831 - val_mae: 26.3172\n",
      "Epoch 46/200\n",
      "6855/6855 [==============================] - 1s 199us/step - loss: 1168.5066 - mse: 1168.5066 - mae: 24.4747 - val_loss: 1381.3320 - val_mse: 1381.3318 - val_mae: 25.8060\n",
      "Epoch 47/200\n",
      "6855/6855 [==============================] - 1s 194us/step - loss: 1135.3040 - mse: 1135.3040 - mae: 24.1059 - val_loss: 1409.1324 - val_mse: 1409.1322 - val_mae: 26.4593\n",
      "Epoch 48/200\n",
      "6855/6855 [==============================] - 1s 196us/step - loss: 1102.0484 - mse: 1102.0481 - mae: 23.8082 - val_loss: 1333.1933 - val_mse: 1333.1936 - val_mae: 25.2021\n",
      "Epoch 49/200\n",
      "6855/6855 [==============================] - 1s 192us/step - loss: 1105.5470 - mse: 1105.5471 - mae: 23.8152 - val_loss: 1351.2818 - val_mse: 1351.2817 - val_mae: 25.2379\n",
      "Epoch 50/200\n",
      "6855/6855 [==============================] - 1s 195us/step - loss: 1100.2921 - mse: 1100.2920 - mae: 23.7091 - val_loss: 1359.5332 - val_mse: 1359.5330 - val_mae: 25.4322\n",
      "Epoch 51/200\n",
      "6855/6855 [==============================] - 1s 194us/step - loss: 1091.6501 - mse: 1091.6503 - mae: 23.6756 - val_loss: 1334.5810 - val_mse: 1334.5813 - val_mae: 25.4059\n",
      "Epoch 52/200\n",
      "6855/6855 [==============================] - 1s 194us/step - loss: 1041.7247 - mse: 1041.7250 - mae: 23.2047 - val_loss: 1345.2224 - val_mse: 1345.2224 - val_mae: 25.2976\n",
      "Epoch 53/200\n",
      "6855/6855 [==============================] - 1s 197us/step - loss: 1036.2664 - mse: 1036.2665 - mae: 22.9850 - val_loss: 1306.4525 - val_mse: 1306.4523 - val_mae: 25.0158\n",
      "Epoch 54/200\n",
      "6855/6855 [==============================] - 1s 194us/step - loss: 1056.8375 - mse: 1056.8378 - mae: 23.3375 - val_loss: 1332.2713 - val_mse: 1332.2712 - val_mae: 25.2897\n",
      "Epoch 55/200\n",
      "6855/6855 [==============================] - 1s 194us/step - loss: 1137.6476 - mse: 1137.6478 - mae: 24.3479 - val_loss: 1326.1961 - val_mse: 1326.1960 - val_mae: 25.1163\n",
      "Epoch 56/200\n",
      "6855/6855 [==============================] - 1s 202us/step - loss: 1071.3863 - mse: 1071.3862 - mae: 23.5611 - val_loss: 1321.8159 - val_mse: 1321.8160 - val_mae: 25.1879\n",
      "Epoch 57/200\n",
      "6855/6855 [==============================] - 1s 200us/step - loss: 1141.4948 - mse: 1141.4950 - mae: 24.1735 - val_loss: 1359.0226 - val_mse: 1359.0226 - val_mae: 25.5965\n",
      "Epoch 58/200\n",
      "6855/6855 [==============================] - 1s 204us/step - loss: 1143.3430 - mse: 1143.3428 - mae: 24.2761 - val_loss: 1336.2622 - val_mse: 1336.2622 - val_mae: 25.1135\n",
      "Epoch 59/200\n",
      "6855/6855 [==============================] - 2s 275us/step - loss: 1215.8260 - mse: 1215.8254 - mae: 24.9738 - val_loss: 1368.3619 - val_mse: 1368.3618 - val_mae: 25.8045se: 1247 - ETA: 0s - loss: 1222.2453 - mse: 1222.2448 - mae: 2\n",
      "Epoch 60/200\n",
      "6855/6855 [==============================] - 2s 225us/step - loss: 1083.2661 - mse: 1083.2657 - mae: 23.5998 - val_loss: 1336.4274 - val_mse: 1336.4272 - val_mae: 25.2485\n",
      "Epoch 61/200\n",
      "6855/6855 [==============================] - 2s 263us/step - loss: 1102.0067 - mse: 1102.0068 - mae: 23.8673 - val_loss: 1376.1286 - val_mse: 1376.1287 - val_mae: 25.3953\n",
      "Epoch 62/200\n",
      "6855/6855 [==============================] - 2s 244us/step - loss: 1032.1708 - mse: 1032.1710 - mae: 23.0350 - val_loss: 1289.7633 - val_mse: 1289.7633 - val_mae: 24.8681\n",
      "Epoch 63/200\n",
      "6855/6855 [==============================] - 2s 259us/step - loss: 1003.3834 - mse: 1003.3832 - mae: 22.7443 - val_loss: 1312.3932 - val_mse: 1312.3932 - val_mae: 25.1422\n",
      "Epoch 64/200\n",
      "6855/6855 [==============================] - 2s 231us/step - loss: 1071.8253 - mse: 1071.8256 - mae: 23.4678 - val_loss: 1347.0042 - val_mse: 1347.0043 - val_mae: 25.2646\n",
      "Epoch 65/200\n",
      "6855/6855 [==============================] - 2s 241us/step - loss: 1006.8786 - mse: 1006.8786 - mae: 22.7785 - val_loss: 1322.2079 - val_mse: 1322.2079 - val_mae: 24.9519\n",
      "Epoch 66/200\n",
      "6855/6855 [==============================] - 2s 247us/step - loss: 975.3442 - mse: 975.3445 - mae: 22.4086 - val_loss: 1302.5044 - val_mse: 1302.5045 - val_mae: 24.6247\n",
      "Epoch 67/200\n",
      "6855/6855 [==============================] - 1s 207us/step - loss: 976.9227 - mse: 976.9225 - mae: 22.4339 - val_loss: 1269.8175 - val_mse: 1269.8173 - val_mae: 24.4733\n",
      "Epoch 68/200\n",
      "6855/6855 [==============================] - 1s 207us/step - loss: 973.6736 - mse: 973.6735 - mae: 22.3355 - val_loss: 1242.7699 - val_mse: 1242.7697 - val_mae: 24.2235\n",
      "Epoch 69/200\n",
      "6855/6855 [==============================] - 1s 210us/step - loss: 976.6473 - mse: 976.6470 - mae: 22.5158 - val_loss: 1256.4091 - val_mse: 1256.4092 - val_mae: 24.3225\n",
      "Epoch 70/200\n",
      "6855/6855 [==============================] - 1s 206us/step - loss: 958.7704 - mse: 958.7703 - mae: 22.1933 - val_loss: 1243.4372 - val_mse: 1243.4373 - val_mae: 24.1843\n",
      "Epoch 71/200\n",
      "6855/6855 [==============================] - 1s 208us/step - loss: 1002.2192 - mse: 1002.2194 - mae: 22.8082 - val_loss: 1286.0962 - val_mse: 1286.0963 - val_mae: 24.8209\n",
      "Epoch 72/200\n",
      "6855/6855 [==============================] - 1s 212us/step - loss: 957.7686 - mse: 957.7684 - mae: 22.2031 - val_loss: 1258.6079 - val_mse: 1258.6079 - val_mae: 24.2461\n",
      "Epoch 73/200\n",
      "6855/6855 [==============================] - 1s 207us/step - loss: 920.7372 - mse: 920.7371 - mae: 21.8849 - val_loss: 1226.5143 - val_mse: 1226.5143 - val_mae: 23.9000\n",
      "Epoch 74/200\n",
      "6855/6855 [==============================] - 1s 208us/step - loss: 910.6436 - mse: 910.6435 - mae: 21.6942 - val_loss: 1217.9231 - val_mse: 1217.9230 - val_mae: 23.8652\n",
      "Epoch 75/200\n",
      "6855/6855 [==============================] - 1s 211us/step - loss: 913.4031 - mse: 913.4031 - mae: 21.7349 - val_loss: 1234.6155 - val_mse: 1234.6157 - val_mae: 24.2375\n",
      "Epoch 76/200\n",
      "6855/6855 [==============================] - 1s 209us/step - loss: 921.4546 - mse: 921.4547 - mae: 21.7289 - val_loss: 1226.6543 - val_mse: 1226.6543 - val_mae: 23.9474\n",
      "Epoch 77/200\n",
      "6855/6855 [==============================] - 1s 208us/step - loss: 924.7081 - mse: 924.7082 - mae: 21.9120 - val_loss: 1280.8215 - val_mse: 1280.8214 - val_mae: 24.5643\n",
      "Epoch 78/200\n",
      "6855/6855 [==============================] - 1s 209us/step - loss: 978.0440 - mse: 978.0439 - mae: 22.4812 - val_loss: 1280.4825 - val_mse: 1280.4827 - val_mae: 24.2183\n",
      "Epoch 79/200\n",
      "6855/6855 [==============================] - 1s 210us/step - loss: 907.5964 - mse: 907.5963 - mae: 21.6460 - val_loss: 1234.8518 - val_mse: 1234.8518 - val_mae: 24.0208\n",
      "Epoch 80/200\n",
      "6855/6855 [==============================] - 1s 208us/step - loss: 911.7347 - mse: 911.7348 - mae: 21.7130 - val_loss: 1225.0532 - val_mse: 1225.0532 - val_mae: 23.8506\n",
      "Epoch 81/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6855/6855 [==============================] - 1s 198us/step - loss: 884.1117 - mse: 884.1115 - mae: 21.4466 - val_loss: 1220.6918 - val_mse: 1220.6918 - val_mae: 24.0569\n",
      "Epoch 82/200\n",
      "6855/6855 [==============================] - 1s 194us/step - loss: 874.5493 - mse: 874.5494 - mae: 21.3387 - val_loss: 1204.9954 - val_mse: 1204.9954 - val_mae: 23.7087\n",
      "Epoch 83/200\n",
      "6855/6855 [==============================] - 1s 192us/step - loss: 914.8630 - mse: 914.8631 - mae: 21.9171 - val_loss: 1246.4248 - val_mse: 1246.4247 - val_mae: 24.4516\n",
      "Epoch 84/200\n",
      "6855/6855 [==============================] - 1s 195us/step - loss: 899.5013 - mse: 899.5012 - mae: 21.6452 - val_loss: 1214.0739 - val_mse: 1214.0739 - val_mae: 23.7886\n",
      "Epoch 85/200\n",
      "6855/6855 [==============================] - 1s 195us/step - loss: 892.0205 - mse: 892.0203 - mae: 21.5406 - val_loss: 1227.5407 - val_mse: 1227.5409 - val_mae: 24.3450\n",
      "Epoch 86/200\n",
      "6855/6855 [==============================] - 1s 194us/step - loss: 903.2123 - mse: 903.2122 - mae: 21.6893 - val_loss: 1242.1935 - val_mse: 1242.1936 - val_mae: 23.9886\n",
      "Epoch 87/200\n",
      "6855/6855 [==============================] - 1s 196us/step - loss: 1008.7381 - mse: 1008.7382 - mae: 22.7895 - val_loss: 1244.8060 - val_mse: 1244.8060 - val_mae: 24.3840\n",
      "Epoch 88/200\n",
      "6855/6855 [==============================] - 1s 194us/step - loss: 915.2209 - mse: 915.2210 - mae: 21.8524 - val_loss: 1228.3594 - val_mse: 1228.3595 - val_mae: 23.9513\n",
      "Epoch 89/200\n",
      "6855/6855 [==============================] - 1s 194us/step - loss: 885.8167 - mse: 885.8165 - mae: 21.5764 - val_loss: 1207.5177 - val_mse: 1207.5178 - val_mae: 23.9190\n",
      "Epoch 90/200\n",
      "6855/6855 [==============================] - 1s 203us/step - loss: 886.9243 - mse: 886.9242 - mae: 21.4476 - val_loss: 1209.2553 - val_mse: 1209.2552 - val_mae: 23.8163\n",
      "Epoch 91/200\n",
      "6855/6855 [==============================] - 1s 196us/step - loss: 863.3038 - mse: 863.3038 - mae: 21.1050 - val_loss: 1199.4910 - val_mse: 1199.4911 - val_mae: 23.7378\n",
      "Epoch 92/200\n",
      "6855/6855 [==============================] - 1s 193us/step - loss: 869.9061 - mse: 869.9062 - mae: 21.3215 - val_loss: 1206.4465 - val_mse: 1206.4464 - val_mae: 23.9146\n",
      "Epoch 93/200\n",
      "6855/6855 [==============================] - 1s 197us/step - loss: 869.1531 - mse: 869.1531 - mae: 21.2538 - val_loss: 1225.4926 - val_mse: 1225.4927 - val_mae: 24.1432\n",
      "Epoch 94/200\n",
      "6855/6855 [==============================] - 1s 196us/step - loss: 973.5597 - mse: 973.5598 - mae: 22.3544 - val_loss: 1236.0595 - val_mse: 1236.0597 - val_mae: 23.9764\n",
      "Epoch 95/200\n",
      "6855/6855 [==============================] - 1s 196us/step - loss: 965.9031 - mse: 965.9031 - mae: 22.4400 - val_loss: 1239.7791 - val_mse: 1239.7792 - val_mae: 24.1916\n",
      "Epoch 96/200\n",
      "6855/6855 [==============================] - 1s 197us/step - loss: 866.2474 - mse: 866.2473 - mae: 21.2410 - val_loss: 1213.3554 - val_mse: 1213.3553 - val_mae: 23.8416\n",
      "Epoch 97/200\n",
      "6855/6855 [==============================] - 1s 194us/step - loss: 909.8202 - mse: 909.8203 - mae: 21.8472 - val_loss: 1220.4962 - val_mse: 1220.4961 - val_mae: 23.8502\n",
      "Epoch 98/200\n",
      "6855/6855 [==============================] - 1s 197us/step - loss: 890.6635 - mse: 890.6633 - mae: 21.5489 - val_loss: 1204.0648 - val_mse: 1204.0649 - val_mae: 23.6402\n",
      "Epoch 99/200\n",
      "6855/6855 [==============================] - 1s 198us/step - loss: 828.7962 - mse: 828.7961 - mae: 20.7243 - val_loss: 1204.5011 - val_mse: 1204.5012 - val_mae: 23.6043\n",
      "Epoch 100/200\n",
      "6855/6855 [==============================] - 1s 195us/step - loss: 875.4824 - mse: 875.4824 - mae: 21.4209 - val_loss: 1221.0962 - val_mse: 1221.0961 - val_mae: 23.7793\n",
      "Epoch 101/200\n",
      "6855/6855 [==============================] - 1s 196us/step - loss: 846.4717 - mse: 846.4718 - mae: 20.9701 - val_loss: 1246.6584 - val_mse: 1246.6584 - val_mae: 24.2181\n",
      "Epoch 102/200\n",
      "6855/6855 [==============================] - 1s 197us/step - loss: 864.7219 - mse: 864.7220 - mae: 21.2880 - val_loss: 1219.6845 - val_mse: 1219.6846 - val_mae: 23.9018\n",
      "Epoch 103/200\n",
      "6855/6855 [==============================] - 1s 196us/step - loss: 847.3708 - mse: 847.3710 - mae: 21.0452 - val_loss: 1194.5135 - val_mse: 1194.5134 - val_mae: 23.6621\n",
      "Epoch 104/200\n",
      "6855/6855 [==============================] - 1s 198us/step - loss: 829.8408 - mse: 829.8410 - mae: 20.8268 - val_loss: 1175.8854 - val_mse: 1175.8854 - val_mae: 23.3875\n",
      "Epoch 105/200\n",
      "6855/6855 [==============================] - 1s 199us/step - loss: 841.6792 - mse: 841.6793 - mae: 21.0236 - val_loss: 1188.8822 - val_mse: 1188.8821 - val_mae: 23.5612\n",
      "Epoch 106/200\n",
      "6855/6855 [==============================] - 1s 196us/step - loss: 874.0619 - mse: 874.0616 - mae: 21.4084 - val_loss: 1226.6288 - val_mse: 1226.6288 - val_mae: 24.0314\n",
      "Epoch 107/200\n",
      "6855/6855 [==============================] - 1s 198us/step - loss: 838.5815 - mse: 838.5813 - mae: 20.9672 - val_loss: 1174.8831 - val_mse: 1174.8831 - val_mae: 23.4870\n",
      "Epoch 108/200\n",
      "6855/6855 [==============================] - 1s 196us/step - loss: 841.2772 - mse: 841.2772 - mae: 20.9814 - val_loss: 1194.7257 - val_mse: 1194.7258 - val_mae: 23.6529\n",
      "Epoch 109/200\n",
      "6855/6855 [==============================] - 1s 196us/step - loss: 882.9473 - mse: 882.9472 - mae: 21.4499 - val_loss: 1188.0272 - val_mse: 1188.0272 - val_mae: 23.7886\n",
      "Epoch 110/200\n",
      "6855/6855 [==============================] - 1s 197us/step - loss: 820.2363 - mse: 820.2365 - mae: 20.6600 - val_loss: 1185.7522 - val_mse: 1185.7523 - val_mae: 23.5418\n",
      "Epoch 111/200\n",
      "6855/6855 [==============================] - 1s 197us/step - loss: 923.0067 - mse: 923.0064 - mae: 21.9760 - val_loss: 1217.1949 - val_mse: 1217.1949 - val_mae: 24.0043\n",
      "Epoch 112/200\n",
      "6855/6855 [==============================] - 1s 199us/step - loss: 829.3807 - mse: 829.3809 - mae: 20.8490 - val_loss: 1223.1079 - val_mse: 1223.1079 - val_mae: 23.7846\n",
      "Epoch 113/200\n",
      "6855/6855 [==============================] - 1s 197us/step - loss: 896.3273 - mse: 896.3273 - mae: 21.6427 - val_loss: 1208.4810 - val_mse: 1208.4811 - val_mae: 23.9383\n",
      "Epoch 114/200\n",
      "6855/6855 [==============================] - 1s 198us/step - loss: 813.6158 - mse: 813.6159 - mae: 20.5969 - val_loss: 1167.4564 - val_mse: 1167.4564 - val_mae: 23.3593\n",
      "Epoch 115/200\n",
      "6855/6855 [==============================] - 1s 205us/step - loss: 777.9988 - mse: 777.9989 - mae: 20.1224 - val_loss: 1198.8762 - val_mse: 1198.8763 - val_mae: 23.5133\n",
      "Epoch 116/200\n",
      "6855/6855 [==============================] - 1s 207us/step - loss: 807.2232 - mse: 807.2229 - mae: 20.5525 - val_loss: 1210.8251 - val_mse: 1210.8252 - val_mae: 24.0550\n",
      "Epoch 117/200\n",
      "6855/6855 [==============================] - 1s 205us/step - loss: 805.6180 - mse: 805.6181 - mae: 20.5259 - val_loss: 1150.0560 - val_mse: 1150.0560 - val_mae: 23.0851\n",
      "Epoch 118/200\n",
      "6855/6855 [==============================] - 1s 218us/step - loss: 784.5994 - mse: 784.5994 - mae: 20.3208 - val_loss: 1173.4542 - val_mse: 1173.4541 - val_mae: 23.3071\n",
      "Epoch 119/200\n",
      "6855/6855 [==============================] - 1s 208us/step - loss: 796.8961 - mse: 796.8963 - mae: 20.4254 - val_loss: 1173.5205 - val_mse: 1173.5204 - val_mae: 23.4369\n",
      "Epoch 120/200\n",
      "6855/6855 [==============================] - 1s 207us/step - loss: 868.9419 - mse: 868.9421 - mae: 21.2980 - val_loss: 1199.9683 - val_mse: 1199.9680 - val_mae: 23.6979\n",
      "Epoch 121/200\n",
      "6855/6855 [==============================] - 1s 205us/step - loss: 789.7013 - mse: 789.7013 - mae: 20.3093 - val_loss: 1169.6220 - val_mse: 1169.6222 - val_mae: 23.4733\n",
      "Epoch 122/200\n",
      "6855/6855 [==============================] - 1s 211us/step - loss: 855.4989 - mse: 855.4991 - mae: 21.2595 - val_loss: 1208.4448 - val_mse: 1208.4448 - val_mae: 23.7711\n",
      "Epoch 123/200\n",
      "6855/6855 [==============================] - 1s 208us/step - loss: 788.5966 - mse: 788.5966 - mae: 20.3789 - val_loss: 1151.3081 - val_mse: 1151.3082 - val_mae: 23.0568\n",
      "Epoch 124/200\n",
      "6855/6855 [==============================] - 1s 196us/step - loss: 776.4626 - mse: 776.4627 - mae: 20.1537 - val_loss: 1159.6649 - val_mse: 1159.6649 - val_mae: 23.4186\n",
      "Epoch 125/200\n",
      "6855/6855 [==============================] - 1s 201us/step - loss: 783.3068 - mse: 783.3069 - mae: 20.2328 - val_loss: 1148.0818 - val_mse: 1148.0819 - val_mae: 23.0929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 126/200\n",
      "6855/6855 [==============================] - 1s 200us/step - loss: 816.8151 - mse: 816.8154 - mae: 20.6458 - val_loss: 1176.5759 - val_mse: 1176.5758 - val_mae: 23.3297\n",
      "Epoch 127/200\n",
      "6855/6855 [==============================] - 1s 194us/step - loss: 802.4996 - mse: 802.4998 - mae: 20.5728 - val_loss: 1159.3546 - val_mse: 1159.3546 - val_mae: 23.0155\n",
      "Epoch 128/200\n",
      "6855/6855 [==============================] - 1s 193us/step - loss: 811.7656 - mse: 811.7656 - mae: 20.5783 - val_loss: 1164.7479 - val_mse: 1164.7479 - val_mae: 23.1226\n",
      "Epoch 129/200\n",
      "6855/6855 [==============================] - 1s 193us/step - loss: 773.3793 - mse: 773.3796 - mae: 20.1071 - val_loss: 1193.6002 - val_mse: 1193.6002 - val_mae: 23.8096\n",
      "Epoch 130/200\n",
      "6855/6855 [==============================] - 1s 195us/step - loss: 815.4832 - mse: 815.4834 - mae: 20.6809 - val_loss: 1173.0394 - val_mse: 1173.0394 - val_mae: 23.3069\n",
      "Epoch 131/200\n",
      "6855/6855 [==============================] - 1s 195us/step - loss: 802.9971 - mse: 802.9969 - mae: 20.5725 - val_loss: 1165.1759 - val_mse: 1165.1757 - val_mae: 23.2395\n",
      "Epoch 132/200\n",
      "6855/6855 [==============================] - 1s 195us/step - loss: 857.9190 - mse: 857.9191 - mae: 21.2189 - val_loss: 1175.1429 - val_mse: 1175.1429 - val_mae: 23.3123\n",
      "Epoch 133/200\n",
      "6855/6855 [==============================] - 1s 194us/step - loss: 791.6433 - mse: 791.6433 - mae: 20.4304 - val_loss: 1179.4785 - val_mse: 1179.4784 - val_mae: 23.3327\n",
      "Epoch 134/200\n",
      "6855/6855 [==============================] - 1s 198us/step - loss: 807.4491 - mse: 807.4492 - mae: 20.6006 - val_loss: 1161.6493 - val_mse: 1161.6494 - val_mae: 23.0964\n",
      "Epoch 135/200\n",
      "6855/6855 [==============================] - 1s 196us/step - loss: 768.1846 - mse: 768.1847 - mae: 20.0979 - val_loss: 1161.9835 - val_mse: 1161.9836 - val_mae: 23.3462\n",
      "Epoch 136/200\n",
      "6855/6855 [==============================] - 1s 194us/step - loss: 762.3105 - mse: 762.3103 - mae: 20.0653 - val_loss: 1170.6504 - val_mse: 1170.6505 - val_mae: 23.4335\n",
      "Epoch 137/200\n",
      "6855/6855 [==============================] - 1s 197us/step - loss: 827.6215 - mse: 827.6214 - mae: 20.8751 - val_loss: 1164.9010 - val_mse: 1164.9010 - val_mae: 23.0541\n",
      "Epoch 138/200\n",
      "6855/6855 [==============================] - 1s 195us/step - loss: 766.5942 - mse: 766.5942 - mae: 20.0278 - val_loss: 1138.2018 - val_mse: 1138.2018 - val_mae: 22.9614\n",
      "Epoch 139/200\n",
      "6855/6855 [==============================] - 1s 196us/step - loss: 771.9602 - mse: 771.9602 - mae: 20.2109 - val_loss: 1161.0131 - val_mse: 1161.0129 - val_mae: 22.9790\n",
      "Epoch 140/200\n",
      "6855/6855 [==============================] - 1s 196us/step - loss: 765.1562 - mse: 765.1561 - mae: 20.1311 - val_loss: 1154.9887 - val_mse: 1154.9886 - val_mae: 23.2162\n",
      "Epoch 141/200\n",
      "6855/6855 [==============================] - 1s 197us/step - loss: 776.0454 - mse: 776.0455 - mae: 20.2505 - val_loss: 1169.6821 - val_mse: 1169.6821 - val_mae: 23.5669\n",
      "Epoch 142/200\n",
      "6855/6855 [==============================] - 1s 195us/step - loss: 762.5950 - mse: 762.5950 - mae: 20.0836 - val_loss: 1145.8805 - val_mse: 1145.8805 - val_mae: 22.9841\n",
      "Epoch 143/200\n",
      "6855/6855 [==============================] - 1s 196us/step - loss: 763.8016 - mse: 763.8015 - mae: 20.0608 - val_loss: 1125.0140 - val_mse: 1125.0140 - val_mae: 22.8463\n",
      "Epoch 144/200\n",
      "6855/6855 [==============================] - 1s 195us/step - loss: 739.1734 - mse: 739.1732 - mae: 19.7204 - val_loss: 1161.7783 - val_mse: 1161.7786 - val_mae: 23.2304\n",
      "Epoch 145/200\n",
      "6855/6855 [==============================] - 1s 197us/step - loss: 768.0380 - mse: 768.0381 - mae: 20.1521 - val_loss: 1165.0374 - val_mse: 1165.0374 - val_mae: 22.9684\n",
      "Epoch 146/200\n",
      "6855/6855 [==============================] - 1s 195us/step - loss: 794.7365 - mse: 794.7365 - mae: 20.4686 - val_loss: 1168.8308 - val_mse: 1168.8307 - val_mae: 23.1750\n",
      "Epoch 147/200\n",
      "6855/6855 [==============================] - 1s 197us/step - loss: 762.4173 - mse: 762.4170 - mae: 20.1017 - val_loss: 1157.6489 - val_mse: 1157.6488 - val_mae: 23.0150\n",
      "Epoch 148/200\n",
      "6855/6855 [==============================] - 1s 197us/step - loss: 727.7871 - mse: 727.7872 - mae: 19.5103 - val_loss: 1148.3764 - val_mse: 1148.3762 - val_mae: 22.8849\n",
      "Epoch 149/200\n",
      "6855/6855 [==============================] - 1s 195us/step - loss: 727.3356 - mse: 727.3358 - mae: 19.6084 - val_loss: 1141.2230 - val_mse: 1141.2231 - val_mae: 22.9183\n",
      "Epoch 150/200\n",
      "6855/6855 [==============================] - 1s 197us/step - loss: 725.4998 - mse: 725.4999 - mae: 19.5553 - val_loss: 1130.2265 - val_mse: 1130.2266 - val_mae: 22.7201\n",
      "Epoch 151/200\n",
      "6855/6855 [==============================] - 1s 196us/step - loss: 728.7800 - mse: 728.7800 - mae: 19.6140 - val_loss: 1134.8755 - val_mse: 1134.8756 - val_mae: 22.7858\n",
      "Epoch 152/200\n",
      "6855/6855 [==============================] - 1s 196us/step - loss: 717.5122 - mse: 717.5122 - mae: 19.4532 - val_loss: 1115.3740 - val_mse: 1115.3740 - val_mae: 22.6204\n",
      "Epoch 153/200\n",
      "6855/6855 [==============================] - 1s 199us/step - loss: 712.0199 - mse: 712.0200 - mae: 19.4472 - val_loss: 1142.8344 - val_mse: 1142.8345 - val_mae: 22.8010\n",
      "Epoch 154/200\n",
      "6855/6855 [==============================] - 1s 196us/step - loss: 753.4250 - mse: 753.4250 - mae: 19.9443 - val_loss: 1126.2249 - val_mse: 1126.2249 - val_mae: 22.6112\n",
      "Epoch 155/200\n",
      "6855/6855 [==============================] - 1s 196us/step - loss: 749.2229 - mse: 749.2229 - mae: 19.8940 - val_loss: 1140.3149 - val_mse: 1140.3149 - val_mae: 22.9936\n",
      "Epoch 156/200\n",
      "6855/6855 [==============================] - 1s 199us/step - loss: 722.1438 - mse: 722.1440 - mae: 19.5368 - val_loss: 1160.4148 - val_mse: 1160.4148 - val_mae: 23.0631\n",
      "Epoch 157/200\n",
      "6855/6855 [==============================] - 1s 199us/step - loss: 717.0459 - mse: 717.0461 - mae: 19.4391 - val_loss: 1127.6266 - val_mse: 1127.6266 - val_mae: 22.7101\n",
      "Epoch 158/200\n",
      "6855/6855 [==============================] - 1s 197us/step - loss: 716.5237 - mse: 716.5239 - mae: 19.4404 - val_loss: 1169.1120 - val_mse: 1169.1118 - val_mae: 23.2980\n",
      "Epoch 159/200\n",
      "6855/6855 [==============================] - 1s 198us/step - loss: 716.3348 - mse: 716.3348 - mae: 19.4005 - val_loss: 1147.7444 - val_mse: 1147.7443 - val_mae: 23.1311\n",
      "Epoch 160/200\n",
      "6855/6855 [==============================] - 1s 207us/step - loss: 709.8374 - mse: 709.8370 - mae: 19.2570 - val_loss: 1145.6070 - val_mse: 1145.6071 - val_mae: 23.0598\n",
      "Epoch 161/200\n",
      "6855/6855 [==============================] - 1s 206us/step - loss: 753.7310 - mse: 753.7310 - mae: 20.0268 - val_loss: 1150.9428 - val_mse: 1150.9426 - val_mae: 23.2417\n",
      "Epoch 162/200\n",
      "6855/6855 [==============================] - 2s 266us/step - loss: 755.2383 - mse: 755.2383 - mae: 20.0146 - val_loss: 1153.2210 - val_mse: 1153.2209 - val_mae: 23.1638\n",
      "Epoch 163/200\n",
      "6855/6855 [==============================] - 2s 246us/step - loss: 747.8004 - mse: 747.8004 - mae: 19.7825 - val_loss: 1135.4305 - val_mse: 1135.4305 - val_mae: 22.8976\n",
      "Epoch 164/200\n",
      "6855/6855 [==============================] - 2s 266us/step - loss: 743.5488 - mse: 743.5487 - mae: 19.8280 - val_loss: 1166.1802 - val_mse: 1166.1802 - val_mae: 23.1440\n",
      "Epoch 165/200\n",
      "6855/6855 [==============================] - 1s 219us/step - loss: 739.7540 - mse: 739.7542 - mae: 19.7708 - val_loss: 1147.6417 - val_mse: 1147.6417 - val_mae: 22.8706\n",
      "Epoch 166/200\n",
      "6855/6855 [==============================] - 1s 218us/step - loss: 736.4793 - mse: 736.4794 - mae: 19.7175 - val_loss: 1178.1452 - val_mse: 1178.1453 - val_mae: 23.4906\n",
      "Epoch 167/200\n",
      "6855/6855 [==============================] - 2s 228us/step - loss: 709.3098 - mse: 709.3098 - mae: 19.2986 - val_loss: 1119.1334 - val_mse: 1119.1334 - val_mae: 22.5324\n",
      "Epoch 168/200\n",
      "6855/6855 [==============================] - 2s 219us/step - loss: 743.2700 - mse: 743.2701 - mae: 19.9157 - val_loss: 1132.7706 - val_mse: 1132.7705 - val_mae: 22.6938\n",
      "Epoch 169/200\n",
      "6855/6855 [==============================] - 2s 226us/step - loss: 700.7461 - mse: 700.7462 - mae: 19.2381 - val_loss: 1145.5729 - val_mse: 1145.5728 - val_mae: 22.6425\n",
      "Epoch 170/200\n",
      "6855/6855 [==============================] - 1s 218us/step - loss: 812.7232 - mse: 812.7233 - mae: 20.7707 - val_loss: 1183.1438 - val_mse: 1183.1437 - val_mae: 23.2980\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 171/200\n",
      "6855/6855 [==============================] - 2s 221us/step - loss: 764.3470 - mse: 764.3472 - mae: 20.1712 - val_loss: 1154.2839 - val_mse: 1154.2838 - val_mae: 23.0269\n",
      "Epoch 172/200\n",
      "6855/6855 [==============================] - 2s 220us/step - loss: 713.8749 - mse: 713.8748 - mae: 19.4094 - val_loss: 1159.4119 - val_mse: 1159.4119 - val_mae: 23.1740\n",
      "Epoch 173/200\n",
      "6855/6855 [==============================] - 2s 224us/step - loss: 726.9823 - mse: 726.9822 - mae: 19.6477 - val_loss: 1164.7222 - val_mse: 1164.7224 - val_mae: 23.3146\n",
      "Epoch 174/200\n",
      "6855/6855 [==============================] - 1s 216us/step - loss: 701.2259 - mse: 701.2259 - mae: 19.2116 - val_loss: 1137.1098 - val_mse: 1137.1097 - val_mae: 22.8425\n",
      "Epoch 175/200\n",
      "6855/6855 [==============================] - 1s 201us/step - loss: 866.8262 - mse: 866.8260 - mae: 21.5546 - val_loss: 1203.6032 - val_mse: 1203.6034 - val_mae: 23.5805\n",
      "Epoch 176/200\n",
      "6855/6855 [==============================] - 1s 203us/step - loss: 738.1285 - mse: 738.1289 - mae: 19.8079 - val_loss: 1161.4642 - val_mse: 1161.4641 - val_mae: 23.0821\n",
      "Epoch 177/200\n",
      "6855/6855 [==============================] - 1s 213us/step - loss: 736.8876 - mse: 736.8878 - mae: 19.7812 - val_loss: 1128.1145 - val_mse: 1128.1145 - val_mae: 22.7617\n",
      "Epoch 178/200\n",
      "6855/6855 [==============================] - 1s 203us/step - loss: 704.3599 - mse: 704.3598 - mae: 19.3498 - val_loss: 1125.9320 - val_mse: 1125.9321 - val_mae: 22.6837\n",
      "Epoch 179/200\n",
      "6855/6855 [==============================] - 1s 203us/step - loss: 727.9781 - mse: 727.9778 - mae: 19.6548 - val_loss: 1171.8477 - val_mse: 1171.8478 - val_mae: 23.1321\n",
      "Epoch 180/200\n",
      "6855/6855 [==============================] - 1s 204us/step - loss: 714.8164 - mse: 714.8163 - mae: 19.4279 - val_loss: 1124.6215 - val_mse: 1124.6215 - val_mae: 22.5645\n",
      "Epoch 181/200\n",
      "6855/6855 [==============================] - 1s 203us/step - loss: 718.5431 - mse: 718.5431 - mae: 19.5425 - val_loss: 1129.2389 - val_mse: 1129.2389 - val_mae: 22.7582\n",
      "Epoch 182/200\n",
      "6800/6855 [============================>.] - ETA: 0s - loss: 699.0744 - mse: 699.0745 - mae: 19.2737"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import BatchNormalization\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "with tf.device('/CPU:0'):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(64, 3, activation='relu',input_shape=(TIME_SERIES_LENGTH,len(newlist))))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(200, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(200, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(24, activation='linear'))\n",
    "    model.summary()\n",
    "    #Fit\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['mse','mae'])\n",
    "    history = model.fit(Xtrain, ytrain, epochs=200, batch_size=50,  verbose=1, validation_split=0.2)\n",
    "    #Print Accuracy\n",
    "    testPred = model.predict(Xtest)\n",
    "    trainPred = model.predict(Xtrain)\n",
    "    print(mean_squared_error(testPred, ytest,squared=False))\n",
    "    print(mean_squared_error(trainPred, ytrain,squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'val_mse', 'val_mae', 'loss', 'mse', 'mae'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for loss\n",
    "plt.plot(np.sqrt(history.history['loss']))\n",
    "plt.plot(np.sqrt(history.history['val_loss']))\n",
    "plt.title('model loss')\n",
    "plt.ylabel('RMSE loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5695458844542312\n",
      "54.50302447946837\n",
      "55.00987602265274\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif, f_regression , mutual_info_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "dfTrain = df[:]\n",
    "features = ['Temperature','Relative Humidity','windX','windY','Year','MonthX','MonthY','hourX','hourY','isWeekend']\n",
    "features.append(\"PM2.5_lag1\")\n",
    "features.append(\"PM2.5_lag2\")\n",
    "\n",
    "features.append(\"PM10_lag1\")\n",
    "features.append(\"PM10_lag2\")\n",
    "\n",
    "features.append(\"NO2_lag1\")\n",
    "features.append(\"NO2_lag2\")\n",
    "\n",
    "features.append(\"SO2_lag1\")\n",
    "features.append(\"SO2_lag2\")\n",
    "\n",
    "features.append(\"NO_lag1\")\n",
    "features.append(\"NO_lag2\")\n",
    "\n",
    "features.append(\"NOx_lag1\")\n",
    "features.append(\"NOx_lag2\")\n",
    "\n",
    "features.append(\"CO_lag1\")\n",
    "features.append(\"CO_lag2\")\n",
    "\n",
    "features.append(\"O3_lag1\")\n",
    "features.append(\"O3_lag2\")\n",
    "\n",
    "features.append(\"NH3_lag1\")\n",
    "features.append(\"NH3_lag2\")\n",
    "\n",
    "features = []\n",
    "rlist=['PM2.5','PM10','NO','NO2','CO']\n",
    "newlist = rlist + ['Temperature','Relative Humidity','windX','windY','Year','MonthX','MonthY','hourX','hourY','isWeekend']\n",
    "for i in newlist:\n",
    "    for j in range(24):\n",
    "        features.append(i+'_t-'+str(j))\n",
    "\n",
    "X = df[features]\n",
    "y = df['PM2.5_pred1']\n",
    "scaler = StandardScaler()\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "scaler.fit(Xtrain)\n",
    "\n",
    "reg = LinearRegression().fit(scaler.transform(Xtrain), ytrain)\n",
    "\n",
    "testPred = reg.predict(scaler.transform(Xtest))\n",
    "trainPred = reg.predict(scaler.transform(Xtrain))\n",
    "mse = np.mean((testPred - np.array(ytest))*(testPred - np.array(ytest)))\n",
    "print(reg.score(scaler.transform(Xtest), ytest))\n",
    "print(mean_squared_error(testPred, ytest,squared=False))\n",
    "print(mean_squared_error(trainPred, ytrain,squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\confusement\\miniconda3\\envs\\mlc\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.685427631493239\n",
      "46.59265479233718\n",
      "42.94414431488849\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif, f_regression , mutual_info_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "dfTrain = df[:]\n",
    "features = ['Temperature','Relative Humidity','windX','windY','Year','MonthX','MonthY','hourX','hourY','isWeekend']\n",
    "features.append(\"PM2.5_lag1\")\n",
    "features.append(\"PM2.5_lag2\")\n",
    "\n",
    "features.append(\"PM10_lag1\")\n",
    "features.append(\"PM10_lag2\")\n",
    "\n",
    "features.append(\"NO2_lag1\")\n",
    "features.append(\"NO2_lag2\")\n",
    "\n",
    "features.append(\"SO2_lag1\")\n",
    "features.append(\"SO2_lag2\")\n",
    "\n",
    "features.append(\"NO_lag1\")\n",
    "features.append(\"NO_lag2\")\n",
    "\n",
    "features.append(\"NOx_lag1\")\n",
    "features.append(\"NOx_lag2\")\n",
    "\n",
    "features.append(\"CO_lag1\")\n",
    "features.append(\"CO_lag2\")\n",
    "\n",
    "features.append(\"O3_lag1\")\n",
    "features.append(\"O3_lag2\")\n",
    "\n",
    "features.append(\"NH3_lag1\")\n",
    "features.append(\"NH3_lag2\")\n",
    "\n",
    "features = []\n",
    "rlist=['PM2.5','PM10','NO','NO2','CO']\n",
    "newlist = rlist + ['Temperature','Relative Humidity','windX','windY','Year','MonthX','MonthY','hourX','hourY','isWeekend']\n",
    "for i in newlist:\n",
    "    for j in range(24):\n",
    "        features.append(i+'_t-'+str(j))\n",
    "        \n",
    "X = df[features]\n",
    "y = df['PM2.5_pred1']\n",
    "scaler = StandardScaler()\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "scaler.fit(Xtrain)\n",
    "\n",
    "reg = MLPRegressor(random_state=1, max_iter=100).fit(scaler.transform(Xtrain), ytrain)\n",
    "\n",
    "testPred = reg.predict(scaler.transform(Xtest))\n",
    "trainPred = reg.predict(scaler.transform(Xtrain))\n",
    "mse = np.mean((testPred - np.array(ytest))*(testPred - np.array(ytest)))\n",
    "print(reg.score(scaler.transform(Xtest), ytest))\n",
    "print(mean_squared_error(testPred, ytest,squared=False))\n",
    "print(mean_squared_error(trainPred, ytrain,squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.44686265572761963\n",
      "61.78366409828595\n",
      "63.108555288839455\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif, f_regression , mutual_info_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "dfTrain = df[:]\n",
    "features = ['Temperature','Relative Humidity','windX','windY','Year','MonthX','MonthY','hourX','hourY','isWeekend']\n",
    "features.append(\"PM2.5_lag1\")\n",
    "features.append(\"PM2.5_lag2\")\n",
    "\n",
    "features.append(\"PM10_lag1\")\n",
    "features.append(\"PM10_lag2\")\n",
    "\n",
    "features.append(\"NO2_lag1\")\n",
    "features.append(\"NO2_lag2\")\n",
    "\n",
    "features.append(\"SO2_lag1\")\n",
    "features.append(\"SO2_lag2\")\n",
    "\n",
    "features.append(\"NO_lag1\")\n",
    "features.append(\"NO_lag2\")\n",
    "\n",
    "features.append(\"NOx_lag1\")\n",
    "features.append(\"NOx_lag2\")\n",
    "\n",
    "features.append(\"CO_lag1\")\n",
    "features.append(\"CO_lag2\")\n",
    "\n",
    "features.append(\"O3_lag1\")\n",
    "features.append(\"O3_lag2\")\n",
    "\n",
    "features.append(\"NH3_lag1\")\n",
    "features.append(\"NH3_lag2\")\n",
    "\n",
    "# features = []\n",
    "# rlist=['PM2.5','PM10','NO','NO2','CO']\n",
    "# newlist = rlist + ['Temperature','Relative Humidity','windX','windY','Year','MonthX','MonthY','hourX','hourY','isWeekend']\n",
    "# for i in newlist:\n",
    "#     for j in range(24):\n",
    "#         features.append(i+'_t-'+str(j))\n",
    "\n",
    "X = df[features]\n",
    "y = df['PM2.5_pred1']\n",
    "scaler = StandardScaler()\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "scaler.fit(Xtrain)\n",
    "\n",
    "reg = SVR(C=1.0, epsilon=0.2).fit(scaler.transform(Xtrain), ytrain)\n",
    "\n",
    "testPred = reg.predict(scaler.transform(Xtest))\n",
    "trainPred = reg.predict(scaler.transform(Xtrain))\n",
    "mse = np.mean((testPred - np.array(ytest))*(testPred - np.array(ytest)))\n",
    "print(reg.score(scaler.transform(Xtest), ytest))\n",
    "print(mean_squared_error(testPred, ytest,squared=False))\n",
    "print(mean_squared_error(trainPred, ytrain,squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8569, 360)\n",
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_86 (Dense)             (None, 200)               72200     \n",
      "_________________________________________________________________\n",
      "dense_87 (Dense)             (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dense_88 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_89 (Dense)             (None, 24)                2424      \n",
      "=================================================================\n",
      "Total params: 104,824\n",
      "Trainable params: 104,824\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 6855 samples, validate on 1714 samples\n",
      "Epoch 1/100\n",
      "6855/6855 [==============================] - 1s 115us/step - loss: 8309.0508 - mse: 8309.0508 - mae: 63.0670 - val_loss: 4379.9809 - val_mse: 4379.9805 - val_mae: 46.6007\n",
      "Epoch 2/100\n",
      "6855/6855 [==============================] - 1s 109us/step - loss: 4454.1584 - mse: 4454.1582 - mae: 46.6320 - val_loss: 4305.2399 - val_mse: 4305.2397 - val_mae: 47.5071\n",
      "Epoch 3/100\n",
      "6855/6855 [==============================] - 1s 115us/step - loss: 4318.2445 - mse: 4318.2451 - mae: 46.0333 - val_loss: 4119.4418 - val_mse: 4119.4419 - val_mae: 45.3639\n",
      "Epoch 4/100\n",
      "6855/6855 [==============================] - 1s 94us/step - loss: 4153.5816 - mse: 4153.5820 - mae: 45.0256 - val_loss: 3946.3698 - val_mse: 3946.3696 - val_mae: 44.3598\n",
      "Epoch 5/100\n",
      "6855/6855 [==============================] - 1s 105us/step - loss: 3930.0813 - mse: 3930.0813 - mae: 43.5269 - val_loss: 3800.8963 - val_mse: 3800.8965 - val_mae: 42.5525\n",
      "Epoch 6/100\n",
      "6855/6855 [==============================] - 1s 112us/step - loss: 3767.2328 - mse: 3767.2327 - mae: 42.4204 - val_loss: 3600.5627 - val_mse: 3600.5625 - val_mae: 41.8675\n",
      "Epoch 7/100\n",
      "6855/6855 [==============================] - 1s 117us/step - loss: 3626.2942 - mse: 3626.2947 - mae: 41.4814 - val_loss: 3490.6497 - val_mse: 3490.6497 - val_mae: 41.1371\n",
      "Epoch 8/100\n",
      "6855/6855 [==============================] - 1s 122us/step - loss: 3486.1876 - mse: 3486.1870 - mae: 40.5335 - val_loss: 3364.0821 - val_mse: 3364.0820 - val_mae: 40.0079\n",
      "Epoch 9/100\n",
      "6855/6855 [==============================] - 1s 143us/step - loss: 3340.8611 - mse: 3340.8601 - mae: 39.5399 - val_loss: 3233.0603 - val_mse: 3233.0613 - val_mae: 38.7013\n",
      "Epoch 10/100\n",
      "6855/6855 [==============================] - 1s 106us/step - loss: 3209.9103 - mse: 3209.9104 - mae: 38.5882 - val_loss: 3091.9288 - val_mse: 3091.9287 - val_mae: 38.4238\n",
      "Epoch 11/100\n",
      "6855/6855 [==============================] - 1s 107us/step - loss: 3038.8932 - mse: 3038.8933 - mae: 37.5948 - val_loss: 2971.0954 - val_mse: 2971.0955 - val_mae: 37.3810\n",
      "Epoch 12/100\n",
      "6855/6855 [==============================] - 1s 93us/step - loss: 2902.5776 - mse: 2902.5769 - mae: 36.7366 - val_loss: 2870.0884 - val_mse: 2870.0884 - val_mae: 37.6606\n",
      "Epoch 13/100\n",
      "6855/6855 [==============================] - 1s 95us/step - loss: 2745.2341 - mse: 2745.2341 - mae: 35.5544 - val_loss: 2741.9229 - val_mse: 2741.9231 - val_mae: 36.1995\n",
      "Epoch 14/100\n",
      "6855/6855 [==============================] - 1s 110us/step - loss: 2640.3511 - mse: 2640.3511 - mae: 34.8988 - val_loss: 2648.7487 - val_mse: 2648.7485 - val_mae: 35.4468\n",
      "Epoch 15/100\n",
      "6855/6855 [==============================] - 1s 138us/step - loss: 2523.5544 - mse: 2523.5540 - mae: 34.1082 - val_loss: 2597.8590 - val_mse: 2597.8591 - val_mae: 34.3114\n",
      "Epoch 16/100\n",
      "6855/6855 [==============================] - 1s 117us/step - loss: 2404.3001 - mse: 2404.2998 - mae: 33.2656 - val_loss: 2477.5860 - val_mse: 2477.5859 - val_mae: 34.3386\n",
      "Epoch 17/100\n",
      "6855/6855 [==============================] - 1s 136us/step - loss: 2305.2574 - mse: 2305.2568 - mae: 32.5906 - val_loss: 2427.9242 - val_mse: 2427.9241 - val_mae: 34.0787\n",
      "Epoch 18/100\n",
      "6855/6855 [==============================] - 1s 123us/step - loss: 2208.8648 - mse: 2208.8645 - mae: 31.9471 - val_loss: 2406.6956 - val_mse: 2406.6956 - val_mae: 32.9069\n",
      "Epoch 19/100\n",
      "6855/6855 [==============================] - 1s 118us/step - loss: 2149.9050 - mse: 2149.9053 - mae: 31.5196 - val_loss: 2287.8447 - val_mse: 2287.8447 - val_mae: 33.1951\n",
      "Epoch 20/100\n",
      "6855/6855 [==============================] - 1s 135us/step - loss: 2063.4924 - mse: 2063.4922 - mae: 30.9380 - val_loss: 2248.0697 - val_mse: 2248.0698 - val_mae: 32.8210\n",
      "Epoch 21/100\n",
      "6855/6855 [==============================] - 1s 135us/step - loss: 2009.4672 - mse: 2009.4672 - mae: 30.5610 - val_loss: 2207.1260 - val_mse: 2207.1260 - val_mae: 32.4090\n",
      "Epoch 22/100\n",
      "6855/6855 [==============================] - 1s 95us/step - loss: 1957.1871 - mse: 1957.1877 - mae: 30.1907 - val_loss: 2208.9145 - val_mse: 2208.9143 - val_mae: 32.6886\n",
      "Epoch 23/100\n",
      "6855/6855 [==============================] - 1s 97us/step - loss: 1925.6476 - mse: 1925.6476 - mae: 29.9804 - val_loss: 2198.8875 - val_mse: 2198.8875 - val_mae: 32.8807\n",
      "Epoch 24/100\n",
      "6855/6855 [==============================] - 1s 95us/step - loss: 1881.5686 - mse: 1881.5691 - mae: 29.6607 - val_loss: 2093.4296 - val_mse: 2093.4297 - val_mae: 31.3503\n",
      "Epoch 25/100\n",
      "6855/6855 [==============================] - 1s 98us/step - loss: 1828.5738 - mse: 1828.5735 - mae: 29.2005 - val_loss: 2081.7707 - val_mse: 2081.7708 - val_mae: 31.0342\n",
      "Epoch 26/100\n",
      "6855/6855 [==============================] - 1s 132us/step - loss: 1794.5711 - mse: 1794.5714 - mae: 28.9473 - val_loss: 2055.5629 - val_mse: 2055.5627 - val_mae: 31.1822\n",
      "Epoch 27/100\n",
      "6855/6855 [==============================] - 1s 123us/step - loss: 1763.8551 - mse: 1763.8551 - mae: 28.6967 - val_loss: 2018.2108 - val_mse: 2018.2109 - val_mae: 31.0297\n",
      "Epoch 28/100\n",
      "6855/6855 [==============================] - 1s 116us/step - loss: 1726.9085 - mse: 1726.9082 - mae: 28.3975 - val_loss: 2003.1630 - val_mse: 2003.1632 - val_mae: 30.9712\n",
      "Epoch 29/100\n",
      "6855/6855 [==============================] - 1s 116us/step - loss: 1700.5693 - mse: 1700.5698 - mae: 28.1975 - val_loss: 1984.6721 - val_mse: 1984.6721 - val_mae: 30.6795\n",
      "Epoch 30/100\n",
      "6855/6855 [==============================] - 1s 119us/step - loss: 1676.3033 - mse: 1676.3030 - mae: 28.0141 - val_loss: 1969.0534 - val_mse: 1969.0536 - val_mae: 30.3888\n",
      "Epoch 31/100\n",
      "6855/6855 [==============================] - 1s 138us/step - loss: 1657.0582 - mse: 1657.0583 - mae: 27.8721 - val_loss: 1951.7274 - val_mse: 1951.7274 - val_mae: 30.2785\n",
      "Epoch 32/100\n",
      "6855/6855 [==============================] - 1s 122us/step - loss: 1634.7988 - mse: 1634.7993 - mae: 27.6779 - val_loss: 1950.2315 - val_mse: 1950.2318 - val_mae: 30.5468\n",
      "Epoch 33/100\n",
      "6855/6855 [==============================] - 1s 118us/step - loss: 1614.1869 - mse: 1614.1868 - mae: 27.5304 - val_loss: 1909.0244 - val_mse: 1909.0245 - val_mae: 29.8875\n",
      "Epoch 34/100\n",
      "6855/6855 [==============================] - 1s 98us/step - loss: 1581.4456 - mse: 1581.4453 - mae: 27.2421 - val_loss: 1900.1663 - val_mse: 1900.1663 - val_mae: 29.9104\n",
      "Epoch 35/100\n",
      "6855/6855 [==============================] - 1s 93us/step - loss: 1569.1170 - mse: 1569.1167 - mae: 27.2032 - val_loss: 1868.1356 - val_mse: 1868.1355 - val_mae: 29.9104\n",
      "Epoch 36/100\n",
      "6855/6855 [==============================] - 1s 96us/step - loss: 1543.0560 - mse: 1543.0558 - mae: 26.9525 - val_loss: 1859.7485 - val_mse: 1859.7485 - val_mae: 29.4879\n",
      "Epoch 37/100\n",
      "6855/6855 [==============================] - 1s 96us/step - loss: 1527.4660 - mse: 1527.4661 - mae: 26.8527 - val_loss: 1851.1415 - val_mse: 1851.1414 - val_mae: 29.6165\n",
      "Epoch 38/100\n",
      "6855/6855 [==============================] - 1s 100us/step - loss: 1498.1246 - mse: 1498.1248 - mae: 26.6174 - val_loss: 1834.3296 - val_mse: 1834.3298 - val_mae: 29.4102\n",
      "Epoch 39/100\n",
      "6855/6855 [==============================] - 1s 114us/step - loss: 1485.1427 - mse: 1485.1422 - mae: 26.5166 - val_loss: 1824.0019 - val_mse: 1824.0017 - val_mae: 29.3469\n",
      "Epoch 40/100\n",
      "6855/6855 [==============================] - 1s 108us/step - loss: 1456.5346 - mse: 1456.5343 - mae: 26.2785 - val_loss: 1791.8186 - val_mse: 1791.8186 - val_mae: 29.2485\n",
      "Epoch 41/100\n",
      "6855/6855 [==============================] - 1s 117us/step - loss: 1439.6803 - mse: 1439.6801 - mae: 26.1258 - val_loss: 1794.5306 - val_mse: 1794.5305 - val_mae: 29.4921\n",
      "Epoch 42/100\n",
      "6855/6855 [==============================] - 1s 101us/step - loss: 1414.6445 - mse: 1414.6445 - mae: 25.9260 - val_loss: 1799.3507 - val_mse: 1799.3508 - val_mae: 28.8777\n",
      "Epoch 43/100\n",
      "6855/6855 [==============================] - 1s 104us/step - loss: 1389.9022 - mse: 1389.9027 - mae: 25.7350 - val_loss: 1743.9225 - val_mse: 1743.9222 - val_mae: 28.8176\n",
      "Epoch 44/100\n",
      "6855/6855 [==============================] - 1s 102us/step - loss: 1372.1187 - mse: 1372.1190 - mae: 25.6055 - val_loss: 1740.3871 - val_mse: 1740.3872 - val_mae: 29.1735\n",
      "Epoch 45/100\n",
      "6855/6855 [==============================] - 1s 100us/step - loss: 1362.8167 - mse: 1362.8168 - mae: 25.5521 - val_loss: 1732.9338 - val_mse: 1732.9335 - val_mae: 28.8346\n",
      "Epoch 46/100\n",
      "6855/6855 [==============================] - 1s 100us/step - loss: 1338.9226 - mse: 1338.9222 - mae: 25.3339 - val_loss: 1695.7375 - val_mse: 1695.7375 - val_mae: 28.6601\n",
      "Epoch 47/100\n",
      "6855/6855 [==============================] - 1s 101us/step - loss: 1315.2737 - mse: 1315.2739 - mae: 25.1183 - val_loss: 1678.8224 - val_mse: 1678.8225 - val_mae: 28.2694\n",
      "Epoch 48/100\n",
      "6855/6855 [==============================] - 1s 101us/step - loss: 1300.5171 - mse: 1300.5167 - mae: 25.0144 - val_loss: 1676.1148 - val_mse: 1676.1150 - val_mae: 28.2272\n",
      "Epoch 49/100\n",
      "6855/6855 [==============================] - 1s 101us/step - loss: 1288.5673 - mse: 1288.5676 - mae: 24.9243 - val_loss: 1662.7076 - val_mse: 1662.7075 - val_mae: 28.3276\n",
      "Epoch 50/100\n",
      "6855/6855 [==============================] - 1s 102us/step - loss: 1267.4541 - mse: 1267.4540 - mae: 24.7144 - val_loss: 1653.7069 - val_mse: 1653.7072 - val_mae: 28.1140\n",
      "Epoch 51/100\n",
      "6855/6855 [==============================] - 1s 84us/step - loss: 1246.4424 - mse: 1246.4423 - mae: 24.5098 - val_loss: 1628.1482 - val_mse: 1628.1482 - val_mae: 28.1022\n",
      "Epoch 52/100\n",
      "6855/6855 [==============================] - 1s 82us/step - loss: 1233.0275 - mse: 1233.0276 - mae: 24.3952 - val_loss: 1641.2886 - val_mse: 1641.2885 - val_mae: 28.3091\n",
      "Epoch 53/100\n",
      "6855/6855 [==============================] - 1s 83us/step - loss: 1220.8255 - mse: 1220.8251 - mae: 24.3168 - val_loss: 1627.8296 - val_mse: 1627.8292 - val_mae: 27.7889\n",
      "Epoch 54/100\n",
      "6855/6855 [==============================] - 1s 82us/step - loss: 1201.9873 - mse: 1201.9875 - mae: 24.1516 - val_loss: 1613.0340 - val_mse: 1613.0338 - val_mae: 28.0653\n",
      "Epoch 55/100\n",
      "6855/6855 [==============================] - 1s 82us/step - loss: 1187.1506 - mse: 1187.1506 - mae: 23.9957 - val_loss: 1584.9315 - val_mse: 1584.9318 - val_mae: 27.8132\n",
      "Epoch 56/100\n",
      "6855/6855 [==============================] - 1s 114us/step - loss: 1173.5191 - mse: 1173.5193 - mae: 23.8841 - val_loss: 1580.3582 - val_mse: 1580.3583 - val_mae: 27.4865\n",
      "Epoch 57/100\n",
      "6855/6855 [==============================] - 1s 123us/step - loss: 1164.1502 - mse: 1164.1501 - mae: 23.7745 - val_loss: 1594.9716 - val_mse: 1594.9717 - val_mae: 27.7470\n",
      "Epoch 58/100\n",
      "6855/6855 [==============================] - 1s 129us/step - loss: 1151.3431 - mse: 1151.3431 - mae: 23.6695 - val_loss: 1575.4820 - val_mse: 1575.4821 - val_mae: 27.6078\n",
      "Epoch 59/100\n",
      "6855/6855 [==============================] - 1s 124us/step - loss: 1147.1171 - mse: 1147.1172 - mae: 23.6234 - val_loss: 1558.3035 - val_mse: 1558.3040 - val_mae: 27.4983\n",
      "Epoch 60/100\n",
      "6855/6855 [==============================] - 1s 120us/step - loss: 1130.7698 - mse: 1130.7697 - mae: 23.4707 - val_loss: 1558.4466 - val_mse: 1558.4467 - val_mae: 27.2664\n",
      "Epoch 61/100\n",
      "6855/6855 [==============================] - 1s 116us/step - loss: 1116.8784 - mse: 1116.8784 - mae: 23.3186 - val_loss: 1537.6835 - val_mse: 1537.6832 - val_mae: 27.1918\n",
      "Epoch 62/100\n",
      "6855/6855 [==============================] - 1s 121us/step - loss: 1112.5199 - mse: 1112.5200 - mae: 23.3041 - val_loss: 1529.1122 - val_mse: 1529.1123 - val_mae: 27.0898\n",
      "Epoch 63/100\n",
      "6855/6855 [==============================] - 1s 122us/step - loss: 1094.5038 - mse: 1094.5039 - mae: 23.1165 - val_loss: 1526.3524 - val_mse: 1526.3524 - val_mae: 27.0882\n",
      "Epoch 64/100\n",
      "6855/6855 [==============================] - 1s 114us/step - loss: 1080.3696 - mse: 1080.3696 - mae: 22.9520 - val_loss: 1519.1723 - val_mse: 1519.1726 - val_mae: 27.0114\n",
      "Epoch 65/100\n",
      "6855/6855 [==============================] - 1s 104us/step - loss: 1071.9252 - mse: 1071.9253 - mae: 22.8910 - val_loss: 1509.5296 - val_mse: 1509.5298 - val_mae: 27.1078\n",
      "Epoch 66/100\n",
      "6855/6855 [==============================] - 1s 98us/step - loss: 1069.1808 - mse: 1069.1808 - mae: 22.8558 - val_loss: 1513.8518 - val_mse: 1513.8518 - val_mae: 26.9867\n",
      "Epoch 67/100\n",
      "6855/6855 [==============================] - 1s 97us/step - loss: 1050.6543 - mse: 1050.6541 - mae: 22.6538 - val_loss: 1503.2386 - val_mse: 1503.2385 - val_mae: 27.0949\n",
      "Epoch 68/100\n",
      "6855/6855 [==============================] - 1s 96us/step - loss: 1055.7956 - mse: 1055.7957 - mae: 22.7426 - val_loss: 1495.0159 - val_mse: 1495.0157 - val_mae: 26.8626\n",
      "Epoch 69/100\n",
      "6855/6855 [==============================] - 1s 108us/step - loss: 1033.6629 - mse: 1033.6628 - mae: 22.4809 - val_loss: 1505.9496 - val_mse: 1505.9497 - val_mae: 27.1623\n",
      "Epoch 70/100\n",
      "6855/6855 [==============================] - 1s 118us/step - loss: 1023.4980 - mse: 1023.4981 - mae: 22.3855 - val_loss: 1487.9487 - val_mse: 1487.9486 - val_mae: 26.9748\n",
      "Epoch 71/100\n",
      "6855/6855 [==============================] - 1s 120us/step - loss: 1022.9524 - mse: 1022.9526 - mae: 22.3852 - val_loss: 1499.7203 - val_mse: 1499.7206 - val_mae: 27.1678\n",
      "Epoch 72/100\n",
      "6855/6855 [==============================] - 1s 116us/step - loss: 1012.6753 - mse: 1012.6754 - mae: 22.2792 - val_loss: 1488.2207 - val_mse: 1488.2208 - val_mae: 27.0050\n",
      "Epoch 73/100\n",
      "6855/6855 [==============================] - 1s 131us/step - loss: 998.4750 - mse: 998.4750 - mae: 22.1326 - val_loss: 1478.5356 - val_mse: 1478.5353 - val_mae: 26.6676\n",
      "Epoch 74/100\n",
      "6855/6855 [==============================] - 1s 119us/step - loss: 991.8004 - mse: 991.8005 - mae: 22.0449 - val_loss: 1483.6846 - val_mse: 1483.6848 - val_mae: 26.7164\n",
      "Epoch 75/100\n",
      "6855/6855 [==============================] - 1s 125us/step - loss: 981.1330 - mse: 981.1330 - mae: 21.9399 - val_loss: 1458.9936 - val_mse: 1458.9934 - val_mae: 26.4956\n",
      "Epoch 76/100\n",
      "6855/6855 [==============================] - 1s 125us/step - loss: 970.0355 - mse: 970.0355 - mae: 21.7875 - val_loss: 1466.6880 - val_mse: 1466.6884 - val_mae: 26.7580\n",
      "Epoch 77/100\n",
      "6855/6855 [==============================] - 1s 121us/step - loss: 969.0204 - mse: 969.0201 - mae: 21.8215 - val_loss: 1463.2497 - val_mse: 1463.2495 - val_mae: 26.6329\n",
      "Epoch 78/100\n",
      "6855/6855 [==============================] - 1s 122us/step - loss: 963.3176 - mse: 963.3176 - mae: 21.7358 - val_loss: 1451.9179 - val_mse: 1451.9177 - val_mae: 26.6182\n",
      "Epoch 79/100\n",
      "6855/6855 [==============================] - 1s 123us/step - loss: 956.1658 - mse: 956.1660 - mae: 21.7002 - val_loss: 1464.5119 - val_mse: 1464.5120 - val_mae: 26.6501\n",
      "Epoch 80/100\n",
      "6855/6855 [==============================] - 1s 122us/step - loss: 947.5671 - mse: 947.5670 - mae: 21.5739 - val_loss: 1453.5377 - val_mse: 1453.5380 - val_mae: 26.5088\n",
      "Epoch 81/100\n",
      "6855/6855 [==============================] - 1s 124us/step - loss: 935.2103 - mse: 935.2104 - mae: 21.4422 - val_loss: 1451.4512 - val_mse: 1451.4512 - val_mae: 26.5877\n",
      "Epoch 82/100\n",
      "6855/6855 [==============================] - 1s 118us/step - loss: 926.8044 - mse: 926.8043 - mae: 21.3249 - val_loss: 1448.0511 - val_mse: 1448.0511 - val_mae: 26.4564\n",
      "Epoch 83/100\n",
      "6855/6855 [==============================] - 1s 124us/step - loss: 929.6251 - mse: 929.6250 - mae: 21.3763 - val_loss: 1458.2201 - val_mse: 1458.2202 - val_mae: 26.5950\n",
      "Epoch 84/100\n",
      "6855/6855 [==============================] - 1s 120us/step - loss: 920.2854 - mse: 920.2855 - mae: 21.2848 - val_loss: 1451.9002 - val_mse: 1451.9003 - val_mae: 26.3742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/100\n",
      "6855/6855 [==============================] - 1s 113us/step - loss: 919.4138 - mse: 919.4137 - mae: 21.2870 - val_loss: 1454.2937 - val_mse: 1454.2937 - val_mae: 26.6006\n",
      "Epoch 86/100\n",
      "6855/6855 [==============================] - 1s 115us/step - loss: 910.4268 - mse: 910.4268 - mae: 21.1605 - val_loss: 1456.6155 - val_mse: 1456.6154 - val_mae: 26.6129\n",
      "Epoch 87/100\n",
      "6855/6855 [==============================] - 1s 112us/step - loss: 896.8170 - mse: 896.8169 - mae: 20.9925 - val_loss: 1434.1384 - val_mse: 1434.1384 - val_mae: 26.4222\n",
      "Epoch 88/100\n",
      "6855/6855 [==============================] - 1s 107us/step - loss: 889.4265 - mse: 889.4266 - mae: 20.9226 - val_loss: 1429.7807 - val_mse: 1429.7809 - val_mae: 26.2525\n",
      "Epoch 89/100\n",
      "6855/6855 [==============================] - 1s 92us/step - loss: 884.4129 - mse: 884.4128 - mae: 20.8641 - val_loss: 1448.8451 - val_mse: 1448.8450 - val_mae: 26.5517\n",
      "Epoch 90/100\n",
      "6855/6855 [==============================] - 1s 98us/step - loss: 880.1832 - mse: 880.1830 - mae: 20.8124 - val_loss: 1445.7557 - val_mse: 1445.7557 - val_mae: 26.6098\n",
      "Epoch 91/100\n",
      "6855/6855 [==============================] - 1s 95us/step - loss: 877.5480 - mse: 877.5482 - mae: 20.7875 - val_loss: 1435.6000 - val_mse: 1435.5997 - val_mae: 26.4391\n",
      "Epoch 92/100\n",
      "6855/6855 [==============================] - 1s 93us/step - loss: 874.1864 - mse: 874.1865 - mae: 20.7766 - val_loss: 1422.7934 - val_mse: 1422.7935 - val_mae: 26.3164\n",
      "Epoch 93/100\n",
      "6855/6855 [==============================] - 1s 103us/step - loss: 866.6818 - mse: 866.6819 - mae: 20.6699 - val_loss: 1453.5081 - val_mse: 1453.5079 - val_mae: 26.6304\n",
      "Epoch 94/100\n",
      "6855/6855 [==============================] - 1s 107us/step - loss: 860.3397 - mse: 860.3399 - mae: 20.6027 - val_loss: 1422.4317 - val_mse: 1422.4318 - val_mae: 26.2149\n",
      "Epoch 95/100\n",
      "6855/6855 [==============================] - 1s 108us/step - loss: 855.4691 - mse: 855.4688 - mae: 20.5414 - val_loss: 1445.9752 - val_mse: 1445.9752 - val_mae: 26.4904\n",
      "Epoch 96/100\n",
      "6855/6855 [==============================] - 1s 108us/step - loss: 846.5245 - mse: 846.5248 - mae: 20.4370 - val_loss: 1435.9446 - val_mse: 1435.9445 - val_mae: 26.4755\n",
      "Epoch 97/100\n",
      "6855/6855 [==============================] - 1s 114us/step - loss: 842.2387 - mse: 842.2387 - mae: 20.3719 - val_loss: 1464.7498 - val_mse: 1464.7498 - val_mae: 26.8669\n",
      "Epoch 98/100\n",
      "6855/6855 [==============================] - 1s 117us/step - loss: 840.8955 - mse: 840.8956 - mae: 20.3680 - val_loss: 1430.5306 - val_mse: 1430.5306 - val_mae: 26.2101\n",
      "Epoch 99/100\n",
      "6855/6855 [==============================] - 1s 118us/step - loss: 828.8268 - mse: 828.8266 - mae: 20.2183 - val_loss: 1426.1414 - val_mse: 1426.1416 - val_mae: 26.2765\n",
      "Epoch 100/100\n",
      "6855/6855 [==============================] - 1s 126us/step - loss: 830.7298 - mse: 830.7300 - mae: 20.2383 - val_loss: 1426.1531 - val_mse: 1426.1530 - val_mae: 26.3356\n",
      "38.368930365196654\n",
      "30.52551651321333\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "features = ['Temperature','Relative Humidity','windX','windY','Year','MonthX','MonthY','hourX','hourY','isWeekend']\n",
    "features = []\n",
    "rlist=['PM2.5','PM10','NO','NO2','CO']\n",
    "newlist = rlist + ['Temperature','Relative Humidity','windX','windY','Year','MonthX','MonthY','hourX','hourY','isWeekend']\n",
    "for i in newlist:\n",
    "    for j in range(24):\n",
    "        features.append(i+'_t-'+str(j))\n",
    "predVector = []\n",
    "for j in range(24):\n",
    "    predVector.append('PM2.5_t+'+str(j))\n",
    "X = df[features]\n",
    "y = df[predVector]\n",
    "scaler = StandardScaler()\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "scaler.fit(Xtrain)\n",
    "print(Xtrain.shape)\n",
    "with tf.device('/CPU:0'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(200, input_dim=360, kernel_initializer='normal', activation='sigmoid'))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(24, activation='linear'))\n",
    "    model.summary()\n",
    "    #Fit\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['mse','mae'])\n",
    "    history = model.fit(scaler.transform(Xtrain), ytrain, epochs=100, batch_size=50,  verbose=1, validation_split=0.2)\n",
    "    #Print Accuracy\n",
    "    testPred = model.predict(scaler.transform(Xtest))\n",
    "    trainPred = model.predict(scaler.transform(Xtrain))\n",
    "    print(mean_squared_error(testPred, ytest,squared=False))\n",
    "    print(mean_squared_error(trainPred, ytrain,squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'val_mse', 'val_mae', 'loss', 'mse', 'mae'])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEXCAYAAACkpJNEAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAABFSklEQVR4nO3deXwU9f348dfMXrnvhAQSAuEKN8hluCIeyOmBWIHWo6iV1mLFqrW21lvQ+it+bWtr69GKVdGqBSmiVASBcN8I4Qg5IeS+jz1m5vdHNIIkQCCbkJ338/HwITs7O/t+7272vfP5zOfzUQzDMBBCCGE6ansHIIQQon1IARBCCJOSAiCEECYlBUAIIUxKCoAQQpiUFAAhhDApKQBCnKd77rmHjz766Kz7bNmyhWnTpp33diHakxQAIYQwKWt7ByCEN2zZsoU//OEPxMXFkZmZib+/Pz/5yU9YsmQJmZmZTJw4kUcffRSApUuXsmTJElRVJSoqiscee4zu3btTUFDAI488QmFhIZ07d6akpKTx+BkZGTz77LOUl5ejaRq33norM2fOPK/YqqqqePLJJ0lPT0dRFMaNG8cDDzyA1Wrl5ZdfZvXq1dhsNsLDw1m4cCExMTHNbhfiohhC+KDNmzcbffv2Nb7++mvDMAzjzjvvNG655RbD6XQaJSUlRv/+/Y2TJ08aaWlpxtVXX22UlJQYhmEYH374oTF58mRD13XjZz/7mbF48WLDMAwjKyvLGDJkiPHhhx8abrfbmDJlirF//37DMAyjsrLSmDx5srFr1y5j8+bNxtSpU5uM59vtDz/8sPH0008buq4bTqfTmDt3rvHqq68aJ06cMC677DLD6XQahmEYr7/+urF69epmtwtxseQMQPis+Ph4+vXrB0DXrl0JDg7GbrcTERFBYGAgFRUVrF+/nilTphAREQHAjBkzePbZZ8nLyyMtLY1f/epXACQmJjJq1CgAsrKyyMnJaTyDAKivr+fAgQP06NHjnHF99dVXvPvuuyiKgt1uZ9asWfzzn//krrvuIjk5mRtvvJHx48czfvx4UlJS0HW9ye1CXCwpAMJn2e32025brWd+3HVdP2ObYRh4PB4URcE4Zaqsbx+vaRrBwcEsW7as8b7i4mKCg4PZvXv3OePSdR1FUU677fF4UFWVt99+m3379rFp0yaee+45xo0bx8MPP9zsdiEuhnQCC1MbN24cK1eupLS0FIAPP/yQsLAwEhMTGTduHEuXLgXgxIkTbNmyBYDu3bvj5+fXWADy8/OZNm0a+/fvP6/nHDt2LG+//TaGYeByuXj//fcZPXo06enpTJs2jR49enDPPfdwxx13sG/fvma3C3Gx5AxAmNqYMWO44447uP3229F1nYiICF599VVUVeXxxx/n17/+NZMnTyY2Npbk5GSg4czilVde4dlnn+W1117D4/Hwi1/8gmHDhjUWibP57W9/yzPPPMP06dNxu92MGzeOefPmYbfbmTx5MjfddBMBAQH4+fnx29/+luTk5Ca3C3GxFMOQ6aCFEMKMpAlICCFMSgqAEEKYlBQAIYQwKSkAQghhUlIAhBDCpKQACCGESXW4cQBlZTXoesuvXI2MDKKkpNoLEV3azJi3GXMGc+ZtxpyhZXmrqkJ4eGCT93W4AqDrxgUVgG8fa0ZmzNuMOYM58zZjztA6eUsTkBBCmJQUACGEMKkO1wQkhBDNMQyDsrIiXK56wHebhgoL1e/NZKtgt/sRHh592kyz5yIFQAjhM6qrK1AUhU6d4lEU323gsFpVPJ7vCoBh6JSXF1NdXUFwcNh5H8d3XyEhhOnU1VUTHBzm01/+TVEUleDgcOrqWnZFlLleJSGET9N1DYvFnA0bFosVXdda9BivvVIffPABb7/9duPtvLw8rr/+eq6++moWLlyI0+lk8uTJLFiwwFshNFq7+zj7jpUyf8ZArz+XEKJ9taQN3JdcSN5eKwA333wzN998MwBHjhzh3nvv5e6772b27NksWbKEuLg47rnnHtatW0dqaqq3wgCgpKKevUeL0Q0D1aQfDiFE2/p//+959u3bg8fjJi8vl27dkgC4+eZZTJ163Tkff8cdc/jHP97xaoxtcq70xBNPsGDBAnJzc0lMTCQhIQGA6dOns2rVKq8XgJAAO5puUFvvIcjf5tXnEkIIgF/+8lcA5OefYP78e1r8Ze7tL39ogwKQlpZGfX09kydPZsWKFURHRzfeFxMTQ0FBgbdDICSwYXHwihqXFAAhRLuaOXM6/foN4MiRQ7zyymu8//677NixjcrKSqKionjqqYVEREQyduxwNmzYzuuvv0pxcRG5uTkUFJxk2rTrufPOu1slFq8XgPfee48f//jHAOi6flo7lWEYLW63iowManEMXbvUA2CxWYmODm7x4zs6ydk8zJj3qTkXFqpYrQ3XtmzYe4Kvdp/wynOOH9KZsYM6n9e+FktDPN/GBTB69Biee+55cnNzyM3N5rXX/oGqqjz55GOsXr2KH/7w1sbHqKpCRsZRXn31daqqqpg58zp+8INZBAef+V6rqtqiz4BXC4DL5WLbtm0sWrQIgNjYWIqKihrvLyoqIiYmpkXHLCmpbvEcGLrLA0DOiXJiQx0temxHFx0dTFFRVXuH0abMmDOYM+/v56zreuP18Zpm4K0VzzXNOO06/LPv27DfqfsnJ/fH49GJi4vn3nvv5+OPPyInJ5t9+/YSF9elcV+PR0fXDYYOHYaiWAgJCSM4OITq6ir8/c+c4E3X9TM+A6qqNPvD2asF4NChQ3Tr1o2AgAAABg8eTGZmJtnZ2cTHx7NixQpuuukmb4YAnN4EJIQwhzED4xgzMK69w2iSw9HwQzQ9/SBPPPEbZs2aw4QJV2GxqBhNVC273d74b0VRWq2weXUcQG5uLrGxsY23HQ4HixYtYv78+UyZMoWkpCQmTZrkzRAACPS3oaoKVbVSAIQQl47du3cwdOgwbrhhJgkJXUlL2/C9KR68y6tnAFOmTGHKlCmnbUtJSWH58uXefNozqIpCaKCdSjkDEEJcQq66aiKPPvoQt912CwB9+vQlP987/RZNUYymzjcuYRfSBwDw9FvbCQ2wc9/MQV6I6tIl7cLmYca8v5/zyZPZxMYmtmNEbeP7cwF9q6n8z9YHYJqpIMKCHFRKE5AQQjQyTQEIDXZIE5AQQpzCNAUgLKihAHSwFi8hhPAaUxUAl0fH6W7ZbHlCCOGrzFMAghuuu5VmICGEaGC+AlDrbudIhBDi0mCaAhAaJGcAQghxKtMsnRMuTUBCiDZ0sesBVFdX8+yzT7Bw4Ytei9E0BSAk8NsmICkAQgjvu9j1AKqqKjly5JA3QmtkmgJgs6oE+lnlDEAIk3Af3oj70FdeObatz3hsvce0+HF5ebm8+OJCKisrcDj8WLDgIXr3Tubzz1fxzjtvoaoqnTt35rHHnuall35PcXERv/71g147CzBNHwA0zAoqBUAI0V6effZxfvaz+3jjjX/x8MO/4fHHHwXg73//C4sX/4k33nibuLgu5ORkcf/9DxEVFS1NQK0lOMAuVwEJYRK23mMu6Fe6t9TW1nLw4AGee+6pxm11dXVUVJQzZsw4fvrTOxk//gpSU6+kV68+bTIpnKkKQEignbzC6vYOQwhhQrquY7c7TusLKCwsICQklPvvf5CjR69n06YNPP30Y8yd+xMGDRri9ZhM1QQUGiBNQEKI9hEUFER8fAKffbYSgG3bNnPvvT9B0zRmzbqRsLAwbr31x0yaNJXDhw9hsVjQNO/OXGCqM4DgQBu1Tg8eTcdqMVXtE0JcAh5//Bl+//vneOedt7BabTz11HNYrVbuvPMe7r//XhwOB+Hh4fzmN08QHBxCp06xzJ9/D3/846teicdUBeDbpSEra1xEhPi1czRCCDOIi+vMv//9CQCJid3405/+dsY+11wziWuuOXN1xL/+9Q2vxmaqn8GhAQ0FoEo6goUQwlwFIFgWhxdCiEamKgCnNgEJIXyTWdf8uJC8TVUAvmsCkgIghC9SVQua5mnvMNqFpnlQVUuLHmOqAuCwW7DbVGkCEsJH+fsHUVVVjmGcuWC6LzMMnaqqMvz9m178vTmmuQrI0Buupw0JsMuEcEL4qKCgUMrKiigoyAN8tylIVVV0/dQip2C3+xEUFNqi45iiALj2fU7e0fU4bniSkEA7VXIGIIRPUhSFiIiY9g7D66Kjgykqqrro45iiCUhxBOIuykUvziYkwE5FjVwGKoQQpigAlq6DAAVP9u6GGUGlCUgIIcxRAFS/YBzxvfHk7CEsyE5VrQun27tzbAghxKXOFAUAILDXcPTiLHpFgmHAseMV7R2SEEK0K9MUgICewwHopmWhKgrpOeXtG5AQQrQz0xQAW3QCSlAk6ol9JMYGcyinrL1DEkKIdmWaAqAoCtauQ/Ac/5p+8YEcy6+UfgAhhKmZpgAAWBOHgMfFoKASPJoh/QBCCFMzVQGwxPUBq4O4+qPSDyCEMD2vFoA1a9YwY8YMJk+ezDPPPANAWloa06dPZ+LEiSxevNibT38GxWrHGt8f8vaS2ClI+gGEEKbmtQKQm5vL448/ziuvvMLy5cs5cOAA69at49FHH+WVV15h5cqV7N+/n3Xr1nkrhCZZ4gdg1JRyWZwh/QBCCFPzWgFYvXo1U6ZMITY2FpvNxuLFi/H39ycxMZGEhASsVivTp09n1apV3gqhSZbYPgD0DZB+ACGEuXltMrjs7GxsNhvz5s0jPz+fK664gl69ehEdHd24T0xMDAUFBd4KoUlqeBw4Aolx56EqPUnPKadvt4g2jUEIIS4FXisAmqaxfft2lixZQkBAAD/96U/x8/NDUZTGfQzDOO32+YiMbNl816eKjg4GQO/aD3fJMXomDOPYyarG7b7K1/NrihlzBnPmbcacoXXy9loBiIqKIiUlhYiIhl/XV199NatWrcJi+W7FmqKiImJiWjZ1a0lJNbre8nm+T50+VYtIwn1kG327q6zYVUreiXIctpatpNNRtNa0sR2JGXMGc+ZtxpyhZXmrqtLsD2ev9QFMmDCBDRs2UFlZiaZprF+/nkmTJpGZmUl2djaaprFixQrGjx/vrRCaZYntBcDA4FI03eCQXA4qhDAhr50BDB48mLvuuos5c+bgdrsZM2YMs2fPJikpifnz5+N0OklNTWXSpEneCqFZalQ3sNiJ9RzHbk1g/7ESBvWIbPM4hBCiPXl1RbCZM2cyc+bM07alpKSwfPlybz7tOSkWK5ZOPTAKjtKn6yD2Z5a2azxCCNEeTDUS+FSW2N7opTkM7hrAydJaisrr2jskIYRoU6YuABgGA4IbRgPLWYAQwmzMWwA69QBFJaQmh6hQP/YfK2nvkIQQok2ZtgAoNj/UqET0k4cZkBTJgewyPJre3mEJIUSbMW0BgIZmIK3oGIO6BuN0aRzNk2khhBDmYeoCYE0cApqHXkYGFlWRfgAhhKmYugBY4vqghMSgZGygZ5dQ6QcQQpiKqQuAoqjYksej5R9iRBednMJqyqqcp+1j6Dqug2sxNHc7RSmEEN5h6gIAYOs9FhSVwaQDsO3g6bOTajl7cK7/B57s3e0QnRBCeI/pC4AaEIY1cQiO3K107xTApq9PLwCeEwcA0CuL2iM8IYTwGtMXAABb8niMukomx5eTXVBFfklN433a8YMAGFWF7RWeEEJ4hRQAwBI/CCUwgt71+1AUGs8C9NoK9LK8hn/LGYAQwsdIAQAUVcXWZxxK/gFGJljY/PVJDMNAy2/oF1CCo9GrpAAIIXyLFIBv2JLHgwJXhxyjuKKejOOVaMcPgN0fa/fhGFUlGLqMFBZC+A4pAN9QgyKxJg4lpmQHAVadTQdO4jlxEGtcMmpYLBgaRo0MFBNC+A4pAKew9bsKnNVMjy/hSHoGRmUhls59UYMbFrKXZiAhhC+RAnAKS5e+KKGxXMYBunjyvtnW77sCUClXAgkhfIcUgFMoioq935X4VWZzhX869WoAangXlKAIUFQMuRJICOFDpAB8j633GLDa6WwpJd3VCY9moKgWlKBIaQISQvgUKQDfozgCsfVMASDd2Ymvv5khVA2RS0GFEL5FCkAT7IMno8Ylc0zpzrb0hkFhanC0NAEJIXzKOQtARkYGH3zwAYZhcP/993P11VezefPmtoit3aihsQROf4Tevbuy60gxbo+GEhKNUV+F4ZLF44UQvuGcBeDxxx/H4XCwdu1aCgoKePbZZ1m8eHFbxNbuRvSNod6lse9YKWpwDAB6VXE7RyWEEK3jnAXA6XRy3XXXsWHDBiZPnsyoUaNwu80xN35y13CC/G1sSy9EDfl2LIBcCiqE8A3nLAAul4vi4mLWrl3L6NGjKS4uxul0nuthPsFqURnWJ5rdR4rx+EUAYFTKGYAQwjecswDccsstTJgwgWHDhtGzZ09mzpzJ7bff3haxXRJGJMfgdGvsyqkFm7+cAQghfIb1XDvMmTOHWbNmoaoNteLjjz8mPDzc64FdKpITw4kJ92fNzhMMDImWaaGFED7jvK4C+vDDDxuvArr55pt9/iqgU6mKwpWXxXP0eAX19nAMGQsghPARchXQeRg7MBa7TSWzyoFeVYRhyLTQQoiOT64COg8BfjZGD4hjf5EKmgejtqK9QxJCiIsmVwGdp6su60KhJxCQaaGFEL5BrgI6T12igwjuFA+A6+BaaQYSQnR4chVQC4wcMYBVqwYx6UgaTkcgjpQ5KIrS3mEJIcQFOWcBqK2t5YUXXuCrr77C4/EwZswYfvOb3xAUFHTOg996662UlpZitTY8zVNPPUVNTQ0LFy7E6XQyefJkFixYcPFZtJGhvaJ4/8tRdLIoDN2/GsUegGP4je0dlhBCXJBzFoCFCxeiaRp//vOf0TSNd955h6effprnn3/+rI8zDIOsrCy+/PLLxgJQX1/PpEmTWLJkCXFxcdxzzz2sW7eO1NTU1snGy1RVYeKIRP65up4+g/xg5zKUoAjsyR0jfiGEONU5C8CePXtYvnx54+1nnnmGqVOnnvPAx44dA2Du3LmUl5fzgx/8gN69e5OYmEhCQgIA06dPZ9WqVR2mAACMHRTHsg2ZvF+Twp1dqnCmvYO1c1/UkJj2Dk0IIVrknJ3Amqah6991eOq6jsViOeeBKysrSUlJ4c9//jP/+Mc/eO+99zhx4gTR0dGN+8TExFBQUHCBobcPh83ClZd1YVdGKRUD54CqUr/2NQxdOoWFEB3LOc8AUlJSuP/++5k9ezYA7777LqNGjTrngYcOHcrQoUMbb8+cOZOXX36ZYcOGNW4zDKPFnaiRkefue2hOdHTwBT/2VD+YmMyqrbmsPVrPj6+9i6JP/og9cy1hl1/fKsdvba2Vd0dixpzBnHmbMWdonbzPWQAeeeQRXnnlFf7whz+gaRrjxo3jZz/72TkPvH37dtxuNykpDcsrGoZBly5dKCr67hr6oqIiYmJa1nRSUlKNrhstegw0vFhFRVUtflxzxgyM5csduUwanoJft8so/fId6sN7YYlIaLXnaA2tnXdHYMacwZx5mzFnaFneqqo0+8P5nE1AVquV++67jw8++ICPPvqIBQsW4HA4zvmkVVVVvPDCCzidTqqrq/n444954IEHyMzMJDs7G03TWLFiBePHjz+vJC41145IQNMNVm7JwTHuDhRHAHWrXkKvKWvv0IQQ4rw0ewYwdOjQszbP7Ny586wHnjBhAnv27OGGG25A13XmzJnD0KFDWbRoEfPnz8fpdJKamsqkSZMuPPp2FBMewPjBnVm76zhXD4snatID1K5YRN2qPxAw/dco9oD2DlEIIc5KMQyjyfaU48ePn/WBXbp08UpA53KpNAEBVFQ7eeRvm+nfLYKfzxiIJ28/dZ8uxhLbC/8pv0Sx2Fr1+S6EGU+RzZgzmDNvM+YMrdcE1OwZQHt9wXckoUEOpozqysfrMzmcW07vhAH4XXEn9V/+Deemd/Ebe1t7hyiEEM06Zx+AOLuJI7sSHuxg6Zoj6IaBrddobP2vxn3wS7TSs59FCSFEe5ICcJEcNgszxieRmV/F9vSG5SIdw24Amz/Oze+2b3BCCHEWF1QASktLWzuODi1lQCydIgJYsyMPAMUvCMdl16Pl7ceTu7edoxNCiKY1WwDmzp3b+O9XX331tPvuvPNO70XUAamKwtiBsRzOq6CgtBYAW/+rUEI64dz8HoautXOEQghxpmYLwKm/8letWnXafc1cOGRqowfEoSiwYV8+AIrFimPUD9DLTuDcsARP/iEMj6udoxRCiO80exXQqWMAvv+FL3Pgnyk82MHApEg27svnxnFJqKqCtdtlWHtejjt9He70tWCxYu2Rgt/YW1Gs9vYOWQhhcs2eAZz6pS9f+Odn7MA4yqtd7M9sOHtSFAX/K+cRdNsf8b/2F9j6jMdzeAO1nyyUEcNCiHZ3XmcA4vwM6RVFkL+NDfvyGdQjsnG74heENXFow38JA6lb8yq1Hz+JfdBk9KpC9LITqKGx+I0z31KbQoj202wBOHbsGNOnTwcgJyen8d8Aubm53o+sA7JaVFL6x7JmZx5VtS6CA85s5rEmDiXg+t9S99lLDZeJ2vxQ7AFoJ9Kxj5iB6mfOmQ2FEG2v2QLw97//vS3j8BnjBsWxensum/afZOLIrk3uY4mIJ/Dm5zDqq1ACI9CLjlH7n6fRjh9A7XHuqbaFEKI1NFsARo4ceca28vJyQkNDpXnoLOJjguiTEMaKTdmMHhhHkH/T8wEpVjtKUEMzkRrVHewBaHn7sUkBEEK0kWY7gaurq3nwwQfZunUrAA888AApKSlcc801ZGdnt1mAHdEPJ/amzunhgy+Pntf+iqpi7dIPT97XcomtEKLNNFsAnn/+eQIDA+nZsyfr1q1j06ZNrFmzhscee+ycC8KbXXx0EBNHJLB+bz6Hc8vP6zGWLv0xakrRK/K9G5wQQnyj2QKwe/dunnjiCSIiIvjqq6+45ppriIuLIzU1laysrDYMsWO6bkx3IkMcLPnsEB7t3OsFW+MHAKDlfe3t0IQQAjhLAbBYLI1t/bt27TqtT0CaKc7NYbfww2v6cLy4hs+3nfuqKTUkGiUkBk/e/jaITgghzlIAVFWlqqqKgoICDh061LgQfEFBATZb+y900hEM6RXFZb2jWbYhk4Ky2nPub40fgJZ/CEP3tEF0Qgiza7YA/OhHP+LGG29kzpw5TJ48mejoaNasWcPcuXOZPXt2W8bYof3wmt5YLQpvrTp0zjMnS5f+4K5HK8hoo+iEEGbW7GWgM2bMoGfPnhQXFzcu3F5WVsZdd93FjTfe2GYBdnThwQ5untCTt1YdYsPefMYN7tzsvtbOyaAoaHn7scb1acMohRBm1GwBABg0aNBpt2+66SavBuOrxg/uzOavC1i65iiDekQSGuRocj/FEYga0wNPzm7sQ6ah2JreTwghWkOzBeDUqR+a8sknn7R6ML5KVRRun9SHx9/Yxt9XHGD+TYNw2CxN7mvrmYJz4xJq3n0Q++Ap2PpdKYVACOEVzRaA2tpanE4n1113HePGjcNiafoLS5yfuMhAbr22N//4NJ0X393FL24e3OQoYXv/q1Aju+La8R+cW5bi3Po+oIBhoARFEHDdo6hBkWc+AWDUV1P/1Rs4Rv4ANSzWyxkJITq6ZjuBv/jiC1566SUqKip48sknWbt2LREREYwcObLJaSLEuY0b1Jmf3TCQ7IJqnluyg+KKuib3s8b2ImDqQwRc9xvsQ6ZhHzwF+5CpGHUVOLd/1OzxXftX48naievr1d5KQQjhQ87aBzB8+HCGDx9OfX09q1evZuHChVRXV3P99dczZ86ctorRpwzrE80vbxnMyx/u4w9L9/DUnSOxWpquw5bYXlhiezXeNnQN995VaAMmYolKPG1fw+3E9fX/APBkbMVImY2invXtFUKY3HktCu/n58fkyZOZM2cONpuNxYsXezsun9anazh3T+/HydJavtx5/Lwf5xg6DRwBOLcsPeOSUnf6OnDWYB8yDaO+Ci1XBpQJIc7unAXg2ykhUlNTWbp0KbNnz2bDhg1tEZtPG9wjkn7dwlm+MZPqOvd5PUZxBOIYdgPa8QNoufsatxu6B9feVVhie2MfdgOKIwj3kTRvhS6E8BHNthH86U9/Yvny5QQEBHDDDTewbNkyoqKi2jI2n6YoCrOu7MXjb25l+YZM5lzT+7weZ+s7Adf+/+Hc8h5qVCJqQCieo1swakqxj7sdxWLF2mMU7kNfYbhqAVlgRgjRtLMWgM6dOxMbG8vmzZvZvHnzaff/9a9/9Xpwvi4+JojUwZ35ctdxJlzWhbjIwHM+RrFY8UuZRd1nL1PzrwewdhuKVpqHGhGPJaFh3IatVwruA1/gydwBXaZ4Ow0hRAfVbAFYuHBhW8ZhWjeMS2LLwQLeWX2YX9w8uNkO4VNZE4cS+IPncB1ci/vwBnDW4Jjwk8bJ+9SYHighnRqagcZKARBCNK3ZAnC26R42btzolWDMKCTQzs1X9OStzw7x6rKvuef6/udVBNSwOPxSZuMYcRN6aS5qdFLjfYqiYOuVgmvHMjyVxYAMJBNCnKnZb5qvv/6aWbNmMW/ePEpLSwE4ceIEP//5z/npT3/aZgGawRVDuzD76l7sOFzEnz7ah9ujnfdjFasdS0yPM5bptPUaDQoUffInDLeztUMWQviAZgvAE088wcSJE4mPj+cvf/kL//vf/7juuuuoq6tj2bJlbRmjKVwzPIHbru3D3owSXv73XupdFzcltBoSg1/qXdRlf03dqj9guJoedCaEMK9mm4CqqqqYO3cumqZx7bXX8umnn/Lkk08yderUtozPVK4Y2gWrReXNTw/y+2+miwgJsF/w8Wy9xxASHkzhf16iduWLBEx+AMVx7o5mIYQ5NHsG4O/vDzSsDOZ0Ovnb3/52QV/+zz//PI888ggAaWlpTJ8+nYkTJ8pgsmaMHRTHz2cMJK+ohoVLdlBcfnG/3IP6jcHvmnvRi7OoW/UShkeag4QQDZotAKeONA0PD6dfv34tPvimTZv4+OOPAaivr+fRRx/llVdeYeXKlezfv59169ZdQMi+b2ivaB6cNYTqOjfPvb2DihrXRR3P1m0YflfOQys8St3qP2NoDc1LWmEGNR8/SdWb86h6/SdUvXYXdZ/9nzQXCWESzRYAXdepqKigvLwcoPHf3/53LuXl5SxevJh58+YBsHfvXhITE0lISMBqtTJ9+nRWrVrVKkn4ol7xYTw0eyg19R7e+O/Bi16H2ZY0Ase4O9By91L/5d+o3/g2tf95BqO2HFufcdj6X4ktORVPzh5qVyxCr61opUyEEJeqZvsADh8+zOWXX974xfPtmsDQcJnhwYMHz3rg3/3udyxYsID8/HwACgsLiY6Obrw/JiaGgoKCiwre13XtFMwtV/bk7c8P878deVwzPOGijmdPTsWor8H1zRTTtv5X4hgxE8Xu37iPtesg6v73Z2qXPUPAlF+ihsq00kL4qmYLQHp6+gUf9IMPPiAuLo6UlBQ++qhh+mJd10+7VNEwjDMuXTwfkZFBFxxXdHTHmxbhBxOTOZxXyQdfZpAyuAvdO4e2+Bin5X3NLVR37ow1LBa/Lr2a2Hks9bGdOPn+czg/+wPxd/8B9ZQC0VF0xPe6NZgxbzPmDK2Tt1fmC165ciVFRUVcf/31VFRUUFtby/Hjx09bVKaoqIiYmJgWH7ukpBpdb3lzSHR0MEVFVS1+3KVgztU9OZRdyqJ/buPBWUMIa2ZJyaY0mXfMEACqmns97LE4rv45dcsXcnzF6/iNux1oKNrOLUvR8g/jf+0vUANaXozaQkd+ry+GGfM2Y87QsrxVVWn2h/N5TQfdUm+++SYrVqxg2bJl3HfffVx55ZW89tprZGZmkp2djaZprFixonGxeXF2IQF27p7ej+LyOh5/Yyt7M0q8/pzW2N7YBk7EffBLPHkNU0u7tn2Ie+8q9OJM6j59EcNZ4/U4hBDe45UC0BSHw8GiRYuYP38+U6ZMISkpiUmTJrXV03d4/bpF8Ls7RhAaaOelD/bw/pqjF3Qm1BKOETehhsVRv+4NnNs/xrV7BbbkVPwnPYBedoLaVYsx3PVejUEI4T2KcbGXl7QxMzYBncrl1nhvzVHW7jrOzCt6MOXyxLPuf7F5a4UZ1C57BgwDa9JI/K6ch6KquI9to/6LV7DE9sYv9U7UkJY15+k1Zbj2fYatzzgs4V0uOL6m+Mp73VJmzNuMOUPrNQHJmoEdjN1m4daJvamuc/PxV8fo1y2cbrEhXns+S0wPHClz0EtycYy9DUVtOGm0JY0A7W7q1/+Dmvd/ja3/1TiGTkfxO3snvWEYeI6kUZ/2L3DV4j60noApD2GJ7ua1HIQQTZMC0AEpisJt1/Yh43gFry4/wBN3jMBht5z7gRfIPuCaJrfbeo3G0rkvru0f4973Oe4DX6JGdcUSmdjw//AuqGFxYA/AqCxEK8zAnbEFLWcPlk69sA+7nvqv3qR2xfP4T34Aa2wTVyUJIbxGmoA6sIPZZbz47i7GD+nM7ZOSm9ynrfLWSnNxp69HL8lGK86GU/sGLHbQvhnNbPPDcdn12AZei6Kq6NUl1P739xg1pfhPWoC1c99zPpdhGDg3LgHVgt/oH55xvy++1+fDjHmbMWeQJiAB9E0MZ9LlXfl0cw6JscFcMaR129JbwhKRgGX0HAAMQ8eoKkYvO4FefgK9pgw1vAuWmCTU8C4o6ndnK2pQJAHTf03df5+n7rP/I2Dqw1hikpp7GgA8hzfgPrCm4Xljezc0RwkhWqzNrgIS3nHjuCQG9YhkyapDpO3Pb+9wAFAUFTUkBmviEOyDp+A3+ofY+16BJbLraV/+31IDQvGf8hCKXzC1n/4/tNLjzR5bryyiPu1fWOL6oEZ3x7nhLfS6Sm+mcxrDXU/N0kdwp3/VZs8phLdIAejgrBaVe28cQN9u4bz+34NsPdgxp9dQA8MJmPoQisVG3crfo5XmnbGPoevUr/07oOB3xd34pd6J4arDufHtVo/H0DU8efsbJ877lvvQBvSKk7j2fnrR8zMJ0d6kAPgAm9XC/BmD6NUllL8tP8BHX2VQW39xC8q0BzUkBv8pD4GuUfvhY9R/9QZ6dSmGrqOV5OLc9A7aycP4jfkRanAUloh47MOux3NsK659n6MVHkMrO4GnuhzD0C84Dq30OLXLnqFu5Yu4tn/UuN3QdVz7PgOLHb08H63gaGukLUS7kT4AH+GwW/jFzYN567NDrEjL5sudx5ma0o1ZzXQOX6osEV0IuPlZXLtW4D6wBveRTaBaGjuVrT1TsPYa3bi/ffAUPFk7cW56p3FbDoBiQQkMQ7H7NyyJ6a4H1YK162CsSSOwdO57RnOUoXtw7VmFa8d/UOz+WGJ7N4xVSB6PGhqLJ3sXRlURflfcRf3Gt3Gnr5Mrl0SHJlcB+aDsk1V8uC6D/ZmlJHUO5c6pycRFdryVwPSqIlx7VgEGlk49sXTqhRIcdcYkgobHhVaUCe46DFc9gTYPVQUn0WtKwVUHNr+GQlBfjSdnD3icKH7B2Ppfjb3/VSh+QXiOH8C58W308hNYk0bgGHMrGDo1Sx/BEteHgEkLqF32LHptOYG3PI9zw1u4j6YR9KOXUOwB7fMCfY+ZPuPfMmPO0HpXAUkB8GG7jhTxj08P4XR7mHN1b8YNirugGVg7mrO914bHhSdvH+70r9By9oDVgSW6G1r+IZTgaPxGz8GaOLRxf9feT3FuXor9sutx7VyGY/QPsQ+4Bq3wGLX/eQrH2Nux95tw2nPo9VW4tn2EUVuOGpOEJToJS6eeKLbzn8SvOYbHiWJt+jhm/IybMWeQy0DFeRjaK5ph/eN4/p/b+Men6RzKKeeOyX2wWb03aOxSp1jt2LoNw9ZtGFppHq49K9GOH8A+/EbsgyajWE9fg9nW/xrcB9fh2rkM7P7Yeo8FQI3ujhqRgDt9XWMBMAwDT8ZmnGnvYDhrUUKi8GTvajiQIxDH0Ouw9b8SxWJrcdyGqw7npndxH1qPbdC1OEbORFGb/vPV6ypRHEGNo7aFaI4UAB8XGerPL2cNYUVaFv9Zn8nJ0lp+PmMg4cEX/2u0o7NExOM/4Sdn3UexWHGM/iF1n/4/7H0nNC6eoygKtuRUnGlv49q/GqOmDE9+OnrhMdToJPyn/RhLRAKGswatMAPXvs9xbn4X1/7PsSWnNhxHtaKGxGDp0q/xzMzQtYaR1Uc3YenUE2viEBSbP/Ubl2DUlGLpnIx77yq0k4fxv+qnqMHfLbJkGDrO3Stxbfs3alQifuPnYok89yJCntx9YOhYuw6+iFezbbiPbcNw1WJPTm3vUHyCNAH5uFPz3nGoiNdWHMDPYeFnNwygV3xY+wbnJd54rz0nDmKJSTqt+cVw1lD99oKGUc6qBTWyK7Zeo7H1u6rJX9+evK9xbv0AvTjrtO2Wzn1xpMxB8Q+m/ou/ouWnY+ncF700D6O+IQ81NBa/K+7C0qkn7mNbqV/3JigN03FYE4eihndB3/wWdRm7sMQPQC/OxnDWYh88GfuQqaet+naqbyf1wzBwXD4b+6BrG+/Tq4ox6ipRo7ufV9Oh4XGi5R3A0nVQk+M9LpZeWUjNB4+C5sF/yoNY4wfI3/V5kD4ApAB8K7ewmj9+uJeSinquGhbPjNQk/Oy+dSLYlu+1VpwNuoYamXBeTTuGYYDHBZobQ3PjydqJa/vHGK4asAeAx43fuNux9R6DoevoRcfQKwqwJo04rXlKryzEuXlpw6/3b6bZUCw27CmzsfWdAM4a6je/h+fwBlAsWDr1wNKlP9bEIaiRXVEUBU/OXuo+/7+G5qyAMDyZ27EPnoJtwDW4di5vGOxmaFhie2MffuNZp+nQ6yqp++wl9MJjWBIG4X/1z1Bsft/l7a4Hq+OC+6AMw6Bu1WK0k4dRAsLAXU/gzGeISYijqKgKQ9cbOvfbcfU6w9DRCo6iOAJbfYbb75MC0EJSAL5T5/Tw0bpjfLEzj6hQP269tg8DkyLbKcLW19Hea8NZg3PHf9CKMvEbdweWiPjzf6zHiXb8AFrBUaKHTaDSEnXa/VphBp6sXXjy9qMXZwMGalgcloRBuA+sQQ3rTMC0h8Hmj3PjEtwHvwRFAVRsfVNRw+Jw7f4vRm15w7QbfcZh7T78tC9avbKQ2pX/D6OmFFuf8bgPrkGN6ob/tfdjVBbi3P1ftJzdqJ164hh2A5Yu/RtirziJ5/jXGK56UBQURQWbA8UegGIPQI3uhurfMNOtO3MH9av/iOPy2Vg696H246exdr+MhFm/4uTebTg3LkGvLsXviruxdR/W9GvldmLUVaAER7fqxRBaaS7uw2l4MrZg1JSComIfOh37Zdd55UwIpAC0+HEd7UuhtZwt78O55bz5aToFpbUM6RnFrKt6EhN+aVzSeDHkvW6aXleJJ3MHnowtaPmHUMPi8J/+SOOXrGEYuPd+il5RgH3I1MY1HgyPC/fBtbi+/gKjsgAsdixd+n7TCW2gnTyCYegEXHs/ltheeLJ3U/fFK6Co4K5HcQRh7TGqYRxFTSlqVCKGswajqvjsCVkd2Addi63fldR+/BSKI5CAGU+gqBacu1bg2vZv/BL6Up97sOHyYEcgenE29iHTsA+fgVFfiSdnD9qJdPTibPTyfMBAjUzE1v9KbD0vb/aKqm8ZHieGx9WQq8UGuobhaRhX4snZg/vwhobCqliwJAzA1mMUntx9eI5uQu3UE/8JP2nxWhnnQwpAC8mXQtPcHp3/bc9leVoWmqYzYWg8Vw+PJzqs4y0E/y15r89Nr61AsTlOa6Y5F8Mw0AszcB/eiHbycMNGRUHxC8Zv7G0NU39/Qys8hnPrB1gTh2BLvgLF5sDQ3LgPbcB9cA1KYCTWroOwxg9ECQgFDND1hi9cVy1GXRXur7/Ac2wrKBYwNPyv+03jwDtD16n77/PohcewDZ6CfchUAJxpb+NO/wolMByjpqwhxIAwLNHdUaMSUWz+uA+tRy/LA3sA1vgBWBMGYokfgBoYflq+7mPbqF/7Gniczb4mamQitj5jsfa8HNXvu0Xa3Uc3U7/hn6C5sQ+8FvuQaSh2/4YifGQjenUp9n5XnvaaGfXVeE4eQlEsDcXGams4K1ItKI7A0wqJFIAWki+FsyurcvLRVxls/roA3TAY0jOK6WO6eXWxGW+R99p3aEVZuHYuQw3vgmPkzNPuMzwuIkOslNae3uHuSl+HJ3MHlrjeWBMGo0bEn9bkYxgGWv4h3IfXo+Xux6irAMDa7TLsQ6ejRiXi2rkc147/oMb0wNbzctA8GJobVGtDX4zVhiW6O5bIrs3GrleX4tz6AZ6jm1D8glFjktBy94OhNYxuN3SsPS7H2m0onmPb8GTtAr35KVwCfvAclrDOgBSAFj/OF/84zkdL8y6rcrJmZx7rdp/A5dF44AdD6J0Q5r0AvUDea/O42JwNw0AvzcOTuQ3X/v+BqxYltBNGRQHWXmPwG3f7GWNDWkorysK5ZSl6xUmsPUZh6zMexS8I156VuL9eA5qroZmsVwq2pJFgsWJ8c6EAhg66DjY/LHF9GguZFIAWMuMfB1x43pU1Lp5/ZydlVU4enDWUpM4d50xA3mvzaM2cDVcdrgNf4E7/Cnu/CdgGTvL6yHm9rhK9JBdLXO8WDRBsrQIgQwVFk0IC7Tw4ayghAXb+sHQ3WSfbbs59IdqDYvfHMWQaQbNeaBgV3gbTpqj+IVjj+1/Q6PBWef52eVbRIYQHO3hw9hD8HBae+sd2nv7ndv67KYuCstr2Dk0I0QqkAIizigr157e3Deem1IZlGj9cd4xH/7aZ11ccoLi8rp2jE0JcDN8aAiq8IizIwdSUbkxN6UZpZT2rt+fyxY7jbD5QwIShXbhubHeC/NvnFFYIceGkAIgWiQjx45Yre3HN8AQ+Scvii515bD5QwIzxSYwf3BlV9f3ppoXwFVIAxAWJCPHj9knJXHlZPP9afZi3PjvEFzvzGNEnhgFJkXSLDZZiIMQlTgqAuCgJMUH8as5Qth4s5PNtuSzbkMl/NmQSEmjn+jHdGD+kMxaZl16IS5IUAHHRFEVhVL9OjOrXiapaF19nlbJu1wmWfH6YL3edYPZVPUlODDfFamRCdCRSAESrCg6wc3m/WEb17cSOQ0UsXXOU37+3m7AgO/26RTCgewSX9Y7GbjPvqmRCXCqkAAivUBSF4ckxDOoRyZYDBezPLGVvRglp+08SGmjn2pFduWJoZ59bi0CIjkT++oRX2W0Wxg3uzLjBndENg0M55axIy+L9L4+ycnM2fRPD6RYXTLfYEJLiQnDY5cxAiLYiBUC0GVVR6JsYTt/EcI4er+B/23PJOF7JtvRCACyqQo/OISQnhjO4ZxTdYoOl30AIL5ICINpFzy6h9OwSCkBVrYvM/CoO5ZRxILuMTzZmsXxjFlGhfgzvE0PqkM50iuj4C9UIcanxagH4v//7Pz777DMURWHmzJn8+Mc/Ji0tjYULF+J0Opk8eTILFizwZgiiAwgOsDOoRySDejQsS1ld52bXkSK2pxexensuq7fncuVl8Vw3thuBfjLiWIjW4rUCsHXrVjZv3szy5cvxeDxMmTKFlJQUHn30UZYsWUJcXBz33HMP69atIzU11VthiA4oyN/GuEGdGTeoMxXVTj5en8n/duSStj+fqSndpPNYiFbitRE6I0eO5K233sJqtVJSUoKmaVRWVpKYmEhCQgJWq5Xp06ezatUqb4UgfEBokIM7JifzxI9H0i02mPe/PMpDr6Txn/XHKK2sb+/whOjQvPozymaz8fLLL/PGG28wadIkCgsLiY6Obrw/JiaGgoICb4YgfERCTBC/nDWUjBMVrNyUzfJv+glCg+x0jw2ha6cgYsL9Gxa1t1qpqXfjsFmwWmQUshDN8fp59H333cfdd9/NvHnzyMrKOmNtzpZe5dHcyjbnIzo6+Nw7+SBfyjs6OpjLB8eTW1DFrkOFHMkt50huGXsyimlqbbvIUD9mXdOHa0Z2xWKCYuBL7/X5MmPO0Dp5e60AZGRk4HK56Nu3L/7+/kycOJFVq1ZhsXx3nXdRURExMTFnOcqZZEnIlvHVvP1USOkbQ0rfhs+P26NTXFFHYVkdmqJQXFqL062xL6OEP/97Dx+uOcIN47oztFcUNqtvjjXw1ff6bMyYM7TekpBeKwB5eXm8/PLLvPvuuwB88cUXzJo1ixdeeIHs7Gzi4+NZsWIFN910k7dCECZis6rERQYSFxl42h/HtJREdh8p5t/rMvjrsq/xs1sarzgKDXIQ4LASFuQgPNjRzhkI0fa8VgBSU1PZu3cvN9xwAxaLhYkTJzJ16lQiIiKYP38+TqeT1NRUJk2a5K0QhEBRFIb2jmZQz0gOZJWx41ARu44UsfVg4Wn7jR4Qy80TehIaaG/yOHVOD352iwxMEz5FMYymWk4vXdIE1DJmzPtcOeu6QX5JDTX1HmrrPRzJK+fzbbnYbZbGhW1s1ob+Ao+m899N2axIyyK5axi3TkomJsy/rVJpEXmvzaO1moCkAPg4M+Z9ITnnl9Tw9ueHOZhdRpC/jdEDYunXLZyP1h0jp7CaAUkRHM2rQNcNrh/Xnasui7/kZjSV99o8Lvk+ACE6krjIQB6cNYQDWWWs232cL3bk8fm2XEICbPx8xkAu6x1NaWU9/1p9mA++zOCjdceIjwmiR+cQLu8f2zithRAdiRQAIb6hKAr9u0fQv3sEFTUu0rPL6NctnOCAhn6BiBA/fj5jIOnZZRzMKSPjeCUb959kzc7jDOsTzU2pPYiNCMDt0SmtrG/45RXiJ0tjikuWFAAhmhAaaGdUv05nbFcUhb7dIujbLQIAp0vjs605fLolh91HigkOsFFR7eLbRkqrRSE6zJ9uscEMT45hQPfIxv4FIdqbFAAhLoLDbuG6sd1JHdqFVVuyqa5zExXqT1SoH5puUFBWS0FpHXszStj0dQH+Dgu948MIDrAT5G/DYlGoqnVTVevCYlG56rIu9Oka3t5pCZOQAiBEKwgNtHPLlb2avd+j6RzMLmPrwQKyT1aTU1hNdZ0bTTMIDrARHGCnvNrJ9vRCesaHMmlkV/p0DZPZT4VXSQEQog1YLSoDkyIZmBR52vZTp0NxujU27M3n0y3Z/OmjfQDEhPnTtVMQYcEOQgPthAU5iAn3p1N4AMEBNhmXIC6KFAAh2tGpX+AOm4WrhsWTOqQzh3PLycyvJOtkFbmF1ezLLMXp0k57bIDDSs/4UJK7hpOcGEZERGBbhy86OCkAQlxirBaVft0i6PdNR/O3nG6N8monhWV1nCyt5URxDYdyytmbUQKAn91C97gQkjqHNDYdGRgYRsPgN8MwCAt20DUmmC7RgTJTqpACIERH4bBZ6BQeQKfwgNOaksqqnBzKKeN4aR37jhaxcnN2kzOjnsqiKsRGBNApIoBOEf707BzKwB6RUhRMRgqAEB1ceLCDy/vHNo4OdXt0PJoOgKI0NDOpioKiQHFFPTkFVeQUVJNfUkN+SQ17jhbzqZ5DkL+Ny/t1YkTfGLrFhjRerlpcXseWgwUUV9QzoHskA5IicFxio6DFhZECIISPsVnVZscaxEYEEBsRwMi+341x8Gg6B7JK2bjvJGt3H+d/O/KwWlS6xQWDAUePVwANTUzrdp/AblUZ2COScYPiGNA9Uga6dWBSAIQwOatFZVCPKAb1iKKm3k16djlHj5dzNK8Ct6ZzU2oSo/p2IizYweHccnYeLmJbeiE7DhUREeLgsl7R1Ls0yqqd1NS5CQm0ExHsICTQTr1Lo6rWRZ1To3dCGCP7xhAR4ndGDG6Pxhc7jhMXGcDgnlHt8CqYk0wG5+PMmLcZc4a2zduj6ew+Usy6PSdIzy4jKMBGeJCDIH8blTUuSqucVNc1LMsZHGDDalE5WVqLAvTpGsbIfp0Y1jua4AA7R/LKeXNlOidLawEY3COS2df0Pq9ZV+W9PjeZDRT5oJiJGXOG9su7uaVdNV3Hon7XFFVQWsuWAwVsOlBAQWktqqLQLS6YzBOVRIT4ceu1vTlRXMuyjZlomsHw5Gi6xgQTHxNIZIgfdqsFm03F/k0Tl0VViY4OprCwEo+moyiKaTqxpQC0kHwpmIcZc4aOk7dhGOQWVrPlYAH7MkpJTgxjxvgk/OwNLdJlVU4+WpfBgewyyqqczR5HVRRUFTxaw/eB1aKSFBdM765h9EkIp1d86Fmn7NYNA/V7hcswDMqrXYQG2i/pvg0pAC3UUf44WpsZ8zZjzuCbeVfXucktrKaixonbrePy6Lg8Gh5Pw7/9/e24nG5sVpWqWjdH8srJPlmNbhhYLSp9EkJJTgwnOsyfyFA/bBaVfcdK2Hm4mOyTVfROCCWlfywDkiLZk1HM2l3HySmoJrlrGPdc15/QoEtzqVApAC3ki38c58OMeZsxZzBn3k3lXO/ycDi3ggNZpezPLOVEcc0Zj+seF0xSXCj7MksoLKtr3B4fHcSApAjW7MjDz2HlJ9P70SU6iMz8SnJOVhEW7KB/twgiQxs6sg3DoLLGhdOtYbWoWFSFAD8rNqt3L5OVBWGEEKIJfnYrg3pEMqhHw2C5OqeHkop6SirrqXV6SO4aTnhwwy97wzDIzK/iYHYpfbqG06NzCIqiMGZALK/8Zz8vvre7yefoFBGAzaJSVF6H062dcX9okJ2oUD/iIgPp2SWUXvGhxEYEnNfcTYZhkF1QRfbJKkb27YS/w3tf03IG4OPMmLcZcwZz5u3NnJ0ujdXbc7FZVbrHhdC1UxDFFfUcyCzlQHYZADHh/sSE+ePvsKLpBpqmU1XnpriinuLyOvKKaqiucwMQEmhncI9IhvSKonNUIFn5VWScqKC4vJ7gABthQQ50w2B7eiEF35yVRIX6cde0fvROCLvgvKUJCHP+cYA58zZjzmDOvC/1nA3D4GRpLUfyGpqk9h0roc753RmD3aoSHe5Pda2byloXGJCcGM6ofp2IDPHjrc/SKS6vZ0pKIjPGJzWeQUgTkBBCXOIURSEuMpC4yEDGD+6MR9M5lFtOUXkd3WNDTpuUT9N13B698WoogCd+PJL3vjjCfzdlM25QHDHhAa0anxQAIYRoI1aLSv/vzfL6LYuqYrGfPo7B32Hlx1P6MuuqXl7pCzDHqAkhhOjAvNURLAVACCFMSgqAEEKYlBQAIYQwKSkAQghhUlIAhBDCpKQACCGESXW4cQAXM0XrpTy9qzeZMW8z5gzmzNuMOcP55322/TrcVBBCCCFahzQBCSGESUkBEEIIk5ICIIQQJiUFQAghTEoKgBBCmJQUACGEMCkpAEIIYVJSAIQQwqSkAAghhEmZogB88sknTJkyhYkTJ/Kvf/2rvcPxmj/96U9MnTqVqVOn8sILLwCQlpbG9OnTmThxIosXL27nCL3n+eef55FHHgHMkfOaNWuYMWMGkydP5plnngF8P+9ly5Y1fr6ff/55wLdzrq6uZtq0aeTl5QHN53rw4EFmzJjBtddey29+8xs8Hs/5P4nh406ePGlMmDDBKCsrM2pqaozp06cbR44cae+wWt3GjRuNW265xXA6nYbL5TJuu+0245NPPjFSU1ONnJwcw+12G3PnzjXWrl3b3qG2urS0NGPUqFHGr371K6Ours7nc87JyTHGjh1r5OfnGy6Xy5g9e7axdu1an867trbWGDFihFFSUmK43W5j5syZxhdffOGzOe/evduYNm2a0b9/fyM3N/esn+upU6cau3btMgzDMH79618b//rXv877eXz+DCAtLY3LL7+csLAwAgICuPbaa1m1alV7h9XqoqOjeeSRR7Db7dhsNnr06EFWVhaJiYkkJCRgtVqZPn26z+VeXl7O4sWLmTdvHgB79+71+ZxXr17NlClTiI2NxWazsXjxYvz9/X06b03T0HWduro6PB4PHo+HoKAgn835/fff5/HHHycmJgZo/nN9/Phx6uvrGTJkCAAzZsxo0WvQ4WYDbanCwkKio6Mbb8fExLB37952jMg7evXq1fjvrKwsPv30U370ox+dkXtBQUF7hOc1v/vd71iwYAH5+flA0++3r+WcnZ2NzWZj3rx55Ofnc8UVV9CrVy+fzjsoKIhf/OIXTJ48GX9/f0aMGOHT7/Wzzz572u3mcv3+9ujo6Ba9Bj5/BqDrOory3XSohmGcdtvXHDlyhLlz5/Lwww+TkJDg07l/8MEHxMXFkZKS0rjNDO+3pmls2rSJ5557jqVLl7J3715yc3N9Ou/09HQ+/PBDvvzyS9avX4+qqmRlZfl0zqdq7nN9sZ93nz8DiI2NZfv27Y23i4qKGk+rfM2OHTu47777ePTRR5k6dSpbt26lqKio8X5fy33lypUUFRVx/fXXU1FRQW1tLcePH8disTTu42s5A0RFRZGSkkJERAQAV199NatWrfLpvDds2EBKSgqRkZFAQ1PH66+/7tM5nyo2NrbJv+Xvby8uLm7Ra+DzZwCjR49m06ZNlJaWUldXx+eff8748ePbO6xWl5+fz7333suLL77I1KlTARg8eDCZmZlkZ2ejaRorVqzwqdzffPNNVqxYwbJly7jvvvu48soree2113w6Z4AJEyawYcMGKisr0TSN9evXM2nSJJ/OOzk5mbS0NGprazEMgzVr1vj85/tUzeXapUsXHA4HO3bsABqulGrJa+DzZwCdOnViwYIF3HbbbbjdbmbOnMmgQYPaO6xW9/rrr+N0Olm0aFHjtlmzZrFo0SLmz5+P0+kkNTWVSZMmtWOU3udwOHw+58GDB3PXXXcxZ84c3G43Y8aMYfbs2SQlJfls3mPHjuXAgQPMmDEDm83GwIEDmT9/PmPGjPHZnE91ts/1iy++yG9/+1uqq6vp378/t91223kfV1YEE0IIk/L5JiAhhBBNkwIghBAmJQVACCFMSgqAEEKYlBQAIYQwKSkAQrShLVu2MG3atPYOQwhACoAQQpiWzw8EE6Il1qxZw1/+8hfcbjd+fn786le/YsOGDWRnZ3Py5EmKiopITk7m2WefJSgoiCNHjvDUU09RXl6OoijMnTuXG264AYB///vfvPnmm6iqSnh4eOMc9rW1tSxYsIBjx47hdDp55plnGD58eDtmLUyr9WawFqJjy8zMNKZNm2aUlpYahmEYhw8fNsaMGWMsWrTIGD9+vFFUVGRommY88MADxqJFiwy3221cddVVxmeffWYYRsPaE+PGjTN27txpHDx40Bg1apRx4sQJwzAM48033zQee+wxY/PmzUbfvn2N3bt3N26/7bbb2idhYXpyBiDENzZu3EhhYSF33HFH4zZFUcjJyWHSpElERUUBMHPmTJ577jluuukmnE4nEydOBBqmHZk4cSLr168nODiYsWPHEhcXB9B4zC1btpCQkMDgwYOBhjluPvzww7ZLUohTSAEQ4hu6rpOSksJLL73UuC0/P5+lS5ficrlO209VVTRNO2PqXcMw8Hg8WCyW0+6rr6/n+PHjANhstsbtiqJgyGwsop1IJ7AQ30hJSWHjxo1kZGQAsG7dOq677jqcTidffPEFVVVV6LrO+++/z4QJE0hKSsJqtfL5558DUFBQwGeffcbo0aMZNWoUmzZtorCwEID33nuP3//+9+2WmxBNkTMAIb7Rs2dPnnrqKR544AEMw8BqtfKXv/yFTZs2ERUVxd13301ZWRkjRoxg3rx52Gw2XnnlFZ555hn++Mc/omka9957L5dffjkADz30EHfddRfQsFLTc889R1ZWVjtmKMTpZDZQIc7hj3/8I2VlZfzud79r71CEaFXSBCSEECYlZwBCCGFScgYghBAmJQVACCFMSgqAEEKYlBQAIYQwKSkAQghhUlIAhBDCpP4/c3Idnlf0iooAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for loss\n",
    "plt.plot(np.sqrt(history.history['loss']))\n",
    "plt.plot(np.sqrt(history.history['val_loss']))\n",
    "plt.title('model loss')\n",
    "plt.ylabel('RMSE loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
