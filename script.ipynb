{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "RKP = \"DL031\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Bidirectional\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "sns.set_theme(style=\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\confusement\\miniconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3338: DtypeWarning: Columns (15) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['StationId', 'Datetime', 'PM2.5', 'PM10', 'NO', 'NO2', 'NOx', 'NH3',\n",
      "       'CO', 'SO2', 'O3', 'Benzene', 'Toluene', 'Xylene', 'AQI', 'AQI_Bucket'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Load datasets and rename columns, load all aqi data but specify metro data name\n",
    "def loadcsv(city=\"./data/rkpuram.csv\"):\n",
    "    met = pd.read_csv(city,delimiter=';',skiprows=24)\n",
    "    aqi = pd.read_csv('./data/station_hour.csv')\n",
    "    print(aqi.columns)\n",
    "    met.rename(columns={'# Date': 'Date',}, inplace=True)\n",
    "    met.rename(columns={'UT time': 'Time',}, inplace=True)\n",
    "    aqi['Time'] = aqi['Datetime'].str[-8:-3]\n",
    "    aqi['Date'] = aqi['Datetime'].str[0:10]\n",
    "    stations = [\"DL\"+str(x).zfill(3) for x in range(1,39)]\n",
    "    split_aqi = {}\n",
    "    for i in range(len(stations)):\n",
    "        split_aqi[stations[i]] = (aqi[aqi['StationId'] == stations[i]])\n",
    "    return met,aqi,split_aqi\n",
    "met,aqi,split_aqi = loadcsv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre - processing and loading data\n",
    "class dataset:\n",
    "    def __init__(self,met,aqi,split_aqi):\n",
    "            self.metro_data = met\n",
    "            self.aqi_data = aqi\n",
    "            self.split_aqi = split_aqi\n",
    "    def mergedData(self,station,rlist=['PM2.5','PM10','NO','NO2','NOx','NH3','CO','SO2','O3','AQI'],roll=48,shift=72):\n",
    "        df_aqi = self.getdf(station)\n",
    "        df = pd.merge(df_aqi, self.metro_data, how='inner', on=['Date', 'Time'])\n",
    "        print(\"Merged Dataset Size\",len(df))\n",
    "        \n",
    "        #Pre Processing merged Data\n",
    "        df['Year'] = df['Date'].str[0:4]\n",
    "        df['Month'] = df['Date'].str[5:7].astype(np.float64)\n",
    "        df['Day'] = df['Date'].str[8:10].astype(np.float64)\n",
    "        df['Hour'] = df['Time'].str[0:2]\n",
    "        \n",
    "        # TRIG TRANSFORMATIONS\n",
    "        df['windX'] = np.cos(np.deg2rad(df['Wind direction'])) * df['Wind speed']\n",
    "        df['windY'] = np.sin(np.deg2rad(df['Wind direction'])) * df['Wind speed']\n",
    "        df['hourX'] = np.cos((df['Hour'].astype(np.float64)-1)*np.pi/24)\n",
    "        df['hourY'] = np.sin((df['Hour'].astype(np.float64)-1)*np.pi/24)\n",
    "        df['MonthX'] = np.cos((df['Month'].astype(np.float64)-1)*np.pi/12)\n",
    "        df['MonthY'] = np.sin((df['Month'].astype(np.float64)-1)*np.pi/12)\n",
    "        \n",
    "        import datetime\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "        df['isWeekend'] =  (df['Date'].dt.dayofweek>=5).astype(int)\n",
    "        \n",
    "        df.interpolate(method='linear', limit=5,inplace=True)\n",
    "        \n",
    "        # Drop Additional columns\n",
    "        df.drop('Benzene', axis=1, inplace=True)\n",
    "        df.drop('Toluene',axis=1, inplace=True)\n",
    "        df.drop('Xylene', axis=1,inplace=True)\n",
    "        df.drop('AQI_Bucket',axis=1,inplace=True)\n",
    "        df.drop('Datetime',axis=1,inplace=True)\n",
    "        df.drop('StationId',axis=1,inplace=True)\n",
    "        df.drop('Short-wave irradiation',axis=1,inplace=True)\n",
    "        df.drop('Date',axis=1,inplace=True)\n",
    "        df.drop('Time',axis=1,inplace=True)\n",
    "        \n",
    "        # Rolling and shifting \n",
    "        print(\"Size before roll\",len(df))\n",
    "        rollList = ['PM2.5','PM10','NO','NO2','CO','AQI','Temperature','Relative Humidity','windX','windY','Year','MonthX','MonthY','hourX','hourY','isWeekend']\n",
    "        for i in rollList:\n",
    "            df[i+'_lagroll1'] = df[i].rolling(window=24, min_periods=12).mean().shift(6)\n",
    "            df[i+'_lagroll2'] = df[i].rolling(window=24, min_periods=12).mean().shift(12)\n",
    "            df[i+'_lagroll3'] = df[i].rolling(window=24, min_periods=12).mean().shift(18)\n",
    "            df[i+'_lagroll4'] = df[i].rolling(window=24, min_periods=12).mean().shift(24)\n",
    "            \n",
    "        for i in rlist:\n",
    "            df[i+'_lag1'] = df[i].shift(24)\n",
    "            df[i+'_lag2'] = df[i].shift(48)\n",
    "            df[i+'_lag3'] = df[i].shift(72)\n",
    "        for i in rlist:\n",
    "            df[i+\"_pred1\"] = df[i].shift(-24)\n",
    "            df[i+\"_pred2\"] = df[i].shift(-48)\n",
    "            df[i+\"_pred3\"] = df[i].shift(-72)\n",
    "        newlist = rlist + ['Temperature','Relative Humidity','windX','windY']\n",
    "        for i in newlist:\n",
    "            for j in range(24):\n",
    "                df[i+\"_t-\"+str(j)] = df[i].shift(j)\n",
    "                df[i+\"_t+\"+str(j)] = df[i].shift(-j-shift)\n",
    "        futurelist = ['Year','MonthX','MonthY','hourX','hourY','isWeekend']\n",
    "        for i in futurelist:\n",
    "            for j in range(24):\n",
    "                df[i+\"_t-\"+str(j)] = df[i].shift(-(shift+23-j))\n",
    "        df.dropna(inplace=True)\n",
    "        print(\"Size after roll\",len(df))\n",
    "        \n",
    "        return df.copy()\n",
    "    def getdf(self,station):\n",
    "        return self.split_aqi[station]\n",
    "    def plot(self,station):\n",
    "        df = self.getdf(station)\n",
    "    def stats(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Model Testing as well\n",
    "def getSplitFeaturesTimeSeries(df,TIME_SERIES_LENGTH = 24):\n",
    "    features = []\n",
    "    rlist=['PM2.5','PM10','NO','NO2','CO','AQI']\n",
    "    for it in rlist:\n",
    "        print(it,np.mean(df[it]),np.std(df[it]))\n",
    "    newlist = rlist + ['Temperature','Relative Humidity','windX','windY','Year','MonthX','MonthY','hourX','hourY','isWeekend']\n",
    "    for j in range(24):\n",
    "        for i in newlist:\n",
    "            features.append(i+'_t-'+str(j))\n",
    "    predVector = []\n",
    "    for j in range(24):\n",
    "        predVector.append('PM2.5_t+'+str(j))\n",
    "    X = df[features]\n",
    "    y = df[predVector]\n",
    "    X = np.array(X).reshape(X.shape[0],TIME_SERIES_LENGTH,len(newlist))\n",
    "    scaler = StandardScaler()\n",
    "    Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "    Xtrain = scaler.fit_transform(Xtrain.reshape(Xtrain.shape[0],TIME_SERIES_LENGTH*len(newlist)))\n",
    "    Xtrain = Xtrain.reshape(Xtrain.shape[0],TIME_SERIES_LENGTH,len(newlist))\n",
    "    Xtest = scaler.transform(Xtest.reshape(Xtest.shape[0],TIME_SERIES_LENGTH*len(newlist)))\n",
    "    Xtest = Xtest.reshape(Xtest.shape[0],TIME_SERIES_LENGTH,len(newlist))\n",
    "    return Xtrain,ytrain,Xtest,ytest\n",
    "def getSplitFeatures(df):\n",
    "    features = []\n",
    "    rlist=['PM2.5','PM10','NO','NO2','CO','AQI']\n",
    "    for it in rlist:\n",
    "        print(it,np.mean(df[it]),np.std(df[it]))\n",
    "    newlist = rlist + ['Temperature','Relative Humidity','windX','windY','Year','MonthX','MonthY','hourX','hourY','isWeekend']\n",
    "    for j in range(1,5):\n",
    "        for i in newlist:\n",
    "            features.append(i+'_lagroll'+str(j))\n",
    "    print(\"features length\",len(newlist))\n",
    "    predVector = ['PM2.5_t+0']\n",
    "    X = df[features]\n",
    "    y = df[predVector]\n",
    "    X = np.array(X)\n",
    "    scaler = StandardScaler()\n",
    "    Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "    Xtrain = scaler.fit_transform(Xtrain)\n",
    "    Xtest = scaler.transform(Xtest)\n",
    "    return Xtrain,ytrain,Xtest,ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def trainModel1(Xtrain,ytrain,Xtest,ytest,TIME_SERIES_LENGTH=24):\n",
    "    model = Sequential()\n",
    "    # model.add(LSTM(50, activation='relu', return_sequences=True, input_shape=(TIME_SERIES_LENGTH,len(newlist))))\n",
    "    model.add(Conv1D(128, 3,activation='relu',input_shape=(TIME_SERIES_LENGTH,len(newlist))))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv1D(128, 6,activation='relu',input_shape=(TIME_SERIES_LENGTH,len(newlist))))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv1D(128, 6,activation='relu',input_shape=(TIME_SERIES_LENGTH,len(newlist))))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # model.add(LSTM(200,activation='relu',input_shape=(TIME_SERIES_LENGTH,len(newlist))))\n",
    "    # model.add(Bidirectional(LSTM(50, activation='relu'), input_shape=(TIME_SERIES_LENGTH,len(newlist))))\n",
    "    # model.add(Dense(200, activation='relu'))\n",
    "    \n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.summary()\n",
    "    #Fit\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['mse','mae'])\n",
    "    history = model.fit(Xtrain, ytrain, epochs=200, batch_size=256,  verbose=1, validation_split=0.2)\n",
    "    return model,history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictStats(model,Xtrain,ytrain,Xtest,ytest):\n",
    "    testPred = model.predict(Xtest)\n",
    "    trainPred = model.predict(Xtrain)\n",
    "    print(mean_squared_error(testPred, ytest,squared=False))\n",
    "    print(mean_squared_error(trainPred, ytrain,squared=False))\n",
    "    print(mean_absolute_error(testPred, ytest))\n",
    "    print(mean_absolute_error(trainPred, ytrain))\n",
    "def plothistory(history):\n",
    "    # list all data in history\n",
    "    print(history.history.keys())\n",
    "    # summarize history for loss\n",
    "    plt.plot(np.sqrt(history.history['loss']))\n",
    "    plt.plot(np.sqrt(history.history['val_loss']))\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('RMSE loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Below for time series models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged Dataset Size 44035\n",
      "Size before roll 44035\n",
      "Size after roll 12746\n",
      "PM2.5 101.98476789313264 82.24685447030713\n",
      "PM10 215.97489333970944 141.24129293818078\n",
      "NO 43.57018346193662 83.25087922077859\n",
      "NO2 58.654915716109564 40.60985358453989\n",
      "CO 1.5801772332177046 2.3063151059462434\n",
      "AQI 224.31361799780322 111.71448247715317\n",
      "Model: \"sequential_73\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_120 (Conv1D)          (None, 22, 128)           6272      \n",
      "_________________________________________________________________\n",
      "batch_normalization_102 (Bat (None, 22, 128)           512       \n",
      "_________________________________________________________________\n",
      "conv1d_121 (Conv1D)          (None, 17, 128)           98432     \n",
      "_________________________________________________________________\n",
      "batch_normalization_103 (Bat (None, 17, 128)           512       \n",
      "_________________________________________________________________\n",
      "conv1d_122 (Conv1D)          (None, 12, 128)           98432     \n",
      "_________________________________________________________________\n",
      "batch_normalization_104 (Bat (None, 12, 128)           512       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 6, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_44 (Flatten)         (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_97 (Dense)             (None, 24)                18456     \n",
      "=================================================================\n",
      "Total params: 223,128\n",
      "Trainable params: 222,360\n",
      "Non-trainable params: 768\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "27/27 [==============================] - 5s 167ms/step - loss: 15475.2080 - mse: 15475.2080 - mae: 96.5657 - val_loss: 15664.5332 - val_mse: 15664.5332 - val_mae: 97.1525\n",
      "Epoch 2/200\n",
      "27/27 [==============================] - 3s 126ms/step - loss: 11904.3447 - mse: 11904.3447 - mae: 83.4027 - val_loss: 14270.1348 - val_mse: 14270.1348 - val_mae: 92.5142\n",
      "Epoch 3/200\n",
      "27/27 [==============================] - 4s 154ms/step - loss: 8607.7744 - mse: 8607.7744 - mae: 68.8132 - val_loss: 10571.7949 - val_mse: 10571.7949 - val_mae: 77.0014\n",
      "Epoch 4/200\n",
      "27/27 [==============================] - 4s 140ms/step - loss: 6396.7285 - mse: 6396.7285 - mae: 58.2689 - val_loss: 8055.4976 - val_mse: 8055.4976 - val_mae: 64.7412\n",
      "Epoch 5/200\n",
      "27/27 [==============================] - 4s 147ms/step - loss: 5006.3486 - mse: 5006.3486 - mae: 51.2950 - val_loss: 6761.2524 - val_mse: 6761.2524 - val_mae: 57.2991\n",
      "Epoch 6/200\n",
      "27/27 [==============================] - 5s 182ms/step - loss: 4246.0459 - mse: 4246.0459 - mae: 47.1296 - val_loss: 5472.5806 - val_mse: 5472.5806 - val_mae: 51.0329\n",
      "Epoch 7/200\n",
      "27/27 [==============================] - 4s 158ms/step - loss: 3930.6287 - mse: 3930.6287 - mae: 45.3781 - val_loss: 4917.7358 - val_mse: 4917.7358 - val_mae: 47.5266\n",
      "Epoch 8/200\n",
      "27/27 [==============================] - 4s 155ms/step - loss: 3680.1343 - mse: 3680.1343 - mae: 43.3145 - val_loss: 4969.3745 - val_mse: 4969.3745 - val_mae: 46.7039\n",
      "Epoch 9/200\n",
      "27/27 [==============================] - 4s 165ms/step - loss: 3508.5295 - mse: 3508.5295 - mae: 41.9294 - val_loss: 4377.2056 - val_mse: 4377.2056 - val_mae: 44.4367\n",
      "Epoch 10/200\n",
      "27/27 [==============================] - 5s 196ms/step - loss: 3431.2549 - mse: 3431.2549 - mae: 41.0533 - val_loss: 4203.9629 - val_mse: 4203.9629 - val_mae: 43.8315\n",
      "Epoch 11/200\n",
      "27/27 [==============================] - 4s 149ms/step - loss: 3278.4905 - mse: 3278.4905 - mae: 39.9676 - val_loss: 4019.9976 - val_mse: 4019.9976 - val_mae: 43.0897\n",
      "Epoch 12/200\n",
      "27/27 [==============================] - 5s 169ms/step - loss: 3181.4360 - mse: 3181.4360 - mae: 39.0087 - val_loss: 3882.8015 - val_mse: 3882.8015 - val_mae: 42.1839\n",
      "Epoch 13/200\n",
      "27/27 [==============================] - 5s 181ms/step - loss: 3079.5984 - mse: 3079.5984 - mae: 38.2852 - val_loss: 3572.4368 - val_mse: 3572.4368 - val_mae: 39.7964\n",
      "Epoch 14/200\n",
      "27/27 [==============================] - 4s 160ms/step - loss: 2991.6035 - mse: 2991.6035 - mae: 37.5712 - val_loss: 3625.2300 - val_mse: 3625.2300 - val_mae: 39.6280\n",
      "Epoch 15/200\n",
      "27/27 [==============================] - 3s 128ms/step - loss: 2883.0527 - mse: 2883.0527 - mae: 36.4426 - val_loss: 3239.9590 - val_mse: 3239.9590 - val_mae: 37.6193\n",
      "Epoch 16/200\n",
      "27/27 [==============================] - 4s 156ms/step - loss: 2766.9595 - mse: 2766.9595 - mae: 35.5180 - val_loss: 3166.8311 - val_mse: 3166.8311 - val_mae: 36.7727\n",
      "Epoch 17/200\n",
      "27/27 [==============================] - 4s 165ms/step - loss: 2674.6729 - mse: 2674.6729 - mae: 34.8550 - val_loss: 3331.5408 - val_mse: 3331.5408 - val_mae: 37.5355\n",
      "Epoch 18/200\n",
      "27/27 [==============================] - ETA: 0s - loss: 2604.7158 - mse: 2604.7158 - mae: 34.34 - 4s 143ms/step - loss: 2604.7158 - mse: 2604.7158 - mae: 34.3432 - val_loss: 3040.6824 - val_mse: 3040.6824 - val_mae: 36.2914\n",
      "Epoch 19/200\n",
      "27/27 [==============================] - 4s 164ms/step - loss: 2525.7173 - mse: 2525.7173 - mae: 33.6644 - val_loss: 2905.2849 - val_mse: 2905.2849 - val_mae: 34.6221\n",
      "Epoch 20/200\n",
      "27/27 [==============================] - 5s 181ms/step - loss: 2139.0010 - mse: 2139.0010 - mae: 31.5227 - val_loss: 2102.5505 - val_mse: 2102.5505 - val_mae: 30.8188\n",
      "Epoch 21/200\n",
      "27/27 [==============================] - 4s 135ms/step - loss: 1770.0696 - mse: 1770.0696 - mae: 29.2506 - val_loss: 1866.3037 - val_mse: 1866.3037 - val_mae: 29.6211\n",
      "Epoch 22/200\n",
      "27/27 [==============================] - 4s 157ms/step - loss: 1621.8470 - mse: 1621.8470 - mae: 28.1002 - val_loss: 1832.9508 - val_mse: 1832.9508 - val_mae: 29.8814\n",
      "Epoch 23/200\n",
      "27/27 [==============================] - 4s 137ms/step - loss: 1508.4620 - mse: 1508.4620 - mae: 27.0238 - val_loss: 1650.6647 - val_mse: 1650.6647 - val_mae: 27.7680\n",
      "Epoch 24/200\n",
      "27/27 [==============================] - 4s 154ms/step - loss: 1418.8536 - mse: 1418.8536 - mae: 26.2644 - val_loss: 1594.7377 - val_mse: 1594.7377 - val_mae: 27.6972\n",
      "Epoch 25/200\n",
      "27/27 [==============================] - 4s 135ms/step - loss: 1357.8425 - mse: 1357.8425 - mae: 25.6886 - val_loss: 1534.1823 - val_mse: 1534.1823 - val_mae: 26.5729\n",
      "Epoch 26/200\n",
      "27/27 [==============================] - 5s 175ms/step - loss: 1364.7548 - mse: 1364.7548 - mae: 26.0402 - val_loss: 1481.4358 - val_mse: 1481.4358 - val_mae: 26.6403\n",
      "Epoch 27/200\n",
      "27/27 [==============================] - 4s 164ms/step - loss: 1285.1484 - mse: 1285.1484 - mae: 25.2441 - val_loss: 1418.5417 - val_mse: 1418.5417 - val_mae: 25.8702\n",
      "Epoch 28/200\n",
      "27/27 [==============================] - 5s 173ms/step - loss: 1193.2615 - mse: 1193.2615 - mae: 24.2072 - val_loss: 1364.9155 - val_mse: 1364.9155 - val_mae: 25.3083\n",
      "Epoch 29/200\n",
      "27/27 [==============================] - 4s 150ms/step - loss: 1168.4800 - mse: 1168.4800 - mae: 24.0263 - val_loss: 1305.8478 - val_mse: 1305.8478 - val_mae: 24.9037\n",
      "Epoch 30/200\n",
      "27/27 [==============================] - 4s 141ms/step - loss: 1139.8158 - mse: 1139.8158 - mae: 23.7785 - val_loss: 1257.6110 - val_mse: 1257.6110 - val_mae: 24.3097\n",
      "Epoch 31/200\n",
      "27/27 [==============================] - 4s 140ms/step - loss: 1076.2375 - mse: 1076.2375 - mae: 23.0053 - val_loss: 1132.1781 - val_mse: 1132.1781 - val_mae: 23.3084\n",
      "Epoch 32/200\n",
      "27/27 [==============================] - 4s 142ms/step - loss: 1009.5089 - mse: 1009.5089 - mae: 22.2429 - val_loss: 1120.5372 - val_mse: 1120.5372 - val_mae: 22.9740\n",
      "Epoch 33/200\n",
      "27/27 [==============================] - 4s 131ms/step - loss: 992.0463 - mse: 992.0463 - mae: 22.0028 - val_loss: 1083.6843 - val_mse: 1083.6843 - val_mae: 22.6636\n",
      "Epoch 34/200\n",
      "27/27 [==============================] - 4s 137ms/step - loss: 945.1286 - mse: 945.1286 - mae: 21.5979 - val_loss: 1128.1049 - val_mse: 1128.1049 - val_mae: 23.3268\n",
      "Epoch 35/200\n",
      "27/27 [==============================] - 4s 150ms/step - loss: 921.5793 - mse: 921.5793 - mae: 21.2891 - val_loss: 987.8195 - val_mse: 987.8195 - val_mae: 21.9124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/200\n",
      "27/27 [==============================] - 4s 148ms/step - loss: 880.7792 - mse: 880.7792 - mae: 20.8292 - val_loss: 982.2567 - val_mse: 982.2567 - val_mae: 21.6437\n",
      "Epoch 37/200\n",
      "27/27 [==============================] - 4s 150ms/step - loss: 863.8290 - mse: 863.8290 - mae: 20.6099 - val_loss: 979.3203 - val_mse: 979.3203 - val_mae: 21.3911\n",
      "Epoch 38/200\n",
      "27/27 [==============================] - 4s 163ms/step - loss: 843.4086 - mse: 843.4086 - mae: 20.4171 - val_loss: 950.5804 - val_mse: 950.5804 - val_mae: 21.3109\n",
      "Epoch 39/200\n",
      "27/27 [==============================] - 4s 140ms/step - loss: 833.3428 - mse: 833.3428 - mae: 20.2106 - val_loss: 896.8322 - val_mse: 896.8322 - val_mae: 20.8596\n",
      "Epoch 40/200\n",
      "27/27 [==============================] - 5s 167ms/step - loss: 802.4127 - mse: 802.4127 - mae: 19.8442 - val_loss: 856.5181 - val_mse: 856.5181 - val_mae: 20.2160\n",
      "Epoch 41/200\n",
      "27/27 [==============================] - 4s 150ms/step - loss: 778.1418 - mse: 778.1418 - mae: 19.5359 - val_loss: 834.0771 - val_mse: 834.0771 - val_mae: 19.9774\n",
      "Epoch 42/200\n",
      "27/27 [==============================] - 4s 162ms/step - loss: 753.5624 - mse: 753.5624 - mae: 19.2032 - val_loss: 842.0168 - val_mse: 842.0168 - val_mae: 20.1063\n",
      "Epoch 43/200\n",
      "27/27 [==============================] - 4s 140ms/step - loss: 723.9656 - mse: 723.9656 - mae: 18.8331 - val_loss: 893.1791 - val_mse: 893.1791 - val_mae: 20.6210\n",
      "Epoch 44/200\n",
      "27/27 [==============================] - 4s 132ms/step - loss: 708.1762 - mse: 708.1762 - mae: 18.5632 - val_loss: 786.3923 - val_mse: 786.3923 - val_mae: 19.2121\n",
      "Epoch 45/200\n",
      "27/27 [==============================] - 4s 146ms/step - loss: 688.0831 - mse: 688.0831 - mae: 18.2965 - val_loss: 758.8941 - val_mse: 758.8941 - val_mae: 18.9904\n",
      "Epoch 46/200\n",
      "27/27 [==============================] - 4s 141ms/step - loss: 689.1792 - mse: 689.1792 - mae: 18.3106 - val_loss: 752.8557 - val_mse: 752.8557 - val_mae: 19.1999\n",
      "Epoch 47/200\n",
      "27/27 [==============================] - 4s 149ms/step - loss: 677.8339 - mse: 677.8339 - mae: 18.2845 - val_loss: 742.3658 - val_mse: 742.3658 - val_mae: 19.1525\n",
      "Epoch 48/200\n",
      "27/27 [==============================] - 6s 206ms/step - loss: 637.3806 - mse: 637.3806 - mae: 17.5145 - val_loss: 690.9339 - val_mse: 690.9339 - val_mae: 18.1071\n",
      "Epoch 49/200\n",
      "27/27 [==============================] - 5s 198ms/step - loss: 619.0276 - mse: 619.0276 - mae: 17.3354 - val_loss: 705.8071 - val_mse: 705.8071 - val_mae: 18.1278\n",
      "Epoch 50/200\n",
      "27/27 [==============================] - 5s 170ms/step - loss: 610.5708 - mse: 610.5708 - mae: 17.2876 - val_loss: 721.8142 - val_mse: 721.8142 - val_mae: 18.4406\n",
      "Epoch 51/200\n",
      "27/27 [==============================] - 4s 161ms/step - loss: 600.8263 - mse: 600.8263 - mae: 17.0840 - val_loss: 645.8880 - val_mse: 645.8880 - val_mae: 17.4766\n",
      "Epoch 52/200\n",
      "27/27 [==============================] - 5s 176ms/step - loss: 561.6250 - mse: 561.6250 - mae: 16.4698 - val_loss: 655.8779 - val_mse: 655.8779 - val_mae: 17.7203\n",
      "Epoch 53/200\n",
      "27/27 [==============================] - 4s 145ms/step - loss: 566.5646 - mse: 566.5646 - mae: 16.6283 - val_loss: 632.8257 - val_mse: 632.8257 - val_mae: 17.2673\n",
      "Epoch 54/200\n",
      "27/27 [==============================] - 5s 186ms/step - loss: 548.2381 - mse: 548.2381 - mae: 16.2347 - val_loss: 611.1100 - val_mse: 611.1100 - val_mae: 16.9620\n",
      "Epoch 55/200\n",
      "27/27 [==============================] - 4s 151ms/step - loss: 524.4561 - mse: 524.4561 - mae: 15.8630 - val_loss: 575.5126 - val_mse: 575.5126 - val_mae: 16.3309\n",
      "Epoch 56/200\n",
      "27/27 [==============================] - 4s 162ms/step - loss: 538.3971 - mse: 538.3971 - mae: 16.2686 - val_loss: 710.0015 - val_mse: 710.0015 - val_mae: 18.3182\n",
      "Epoch 57/200\n",
      "27/27 [==============================] - 4s 157ms/step - loss: 519.9539 - mse: 519.9539 - mae: 15.9098 - val_loss: 577.6761 - val_mse: 577.6761 - val_mae: 16.4449\n",
      "Epoch 58/200\n",
      "27/27 [==============================] - 4s 153ms/step - loss: 505.6070 - mse: 505.6070 - mae: 15.6235 - val_loss: 581.4065 - val_mse: 581.4065 - val_mae: 16.2417\n",
      "Epoch 59/200\n",
      "27/27 [==============================] - 4s 141ms/step - loss: 492.5786 - mse: 492.5786 - mae: 15.4162 - val_loss: 576.7591 - val_mse: 576.7591 - val_mae: 16.5021\n",
      "Epoch 60/200\n",
      "27/27 [==============================] - 4s 165ms/step - loss: 524.9650 - mse: 524.9650 - mae: 16.0681 - val_loss: 597.9719 - val_mse: 597.9719 - val_mae: 16.9311\n",
      "Epoch 61/200\n",
      "27/27 [==============================] - 4s 159ms/step - loss: 501.5965 - mse: 501.5965 - mae: 15.5881 - val_loss: 555.1439 - val_mse: 555.1439 - val_mae: 16.0889\n",
      "Epoch 62/200\n",
      "27/27 [==============================] - 4s 156ms/step - loss: 486.4939 - mse: 486.4939 - mae: 15.3183 - val_loss: 566.4128 - val_mse: 566.4128 - val_mae: 16.3119\n",
      "Epoch 63/200\n",
      "27/27 [==============================] - 5s 168ms/step - loss: 467.9230 - mse: 467.9230 - mae: 15.0928 - val_loss: 555.2726 - val_mse: 555.2726 - val_mae: 15.9966\n",
      "Epoch 64/200\n",
      "27/27 [==============================] - 4s 150ms/step - loss: 478.1817 - mse: 478.1817 - mae: 15.2341 - val_loss: 519.1027 - val_mse: 519.1027 - val_mae: 15.5331\n",
      "Epoch 65/200\n",
      "27/27 [==============================] - 4s 139ms/step - loss: 449.1838 - mse: 449.1838 - mae: 14.6917 - val_loss: 530.8604 - val_mse: 530.8604 - val_mae: 15.5562\n",
      "Epoch 66/200\n",
      "27/27 [==============================] - 4s 151ms/step - loss: 445.8975 - mse: 445.8975 - mae: 14.6617 - val_loss: 512.2902 - val_mse: 512.2902 - val_mae: 15.3319\n",
      "Epoch 67/200\n",
      "27/27 [==============================] - 4s 156ms/step - loss: 446.0979 - mse: 446.0979 - mae: 14.7124 - val_loss: 545.6800 - val_mse: 545.6800 - val_mae: 15.7311\n",
      "Epoch 68/200\n",
      "27/27 [==============================] - 4s 163ms/step - loss: 444.6292 - mse: 444.6292 - mae: 14.6997 - val_loss: 495.7715 - val_mse: 495.7715 - val_mae: 15.1190\n",
      "Epoch 69/200\n",
      "27/27 [==============================] - 4s 155ms/step - loss: 419.0541 - mse: 419.0541 - mae: 14.2144 - val_loss: 515.9350 - val_mse: 515.9350 - val_mae: 15.5634\n",
      "Epoch 70/200\n",
      "27/27 [==============================] - 4s 154ms/step - loss: 417.9462 - mse: 417.9462 - mae: 14.2183 - val_loss: 478.2035 - val_mse: 478.2035 - val_mae: 14.7947\n",
      "Epoch 71/200\n",
      "27/27 [==============================] - 5s 189ms/step - loss: 412.6720 - mse: 412.6720 - mae: 14.1135 - val_loss: 475.8677 - val_mse: 475.8677 - val_mae: 14.7681\n",
      "Epoch 72/200\n",
      "27/27 [==============================] - 5s 178ms/step - loss: 397.0761 - mse: 397.0761 - mae: 13.8550 - val_loss: 484.1255 - val_mse: 484.1255 - val_mae: 15.0104\n",
      "Epoch 73/200\n",
      "27/27 [==============================] - 4s 152ms/step - loss: 401.9846 - mse: 401.9846 - mae: 14.0130 - val_loss: 460.8350 - val_mse: 460.8350 - val_mae: 14.5798\n",
      "Epoch 74/200\n",
      "27/27 [==============================] - 4s 149ms/step - loss: 404.9969 - mse: 404.9969 - mae: 14.0358 - val_loss: 476.2367 - val_mse: 476.2367 - val_mae: 15.1168\n",
      "Epoch 75/200\n",
      "27/27 [==============================] - 4s 146ms/step - loss: 392.6084 - mse: 392.6084 - mae: 13.8066 - val_loss: 456.7184 - val_mse: 456.7184 - val_mae: 14.4959\n",
      "Epoch 76/200\n",
      "27/27 [==============================] - 4s 143ms/step - loss: 387.8797 - mse: 387.8797 - mae: 13.7994 - val_loss: 449.7579 - val_mse: 449.7579 - val_mae: 14.2894\n",
      "Epoch 77/200\n",
      "27/27 [==============================] - 4s 132ms/step - loss: 385.8210 - mse: 385.8210 - mae: 13.6981 - val_loss: 447.9966 - val_mse: 447.9966 - val_mae: 14.2251\n",
      "Epoch 78/200\n",
      "27/27 [==============================] - 3s 127ms/step - loss: 387.3994 - mse: 387.3994 - mae: 13.6863 - val_loss: 469.8116 - val_mse: 469.8116 - val_mae: 15.1614\n",
      "Epoch 79/200\n",
      "27/27 [==============================] - 4s 152ms/step - loss: 356.6699 - mse: 356.6699 - mae: 13.1408 - val_loss: 432.4195 - val_mse: 432.4195 - val_mae: 14.0215\n",
      "Epoch 80/200\n",
      "27/27 [==============================] - 4s 132ms/step - loss: 356.6697 - mse: 356.6697 - mae: 13.1610 - val_loss: 433.6340 - val_mse: 433.6340 - val_mae: 14.0932\n",
      "Epoch 81/200\n",
      "27/27 [==============================] - 4s 153ms/step - loss: 354.6950 - mse: 354.6950 - mae: 13.1320 - val_loss: 436.5185 - val_mse: 436.5185 - val_mae: 14.0980\n",
      "Epoch 82/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 4s 157ms/step - loss: 367.7048 - mse: 367.7048 - mae: 13.3939 - val_loss: 473.2642 - val_mse: 473.2642 - val_mae: 14.9900\n",
      "Epoch 83/200\n",
      "27/27 [==============================] - 4s 158ms/step - loss: 377.4418 - mse: 377.4418 - mae: 13.6981 - val_loss: 435.4294 - val_mse: 435.4294 - val_mae: 14.2308\n",
      "Epoch 84/200\n",
      "27/27 [==============================] - 4s 153ms/step - loss: 351.9258 - mse: 351.9258 - mae: 13.0754 - val_loss: 429.5078 - val_mse: 429.5078 - val_mae: 14.0446\n",
      "Epoch 85/200\n",
      "27/27 [==============================] - 4s 160ms/step - loss: 352.3631 - mse: 352.3631 - mae: 13.0969 - val_loss: 462.4547 - val_mse: 462.4547 - val_mae: 14.6569\n",
      "Epoch 86/200\n",
      "27/27 [==============================] - 4s 164ms/step - loss: 356.8940 - mse: 356.8940 - mae: 13.2316 - val_loss: 415.7231 - val_mse: 415.7231 - val_mae: 13.7158\n",
      "Epoch 87/200\n",
      "27/27 [==============================] - 4s 152ms/step - loss: 343.2411 - mse: 343.2411 - mae: 12.9281 - val_loss: 424.7916 - val_mse: 424.7916 - val_mae: 14.2530\n",
      "Epoch 88/200\n",
      "27/27 [==============================] - 4s 149ms/step - loss: 344.1549 - mse: 344.1549 - mae: 12.9508 - val_loss: 420.9579 - val_mse: 420.9579 - val_mae: 13.9155\n",
      "Epoch 89/200\n",
      "27/27 [==============================] - 4s 156ms/step - loss: 333.5592 - mse: 333.5592 - mae: 12.7835 - val_loss: 411.4493 - val_mse: 411.4493 - val_mae: 13.7489\n",
      "Epoch 90/200\n",
      "27/27 [==============================] - 4s 157ms/step - loss: 339.7448 - mse: 339.7448 - mae: 12.8891 - val_loss: 447.7230 - val_mse: 447.7230 - val_mae: 14.4589\n",
      "Epoch 91/200\n",
      "27/27 [==============================] - 4s 138ms/step - loss: 360.6375 - mse: 360.6375 - mae: 13.3969 - val_loss: 417.2785 - val_mse: 417.2785 - val_mae: 13.7239\n",
      "Epoch 92/200\n",
      "27/27 [==============================] - 4s 155ms/step - loss: 323.5403 - mse: 323.5403 - mae: 12.5698 - val_loss: 397.8730 - val_mse: 397.8730 - val_mae: 13.4883\n",
      "Epoch 93/200\n",
      "27/27 [==============================] - 4s 157ms/step - loss: 310.4355 - mse: 310.4355 - mae: 12.3039 - val_loss: 384.8555 - val_mse: 384.8555 - val_mae: 13.2255\n",
      "Epoch 94/200\n",
      "27/27 [==============================] - 4s 149ms/step - loss: 324.0793 - mse: 324.0793 - mae: 12.5604 - val_loss: 393.3972 - val_mse: 393.3972 - val_mae: 13.3422\n",
      "Epoch 95/200\n",
      "27/27 [==============================] - 4s 160ms/step - loss: 313.8613 - mse: 313.8613 - mae: 12.3780 - val_loss: 384.0162 - val_mse: 384.0162 - val_mae: 13.2504\n",
      "Epoch 96/200\n",
      "27/27 [==============================] - 4s 163ms/step - loss: 310.9669 - mse: 310.9669 - mae: 12.4054 - val_loss: 389.2767 - val_mse: 389.2767 - val_mae: 13.3841\n",
      "Epoch 97/200\n",
      "27/27 [==============================] - 4s 147ms/step - loss: 318.1255 - mse: 318.1255 - mae: 12.5102 - val_loss: 381.2939 - val_mse: 381.2939 - val_mae: 13.0989\n",
      "Epoch 98/200\n",
      "27/27 [==============================] - 4s 165ms/step - loss: 298.5150 - mse: 298.5150 - mae: 12.0642 - val_loss: 392.5324 - val_mse: 392.5324 - val_mae: 13.4102\n",
      "Epoch 99/200\n",
      "27/27 [==============================] - 5s 171ms/step - loss: 316.4339 - mse: 316.4339 - mae: 12.5192 - val_loss: 375.7434 - val_mse: 375.7434 - val_mae: 13.1139\n",
      "Epoch 100/200\n",
      "27/27 [==============================] - 4s 155ms/step - loss: 318.0135 - mse: 318.0135 - mae: 12.5853 - val_loss: 375.0974 - val_mse: 375.0974 - val_mae: 13.0472\n",
      "Epoch 101/200\n",
      "27/27 [==============================] - 4s 154ms/step - loss: 315.0852 - mse: 315.0852 - mae: 12.5479 - val_loss: 374.5831 - val_mse: 374.5831 - val_mae: 13.0920\n",
      "Epoch 102/200\n",
      "27/27 [==============================] - 5s 173ms/step - loss: 317.6450 - mse: 317.6450 - mae: 12.4801 - val_loss: 382.1151 - val_mse: 382.1151 - val_mae: 13.3955\n",
      "Epoch 103/200\n",
      "27/27 [==============================] - 4s 162ms/step - loss: 282.3970 - mse: 282.3970 - mae: 11.8241 - val_loss: 370.3454 - val_mse: 370.3454 - val_mae: 12.9646\n",
      "Epoch 104/200\n",
      "27/27 [==============================] - 4s 140ms/step - loss: 269.1871 - mse: 269.1871 - mae: 11.4448 - val_loss: 345.2337 - val_mse: 345.2337 - val_mae: 12.4637\n",
      "Epoch 105/200\n",
      "27/27 [==============================] - 4s 156ms/step - loss: 279.4581 - mse: 279.4581 - mae: 11.6973 - val_loss: 345.4007 - val_mse: 345.4007 - val_mae: 12.5346\n",
      "Epoch 106/200\n",
      "27/27 [==============================] - 5s 169ms/step - loss: 300.8916 - mse: 300.8916 - mae: 12.2569 - val_loss: 350.8494 - val_mse: 350.8494 - val_mae: 12.6463\n",
      "Epoch 107/200\n",
      "27/27 [==============================] - 4s 145ms/step - loss: 277.2350 - mse: 277.2350 - mae: 11.7255 - val_loss: 356.8059 - val_mse: 356.8059 - val_mae: 12.6675\n",
      "Epoch 108/200\n",
      "27/27 [==============================] - 4s 159ms/step - loss: 277.7381 - mse: 277.7381 - mae: 11.7361 - val_loss: 357.7123 - val_mse: 357.7123 - val_mae: 12.7423\n",
      "Epoch 109/200\n",
      "27/27 [==============================] - 4s 160ms/step - loss: 272.6006 - mse: 272.6006 - mae: 11.6441 - val_loss: 342.9520 - val_mse: 342.9520 - val_mae: 12.5055\n",
      "Epoch 110/200\n",
      "27/27 [==============================] - 5s 173ms/step - loss: 271.9233 - mse: 271.9233 - mae: 11.6234 - val_loss: 349.3890 - val_mse: 349.3890 - val_mae: 12.6660\n",
      "Epoch 111/200\n",
      "27/27 [==============================] - 4s 154ms/step - loss: 263.6262 - mse: 263.6262 - mae: 11.4314 - val_loss: 343.0306 - val_mse: 343.0306 - val_mae: 12.3809\n",
      "Epoch 112/200\n",
      "27/27 [==============================] - 4s 148ms/step - loss: 261.9896 - mse: 261.9896 - mae: 11.3673 - val_loss: 348.4127 - val_mse: 348.4127 - val_mae: 12.5851\n",
      "Epoch 113/200\n",
      "27/27 [==============================] - 4s 152ms/step - loss: 282.6750 - mse: 282.6750 - mae: 11.8832 - val_loss: 369.8611 - val_mse: 369.8611 - val_mae: 13.3380\n",
      "Epoch 114/200\n",
      "27/27 [==============================] - 4s 153ms/step - loss: 269.4739 - mse: 269.4739 - mae: 11.5810 - val_loss: 351.5259 - val_mse: 351.5259 - val_mae: 12.8395\n",
      "Epoch 115/200\n",
      "27/27 [==============================] - 4s 151ms/step - loss: 251.1959 - mse: 251.1959 - mae: 11.1594 - val_loss: 347.5410 - val_mse: 347.5410 - val_mae: 12.4640\n",
      "Epoch 116/200\n",
      "27/27 [==============================] - 4s 151ms/step - loss: 247.6846 - mse: 247.6846 - mae: 11.0882 - val_loss: 341.8206 - val_mse: 341.8206 - val_mae: 12.5204\n",
      "Epoch 117/200\n",
      "27/27 [==============================] - 4s 136ms/step - loss: 251.9909 - mse: 251.9909 - mae: 11.2537 - val_loss: 336.3805 - val_mse: 336.3805 - val_mae: 12.2368\n",
      "Epoch 118/200\n",
      "27/27 [==============================] - 4s 150ms/step - loss: 256.8297 - mse: 256.8297 - mae: 11.3292 - val_loss: 335.3883 - val_mse: 335.3883 - val_mae: 12.3140\n",
      "Epoch 119/200\n",
      "27/27 [==============================] - 4s 155ms/step - loss: 247.7785 - mse: 247.7785 - mae: 11.0965 - val_loss: 370.0510 - val_mse: 370.0510 - val_mae: 13.0647\n",
      "Epoch 120/200\n",
      "27/27 [==============================] - 4s 157ms/step - loss: 252.0008 - mse: 252.0008 - mae: 11.2481 - val_loss: 351.7747 - val_mse: 351.7747 - val_mae: 12.6667\n",
      "Epoch 121/200\n",
      "27/27 [==============================] - 4s 167ms/step - loss: 249.6714 - mse: 249.6714 - mae: 11.1794 - val_loss: 344.9506 - val_mse: 344.9506 - val_mae: 12.5859\n",
      "Epoch 122/200\n",
      "27/27 [==============================] - 5s 176ms/step - loss: 254.6812 - mse: 254.6812 - mae: 11.3215 - val_loss: 324.6009 - val_mse: 324.6009 - val_mae: 12.0627\n",
      "Epoch 123/200\n",
      "27/27 [==============================] - 4s 165ms/step - loss: 261.6513 - mse: 261.6513 - mae: 11.5811 - val_loss: 369.7980 - val_mse: 369.7980 - val_mae: 13.1491\n",
      "Epoch 124/200\n",
      "27/27 [==============================] - ETA: 0s - loss: 241.3527 - mse: 241.3527 - mae: 11.04 - 4s 141ms/step - loss: 241.3527 - mse: 241.3527 - mae: 11.0400 - val_loss: 329.1702 - val_mse: 329.1702 - val_mae: 12.2311\n",
      "Epoch 125/200\n",
      "27/27 [==============================] - 5s 192ms/step - loss: 239.3541 - mse: 239.3541 - mae: 10.9284 - val_loss: 321.8004 - val_mse: 321.8004 - val_mae: 12.0132\n",
      "Epoch 126/200\n",
      "27/27 [==============================] - 4s 157ms/step - loss: 249.5266 - mse: 249.5266 - mae: 11.2132 - val_loss: 319.0805 - val_mse: 319.0805 - val_mae: 11.9894\n",
      "Epoch 127/200\n",
      "27/27 [==============================] - 4s 162ms/step - loss: 243.0984 - mse: 243.0984 - mae: 11.0780 - val_loss: 354.0424 - val_mse: 354.0424 - val_mae: 12.7975\n",
      "Epoch 128/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 4s 149ms/step - loss: 245.2249 - mse: 245.2249 - mae: 11.0689 - val_loss: 361.5123 - val_mse: 361.5123 - val_mae: 12.9751\n",
      "Epoch 129/200\n",
      "27/27 [==============================] - 4s 132ms/step - loss: 248.0695 - mse: 248.0695 - mae: 11.2813 - val_loss: 320.7560 - val_mse: 320.7560 - val_mae: 12.0607\n",
      "Epoch 130/200\n",
      "27/27 [==============================] - 4s 148ms/step - loss: 227.0330 - mse: 227.0330 - mae: 10.7006 - val_loss: 314.9121 - val_mse: 314.9121 - val_mae: 11.7872\n",
      "Epoch 131/200\n",
      "27/27 [==============================] - 4s 154ms/step - loss: 221.0694 - mse: 221.0694 - mae: 10.5510 - val_loss: 311.3075 - val_mse: 311.3075 - val_mae: 11.8349\n",
      "Epoch 132/200\n",
      "27/27 [==============================] - 4s 149ms/step - loss: 236.9973 - mse: 236.9973 - mae: 10.9363 - val_loss: 320.4540 - val_mse: 320.4540 - val_mae: 12.1801\n",
      "Epoch 133/200\n",
      "27/27 [==============================] - 3s 128ms/step - loss: 230.7816 - mse: 230.7816 - mae: 10.8315 - val_loss: 345.3766 - val_mse: 345.3766 - val_mae: 12.8290\n",
      "Epoch 134/200\n",
      "27/27 [==============================] - 4s 145ms/step - loss: 223.5123 - mse: 223.5123 - mae: 10.7020 - val_loss: 318.0458 - val_mse: 318.0458 - val_mae: 12.1022\n",
      "Epoch 135/200\n",
      "27/27 [==============================] - 4s 158ms/step - loss: 212.8997 - mse: 212.8997 - mae: 10.3691 - val_loss: 313.6707 - val_mse: 313.6707 - val_mae: 11.9622\n",
      "Epoch 136/200\n",
      "27/27 [==============================] - 4s 153ms/step - loss: 225.4698 - mse: 225.4698 - mae: 10.7049 - val_loss: 323.1505 - val_mse: 323.1505 - val_mae: 12.3427\n",
      "Epoch 137/200\n",
      "27/27 [==============================] - 4s 164ms/step - loss: 222.8332 - mse: 222.8332 - mae: 10.6411 - val_loss: 304.0872 - val_mse: 304.0872 - val_mae: 11.6810\n",
      "Epoch 138/200\n",
      "27/27 [==============================] - 5s 184ms/step - loss: 215.2636 - mse: 215.2636 - mae: 10.4200 - val_loss: 312.1732 - val_mse: 312.1732 - val_mae: 11.9869\n",
      "Epoch 139/200\n",
      "27/27 [==============================] - 5s 175ms/step - loss: 225.8186 - mse: 225.8186 - mae: 10.6868 - val_loss: 381.8200 - val_mse: 381.8200 - val_mae: 14.1301\n",
      "Epoch 140/200\n",
      "27/27 [==============================] - 4s 145ms/step - loss: 215.4786 - mse: 215.4786 - mae: 10.5138 - val_loss: 307.7355 - val_mse: 307.7355 - val_mae: 11.8080\n",
      "Epoch 141/200\n",
      "27/27 [==============================] - 4s 154ms/step - loss: 219.9748 - mse: 219.9748 - mae: 10.6236 - val_loss: 295.7585 - val_mse: 295.7585 - val_mae: 11.5067\n",
      "Epoch 142/200\n",
      "27/27 [==============================] - 4s 165ms/step - loss: 215.0679 - mse: 215.0679 - mae: 10.4661 - val_loss: 322.6460 - val_mse: 322.6460 - val_mae: 12.1737\n",
      "Epoch 143/200\n",
      "27/27 [==============================] - 5s 171ms/step - loss: 216.0853 - mse: 216.0853 - mae: 10.4982 - val_loss: 303.9418 - val_mse: 303.9418 - val_mae: 11.7415\n",
      "Epoch 144/200\n",
      "27/27 [==============================] - 4s 162ms/step - loss: 208.9895 - mse: 208.9895 - mae: 10.2655 - val_loss: 305.9913 - val_mse: 305.9913 - val_mae: 11.7408\n",
      "Epoch 145/200\n",
      "27/27 [==============================] - 4s 144ms/step - loss: 194.8307 - mse: 194.8307 - mae: 9.9525 - val_loss: 286.7914 - val_mse: 286.7914 - val_mae: 11.3532\n",
      "Epoch 146/200\n",
      "27/27 [==============================] - 5s 168ms/step - loss: 204.8693 - mse: 204.8693 - mae: 10.2305 - val_loss: 309.0413 - val_mse: 309.0413 - val_mae: 11.9691\n",
      "Epoch 147/200\n",
      "27/27 [==============================] - 4s 165ms/step - loss: 205.1451 - mse: 205.1451 - mae: 10.2921 - val_loss: 291.0529 - val_mse: 291.0529 - val_mae: 11.4275\n",
      "Epoch 148/200\n",
      "27/27 [==============================] - 4s 156ms/step - loss: 194.0711 - mse: 194.0711 - mae: 9.9710 - val_loss: 295.0074 - val_mse: 295.0074 - val_mae: 11.6151\n",
      "Epoch 149/200\n",
      "27/27 [==============================] - 4s 144ms/step - loss: 209.2839 - mse: 209.2839 - mae: 10.3173 - val_loss: 297.5526 - val_mse: 297.5526 - val_mae: 11.5666\n",
      "Epoch 150/200\n",
      "27/27 [==============================] - 4s 159ms/step - loss: 194.8598 - mse: 194.8598 - mae: 9.9762 - val_loss: 299.8560 - val_mse: 299.8560 - val_mae: 11.6226\n",
      "Epoch 151/200\n",
      "27/27 [==============================] - 4s 166ms/step - loss: 189.5322 - mse: 189.5322 - mae: 9.8722 - val_loss: 282.7162 - val_mse: 282.7162 - val_mae: 11.2840\n",
      "Epoch 152/200\n",
      "27/27 [==============================] - 4s 163ms/step - loss: 193.1214 - mse: 193.1214 - mae: 9.9668 - val_loss: 282.8596 - val_mse: 282.8596 - val_mae: 11.2853\n",
      "Epoch 153/200\n",
      "27/27 [==============================] - 4s 150ms/step - loss: 182.8262 - mse: 182.8262 - mae: 9.6953 - val_loss: 281.7307 - val_mse: 281.7307 - val_mae: 11.2301\n",
      "Epoch 154/200\n",
      "27/27 [==============================] - 4s 141ms/step - loss: 191.3555 - mse: 191.3555 - mae: 9.8902 - val_loss: 304.5750 - val_mse: 304.5750 - val_mae: 11.7103\n",
      "Epoch 155/200\n",
      "27/27 [==============================] - 4s 154ms/step - loss: 195.3913 - mse: 195.3913 - mae: 10.0792 - val_loss: 293.7834 - val_mse: 293.7834 - val_mae: 11.6385\n",
      "Epoch 156/200\n",
      "27/27 [==============================] - 4s 149ms/step - loss: 182.0788 - mse: 182.0788 - mae: 9.6885 - val_loss: 282.2303 - val_mse: 282.2303 - val_mae: 11.2044\n",
      "Epoch 157/200\n",
      "27/27 [==============================] - 4s 156ms/step - loss: 197.2956 - mse: 197.2956 - mae: 10.1246 - val_loss: 285.5401 - val_mse: 285.5401 - val_mae: 11.3272\n",
      "Epoch 158/200\n",
      "27/27 [==============================] - 4s 156ms/step - loss: 185.2986 - mse: 185.2986 - mae: 9.8269 - val_loss: 293.9046 - val_mse: 293.9046 - val_mae: 11.4949\n",
      "Epoch 159/200\n",
      "27/27 [==============================] - 4s 138ms/step - loss: 182.4580 - mse: 182.4580 - mae: 9.6964 - val_loss: 280.8381 - val_mse: 280.8381 - val_mae: 11.2185\n",
      "Epoch 160/200\n",
      "27/27 [==============================] - 5s 172ms/step - loss: 182.2168 - mse: 182.2168 - mae: 9.7120 - val_loss: 294.4282 - val_mse: 294.4282 - val_mae: 11.4891\n",
      "Epoch 161/200\n",
      "27/27 [==============================] - 4s 156ms/step - loss: 173.5151 - mse: 173.5151 - mae: 9.4807 - val_loss: 274.2889 - val_mse: 274.2889 - val_mae: 11.1060\n",
      "Epoch 162/200\n",
      "27/27 [==============================] - 4s 141ms/step - loss: 185.5699 - mse: 185.5699 - mae: 9.8374 - val_loss: 273.1726 - val_mse: 273.1726 - val_mae: 11.0365\n",
      "Epoch 163/200\n",
      "27/27 [==============================] - 4s 142ms/step - loss: 176.9794 - mse: 176.9794 - mae: 9.6172 - val_loss: 275.0752 - val_mse: 275.0752 - val_mae: 11.0614\n",
      "Epoch 164/200\n",
      "27/27 [==============================] - 4s 153ms/step - loss: 184.5655 - mse: 184.5655 - mae: 9.7452 - val_loss: 292.7056 - val_mse: 292.7056 - val_mae: 11.4060\n",
      "Epoch 165/200\n",
      "27/27 [==============================] - 4s 148ms/step - loss: 188.0854 - mse: 188.0854 - mae: 9.8917 - val_loss: 295.2330 - val_mse: 295.2330 - val_mae: 11.8550\n",
      "Epoch 166/200\n",
      "27/27 [==============================] - 4s 148ms/step - loss: 177.9208 - mse: 177.9208 - mae: 9.6350 - val_loss: 290.2396 - val_mse: 290.2396 - val_mae: 11.4807\n",
      "Epoch 167/200\n",
      "27/27 [==============================] - 4s 162ms/step - loss: 168.0405 - mse: 168.0405 - mae: 9.3548 - val_loss: 275.7290 - val_mse: 275.7290 - val_mae: 11.2136\n",
      "Epoch 168/200\n",
      "27/27 [==============================] - 4s 142ms/step - loss: 166.8118 - mse: 166.8118 - mae: 9.3259 - val_loss: 278.4691 - val_mse: 278.4691 - val_mae: 11.2851\n",
      "Epoch 169/200\n",
      "27/27 [==============================] - 4s 161ms/step - loss: 166.7054 - mse: 166.7054 - mae: 9.3552 - val_loss: 296.0012 - val_mse: 296.0012 - val_mae: 11.7442\n",
      "Epoch 170/200\n",
      "27/27 [==============================] - 4s 159ms/step - loss: 189.8478 - mse: 189.8478 - mae: 10.0285 - val_loss: 288.9564 - val_mse: 288.9564 - val_mae: 11.5065\n",
      "Epoch 171/200\n",
      "27/27 [==============================] - 4s 151ms/step - loss: 184.2272 - mse: 184.2272 - mae: 9.8991 - val_loss: 286.2985 - val_mse: 286.2985 - val_mae: 11.5885\n",
      "Epoch 172/200\n",
      "27/27 [==============================] - 4s 144ms/step - loss: 183.2010 - mse: 183.2010 - mae: 9.8879 - val_loss: 276.8813 - val_mse: 276.8813 - val_mae: 11.2424\n",
      "Epoch 173/200\n",
      "27/27 [==============================] - 4s 154ms/step - loss: 178.8813 - mse: 178.8813 - mae: 9.6930 - val_loss: 274.5569 - val_mse: 274.5569 - val_mae: 11.0590\n",
      "Epoch 174/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 4s 157ms/step - loss: 176.5504 - mse: 176.5504 - mae: 9.6564 - val_loss: 267.7699 - val_mse: 267.7699 - val_mae: 11.0184\n",
      "Epoch 175/200\n",
      "27/27 [==============================] - 4s 156ms/step - loss: 168.1906 - mse: 168.1906 - mae: 9.4416 - val_loss: 265.7704 - val_mse: 265.7704 - val_mae: 11.0047\n",
      "Epoch 176/200\n",
      "27/27 [==============================] - 4s 149ms/step - loss: 180.7781 - mse: 180.7781 - mae: 9.8089 - val_loss: 302.8242 - val_mse: 302.8242 - val_mae: 11.6544\n",
      "Epoch 177/200\n",
      "27/27 [==============================] - 4s 137ms/step - loss: 172.9539 - mse: 172.9539 - mae: 9.6079 - val_loss: 288.0204 - val_mse: 288.0204 - val_mae: 11.6655\n",
      "Epoch 178/200\n",
      "27/27 [==============================] - 4s 151ms/step - loss: 167.6932 - mse: 167.6932 - mae: 9.4524 - val_loss: 275.0854 - val_mse: 275.0854 - val_mae: 11.1013\n",
      "Epoch 179/200\n",
      "27/27 [==============================] - 4s 151ms/step - loss: 171.4711 - mse: 171.4711 - mae: 9.4560 - val_loss: 274.3524 - val_mse: 274.3524 - val_mae: 11.2121\n",
      "Epoch 180/200\n",
      "27/27 [==============================] - 5s 170ms/step - loss: 167.6633 - mse: 167.6633 - mae: 9.4720 - val_loss: 270.3302 - val_mse: 270.3302 - val_mae: 11.0543\n",
      "Epoch 181/200\n",
      "27/27 [==============================] - 4s 157ms/step - loss: 157.2128 - mse: 157.2128 - mae: 9.1204 - val_loss: 302.3518 - val_mse: 302.3518 - val_mae: 12.2358\n",
      "Epoch 182/200\n",
      "27/27 [==============================] - 4s 136ms/step - loss: 163.3769 - mse: 163.3769 - mae: 9.3287 - val_loss: 275.2382 - val_mse: 275.2382 - val_mae: 11.2744\n",
      "Epoch 183/200\n",
      "27/27 [==============================] - 4s 167ms/step - loss: 145.1538 - mse: 145.1538 - mae: 8.7495 - val_loss: 255.3947 - val_mse: 255.3947 - val_mae: 10.6092\n",
      "Epoch 184/200\n",
      "27/27 [==============================] - 4s 151ms/step - loss: 144.3735 - mse: 144.3735 - mae: 8.7515 - val_loss: 256.7866 - val_mse: 256.7866 - val_mae: 10.6366\n",
      "Epoch 185/200\n",
      "27/27 [==============================] - 4s 158ms/step - loss: 159.9374 - mse: 159.9374 - mae: 9.1887 - val_loss: 296.0151 - val_mse: 296.0151 - val_mae: 11.7262\n",
      "Epoch 186/200\n",
      "27/27 [==============================] - 4s 135ms/step - loss: 163.9639 - mse: 163.9639 - mae: 9.3635 - val_loss: 264.1482 - val_mse: 264.1482 - val_mae: 10.9460\n",
      "Epoch 187/200\n",
      "27/27 [==============================] - 4s 160ms/step - loss: 150.6534 - mse: 150.6534 - mae: 8.9627 - val_loss: 270.3321 - val_mse: 270.3321 - val_mae: 11.1634\n",
      "Epoch 188/200\n",
      "27/27 [==============================] - 4s 160ms/step - loss: 161.7470 - mse: 161.7470 - mae: 9.2700 - val_loss: 285.2870 - val_mse: 285.2870 - val_mae: 11.4309\n",
      "Epoch 189/200\n",
      "27/27 [==============================] - 4s 154ms/step - loss: 162.2466 - mse: 162.2466 - mae: 9.3055 - val_loss: 264.2257 - val_mse: 264.2257 - val_mae: 10.9447\n",
      "Epoch 190/200\n",
      "27/27 [==============================] - 5s 197ms/step - loss: 152.8687 - mse: 152.8687 - mae: 9.0605 - val_loss: 259.1421 - val_mse: 259.1421 - val_mae: 10.7657\n",
      "Epoch 191/200\n",
      "27/27 [==============================] - 4s 157ms/step - loss: 158.2316 - mse: 158.2316 - mae: 9.1576 - val_loss: 292.0620 - val_mse: 292.0620 - val_mae: 11.9115\n",
      "Epoch 192/200\n",
      "27/27 [==============================] - 4s 156ms/step - loss: 149.9368 - mse: 149.9368 - mae: 9.0276 - val_loss: 270.3530 - val_mse: 270.3530 - val_mae: 11.1602\n",
      "Epoch 193/200\n",
      "27/27 [==============================] - 4s 137ms/step - loss: 158.0600 - mse: 158.0600 - mae: 9.2175 - val_loss: 258.5163 - val_mse: 258.5163 - val_mae: 10.8565\n",
      "Epoch 194/200\n",
      "27/27 [==============================] - 4s 150ms/step - loss: 149.1583 - mse: 149.1583 - mae: 8.9597 - val_loss: 263.6533 - val_mse: 263.6533 - val_mae: 10.9781\n",
      "Epoch 195/200\n",
      "27/27 [==============================] - 5s 174ms/step - loss: 149.1876 - mse: 149.1876 - mae: 8.8894 - val_loss: 323.9206 - val_mse: 323.9206 - val_mae: 12.8194\n",
      "Epoch 196/200\n",
      "27/27 [==============================] - 5s 168ms/step - loss: 154.8353 - mse: 154.8353 - mae: 9.1291 - val_loss: 256.8094 - val_mse: 256.8094 - val_mae: 10.8482\n",
      "Epoch 197/200\n",
      "27/27 [==============================] - 5s 185ms/step - loss: 145.3028 - mse: 145.3028 - mae: 8.8770 - val_loss: 263.0362 - val_mse: 263.0362 - val_mae: 10.9442\n",
      "Epoch 198/200\n",
      "27/27 [==============================] - 5s 190ms/step - loss: 142.5171 - mse: 142.5171 - mae: 8.7682 - val_loss: 249.8185 - val_mse: 249.8185 - val_mae: 10.6330\n",
      "Epoch 199/200\n",
      "27/27 [==============================] - 5s 167ms/step - loss: 154.5707 - mse: 154.5707 - mae: 9.1719 - val_loss: 272.7729 - val_mse: 272.7729 - val_mae: 11.2486\n",
      "Epoch 200/200\n",
      "27/27 [==============================] - 4s 167ms/step - loss: 149.0736 - mse: 149.0736 - mae: 8.9797 - val_loss: 261.7426 - val_mse: 261.7426 - val_mae: 10.8444\n",
      "INFO:tensorflow:Assets written to: 2daypm1_366\\assets\n",
      "16.050441970060152\n",
      "12.242668241745667\n",
      "10.818789923469074\n",
      "8.617433853286531\n",
      "dict_keys(['loss', 'mse', 'mae', 'val_loss', 'val_mse', 'val_mae'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dat = dataset(met,aqi,split_aqi)\n",
    "df = dat.mergedData('DL031',roll=48,shift=48)\n",
    "Xtrain,ytrain,Xtest,ytest = getSplitFeaturesTimeSeries(df,TIME_SERIES_LENGTH = 24)\n",
    "model,history = trainModel1(Xtrain,ytrain,Xtest,ytest,TIME_SERIES_LENGTH=24)\n",
    "model.save('2daypm1_366')\n",
    "\n",
    "# reconstructed_model = keras.models.load_model(\"2daypm1_366\")\n",
    "# predictStats(reconstructed_model,Xtrain,ytrain,Xtest,ytest)\n",
    "\n",
    "predictStats(model,Xtrain,ytrain,Xtest,ytest)\n",
    "\n",
    "plothistory(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run below for non time series model (rolling mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PM2.5 101.89062177480847 82.2323519352033\n",
      "PM10 215.52704148344912 140.97411211440493\n",
      "NO 43.47371008555342 83.06738197153933\n",
      "NO2 58.66296373444158 40.64878421077701\n",
      "CO 1.5791442652531407 2.3089151644232633\n",
      "AQI 224.23316853135833 111.67767090442183\n",
      "features length 16\n",
      "(8503, 64) (8503, 1) (4189, 64) (4189, 1)\n"
     ]
    }
   ],
   "source": [
    "# dat = dataset(met,aqi,split_aqi)\n",
    "# df = dat.mergedData('DL031',roll=48,shift=48)\n",
    "Xtrain,ytrain,Xtest,ytest = getSplitFeatures(df)\n",
    "print(Xtrain.shape,ytrain.shape,Xtest.shape,ytest.shape)\n",
    "# predictStats(model,Xtrain,ytrain,Xtest,ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif, f_regression , mutual_info_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.svm import SVR\n",
    "dfTrain = df[:]\n",
    "features = ['Temperature','Relative Humidity','windX','windY','Year','MonthX','MonthY','hourX','hourY','isWeekend']\n",
    "features.append(\"PM2.5_lag1\")\n",
    "features.append(\"PM2.5_lag2\")\n",
    "\n",
    "features.append(\"PM10_lag1\")\n",
    "features.append(\"PM10_lag2\")\n",
    "\n",
    "features.append(\"NO2_lag1\")\n",
    "features.append(\"NO2_lag2\")\n",
    "\n",
    "features.append(\"SO2_lag1\")\n",
    "features.append(\"SO2_lag2\")\n",
    "\n",
    "features.append(\"NO_lag1\")\n",
    "features.append(\"NO_lag2\")\n",
    "\n",
    "features.append(\"NOx_lag1\")\n",
    "features.append(\"NOx_lag2\")\n",
    "\n",
    "# features.append(\"CO_lag1\")\n",
    "# features.append(\"CO_lag2\")\n",
    "\n",
    "# features.append(\"O3_lag1\")\n",
    "# features.append(\"O3_lag2\")\n",
    "\n",
    "# features.append(\"NH3_lag1\")\n",
    "# features.append(\"NH3_lag2\")\n",
    "\n",
    "# features = []\n",
    "# rlist=['PM2.5','PM10','NO','NO2','CO']\n",
    "# newlist = rlist + ['Temperature','Relative Humidity','windX','windY','Year','MonthX','MonthY','hourX','hourY','isWeekend']\n",
    "# for i in newlist:\n",
    "#     for j in range(24):\n",
    "#         features.append(i+'_t-'+str(j))\n",
    "        \n",
    "X = df[features]\n",
    "y = df['PM2.5_pred3']\n",
    "scaler = StandardScaler()\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "scaler.fit(Xtrain)\n",
    "\n",
    "reg = LinearRegression().fit(scaler.transform(Xtrain), ytrain)\n",
    "\n",
    "testPred = reg.predict(scaler.transform(Xtest))\n",
    "trainPred = reg.predict(scaler.transform(Xtrain))\n",
    "mse = np.mean((testPred - np.array(ytest))*(testPred - np.array(ytest)))\n",
    "print(reg.score(scaler.transform(Xtest), ytest))\n",
    "print(mean_squared_error(testPred, ytest,squared=False))\n",
    "print(mean_squared_error(trainPred, ytrain,squared=False))\n",
    "print(mean_absolute_error(testPred, ytest))\n",
    "print(mean_absolute_error(trainPred, ytrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif, f_regression , mutual_info_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "dfTrain = df[:]\n",
    "features = ['Temperature','Relative Humidity','windX','windY','Year','MonthX','MonthY','hourX','hourY','isWeekend']\n",
    "features.append(\"PM2.5_lag1\")\n",
    "features.append(\"PM2.5_lag2\")\n",
    "\n",
    "features.append(\"PM10_lag1\")\n",
    "features.append(\"PM10_lag2\")\n",
    "\n",
    "features.append(\"NO2_lag1\")\n",
    "features.append(\"NO2_lag2\")\n",
    "\n",
    "features.append(\"SO2_lag1\")\n",
    "features.append(\"SO2_lag2\")\n",
    "\n",
    "features.append(\"NO_lag1\")\n",
    "features.append(\"NO_lag2\")\n",
    "\n",
    "features.append(\"NOx_lag1\")\n",
    "features.append(\"NOx_lag2\")\n",
    "\n",
    "features.append(\"CO_lag1\")\n",
    "features.append(\"CO_lag2\")\n",
    "\n",
    "features.append(\"O3_lag1\")\n",
    "features.append(\"O3_lag2\")\n",
    "\n",
    "features.append(\"NH3_lag1\")\n",
    "features.append(\"NH3_lag2\")\n",
    "\n",
    "features = []\n",
    "rlist=['PM2.5','PM10','NO','NO2','CO']\n",
    "newlist = rlist + ['Temperature','Relative Humidity','windX','windY','Year','MonthX','MonthY','hourX','hourY','isWeekend']\n",
    "for i in newlist:\n",
    "    for j in range(24):\n",
    "        features.append(i+'_t-'+str(j))\n",
    "        \n",
    "X = df[features]\n",
    "y = df['PM2.5_pred1']\n",
    "scaler = StandardScaler()\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "scaler.fit(Xtrain)\n",
    "\n",
    "reg = MLPRegressor(random_state=1, max_iter=100).fit(scaler.transform(Xtrain), ytrain)\n",
    "\n",
    "testPred = reg.predict(scaler.transform(Xtest))\n",
    "trainPred = reg.predict(scaler.transform(Xtrain))\n",
    "mse = np.mean((testPred - np.array(ytest))*(testPred - np.array(ytest)))\n",
    "print(reg.score(scaler.transform(Xtest), ytest))\n",
    "print(mean_squared_error(testPred, ytest,squared=False))\n",
    "print(mean_squared_error(trainPred, ytrain,squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7769, 360) (7769,)\n",
      "0.3379685723175532\n",
      "65.78093011534429\n",
      "67.0801815314519\n",
      "43.41983467544137\n",
      "43.25189223855267\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif, f_regression , mutual_info_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.svm import SVR\n",
    "dfTrain = df[:]\n",
    "features = ['Temperature','Relative Humidity','windX','windY','Year','MonthX','MonthY','hourX','hourY','isWeekend']\n",
    "features.append(\"PM2.5_lag1\")\n",
    "features.append(\"PM2.5_lag2\")\n",
    "\n",
    "features.append(\"PM10_lag1\")\n",
    "features.append(\"PM10_lag2\")\n",
    "\n",
    "features.append(\"NO2_lag1\")\n",
    "features.append(\"NO2_lag2\")\n",
    "\n",
    "features.append(\"SO2_lag1\")\n",
    "features.append(\"SO2_lag2\")\n",
    "\n",
    "features.append(\"NO_lag1\")\n",
    "features.append(\"NO_lag2\")\n",
    "\n",
    "features.append(\"NOx_lag1\")\n",
    "features.append(\"NOx_lag2\")\n",
    "\n",
    "features.append(\"CO_lag1\")\n",
    "features.append(\"CO_lag2\")\n",
    "\n",
    "features.append(\"O3_lag1\")\n",
    "features.append(\"O3_lag2\")\n",
    "\n",
    "features.append(\"NH3_lag1\")\n",
    "features.append(\"NH3_lag2\")\n",
    "\n",
    "features = []\n",
    "rlist=['PM2.5','PM10','NO','NO2','CO']\n",
    "newlist = rlist + ['Temperature','Relative Humidity','windX','windY','Year','MonthX','MonthY','hourX','hourY','isWeekend']\n",
    "for i in newlist:\n",
    "    for j in range(24):\n",
    "        features.append(i+'_t-'+str(j))\n",
    "\n",
    "X = df[features]\n",
    "y = df['PM2.5_pred1']\n",
    "scaler = StandardScaler()\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "print(Xtrain.shape,ytrain.shape)\n",
    "# scaler.fit(Xtrain)\n",
    "\n",
    "reg = SVR(C=3.0, epsilon=0.2).fit(Xtrain, ytrain)\n",
    "\n",
    "testPred = reg.predict(Xtest)\n",
    "trainPred = reg.predict(Xtrain)\n",
    "mse = np.mean((testPred - np.array(ytest))*(testPred - np.array(ytest)))\n",
    "print(reg.score(Xtest, ytest))\n",
    "print(mean_squared_error(testPred, ytest,squared=False))\n",
    "print(mean_squared_error(trainPred, ytrain,squared=False))\n",
    "print(mean_absolute_error(testPred, ytest))\n",
    "print(mean_absolute_error(trainPred, ytrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf  \n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "features = ['Temperature','Relative Humidity','windX','windY','Year','MonthX','MonthY','hourX','hourY','isWeekend']\n",
    "features = []\n",
    "rlist=['PM2.5','PM10','NO','NO2','CO']\n",
    "newlist = rlist + ['Temperature','Relative Humidity','windX','windY','Year','MonthX','MonthY','hourX','hourY','isWeekend']\n",
    "for i in newlist:\n",
    "    for j in range(24):\n",
    "        features.append(i+'_t-'+str(j))\n",
    "predVector = []\n",
    "for j in range(24):\n",
    "    predVector.append('PM2.5_t+'+str(j))\n",
    "X = df[features]\n",
    "y = df[predVector]\n",
    "scaler = StandardScaler()\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "scaler.fit(Xtrain)\n",
    "print(Xtrain.shape)\n",
    "model = Sequential()\n",
    "model.add(Dense(200, input_dim=360, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(24, activation='linear'))\n",
    "model.summary()\n",
    "#Fit\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['mse','mae'])\n",
    "history = model.fit(scaler.transform(Xtrain), ytrain, epochs=100, batch_size=50,  verbose=1, validation_split=0.2)\n",
    "#Print Accuracy\n",
    "testPred = model.predict(scaler.transform(Xtest))\n",
    "trainPred = model.predict(scaler.transform(Xtrain))\n",
    "print(mean_squared_error(testPred, ytest,squared=False))\n",
    "print(mean_squared_error(trainPred, ytrain,squared=False))\n",
    "print(mean_absolute_error(testPred, ytest))\n",
    "print(mean_absolute_error(trainPred, ytrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for loss\n",
    "plt.plot(np.sqrt(history.history['loss']))\n",
    "plt.plot(np.sqrt(history.history['val_loss']))\n",
    "plt.title('model loss')\n",
    "plt.ylabel('RMSE loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
