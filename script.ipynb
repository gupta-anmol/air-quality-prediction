{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "RKP = \"DL031\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Bidirectional\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "sns.set_theme(style=\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\confusement\\miniconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3338: DtypeWarning: Columns (15) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['StationId', 'Datetime', 'PM2.5', 'PM10', 'NO', 'NO2', 'NOx', 'NH3',\n",
      "       'CO', 'SO2', 'O3', 'Benzene', 'Toluene', 'Xylene', 'AQI', 'AQI_Bucket'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Load datasets and rename columns, load all aqi data but specify metro data name\n",
    "def loadcsv(city=\"./data/rkpuram.csv\"):\n",
    "    met = pd.read_csv(city,delimiter=';',skiprows=24)\n",
    "    aqi = pd.read_csv('./data/station_hour.csv')\n",
    "    print(aqi.columns)\n",
    "    met.rename(columns={'# Date': 'Date',}, inplace=True)\n",
    "    met.rename(columns={'UT time': 'Time',}, inplace=True)\n",
    "    aqi['Time'] = aqi['Datetime'].str[-8:-3]\n",
    "    aqi['Date'] = aqi['Datetime'].str[0:10]\n",
    "    stations = [\"DL\"+str(x).zfill(3) for x in range(1,39)]\n",
    "    split_aqi = {}\n",
    "    for i in range(len(stations)):\n",
    "        split_aqi[stations[i]] = (aqi[aqi['StationId'] == stations[i]])\n",
    "    return met,aqi,split_aqi\n",
    "met,aqi,split_aqi = loadcsv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre - processing and loading data\n",
    "class dataset:\n",
    "    def __init__(self,met,aqi,split_aqi):\n",
    "            self.metro_data = met\n",
    "            self.aqi_data = aqi\n",
    "            self.split_aqi = split_aqi\n",
    "    def mergedData(self,station,rlist=['PM2.5','PM10','NO','NO2','NOx','NH3','CO','SO2','O3','AQI'],roll=48,shift=72):\n",
    "        df_aqi = self.getdf(station)\n",
    "        df = pd.merge(df_aqi, self.metro_data, how='inner', on=['Date', 'Time'])\n",
    "        print(\"Merged Dataset Size\",len(df))\n",
    "        \n",
    "        #Pre Processing merged Data\n",
    "        df['Year'] = df['Date'].str[0:4]\n",
    "        df['Month'] = df['Date'].str[5:7].astype(np.float64)\n",
    "        df['Day'] = df['Date'].str[8:10].astype(np.float64)\n",
    "        df['Hour'] = df['Time'].str[0:2]\n",
    "        \n",
    "        # TRIG TRANSFORMATIONS\n",
    "        df['windX'] = np.cos(np.deg2rad(df['Wind direction'])) * df['Wind speed']\n",
    "        df['windY'] = np.sin(np.deg2rad(df['Wind direction'])) * df['Wind speed']\n",
    "        df['hourX'] = np.cos((df['Hour'].astype(np.float64)-1)*np.pi/24)\n",
    "        df['hourY'] = np.sin((df['Hour'].astype(np.float64)-1)*np.pi/24)\n",
    "        df['MonthX'] = np.cos((df['Month'].astype(np.float64)-1)*np.pi/12)\n",
    "        df['MonthY'] = np.sin((df['Month'].astype(np.float64)-1)*np.pi/12)\n",
    "        \n",
    "        import datetime\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "        df['isWeekend'] =  (df['Date'].dt.dayofweek>=5).astype(int)\n",
    "        \n",
    "        df.interpolate(method='linear', limit=5,inplace=True)\n",
    "        \n",
    "        # Drop Additional columns\n",
    "        df.drop('Benzene', axis=1, inplace=True)\n",
    "        df.drop('Toluene',axis=1, inplace=True)\n",
    "        df.drop('Xylene', axis=1,inplace=True)\n",
    "        df.drop('AQI_Bucket',axis=1,inplace=True)\n",
    "        df.drop('Datetime',axis=1,inplace=True)\n",
    "        df.drop('StationId',axis=1,inplace=True)\n",
    "        df.drop('Short-wave irradiation',axis=1,inplace=True)\n",
    "        df.drop('Date',axis=1,inplace=True)\n",
    "        df.drop('Time',axis=1,inplace=True)\n",
    "        \n",
    "        # Rolling and shifting \n",
    "        print(\"Size before roll\",len(df))\n",
    "        rollList = ['PM2.5','PM10','NO','NO2','CO','AQI','Temperature','Relative Humidity','windX','windY','Year','MonthX','MonthY','hourX','hourY','isWeekend']\n",
    "        for i in rollList:\n",
    "            df[i+'_lagroll1'] = df[i].rolling(window=24, min_periods=12).mean().shift(6)\n",
    "            df[i+'_lagroll2'] = df[i].rolling(window=24, min_periods=12).mean().shift(12)\n",
    "            df[i+'_lagroll3'] = df[i].rolling(window=24, min_periods=12).mean().shift(18)\n",
    "            df[i+'_lagroll4'] = df[i].rolling(window=24, min_periods=12).mean().shift(24)\n",
    "            \n",
    "        for i in rlist:\n",
    "            df[i+'_lag1'] = df[i].shift(24)\n",
    "            df[i+'_lag2'] = df[i].shift(48)\n",
    "            df[i+'_lag3'] = df[i].shift(72)\n",
    "        for i in rlist:\n",
    "            df[i+\"_pred1\"] = df[i].shift(-24)\n",
    "            df[i+\"_pred2\"] = df[i].shift(-48)\n",
    "            df[i+\"_pred3\"] = df[i].shift(-72)\n",
    "        newlist = rlist + ['Temperature','Relative Humidity','windX','windY']\n",
    "        for i in newlist:\n",
    "            for j in range(24):\n",
    "                df[i+\"_t-\"+str(j)] = df[i].shift(j)\n",
    "                df[i+\"_t+\"+str(j)] = df[i].shift(-j-shift)\n",
    "        futurelist = ['Year','MonthX','MonthY','hourX','hourY','isWeekend']\n",
    "        for i in futurelist:\n",
    "            for j in range(24):\n",
    "                df[i+\"_t-\"+str(j)] = df[i].shift(-(shift+23-j))\n",
    "        df.dropna(inplace=True)\n",
    "        print(\"Size after roll\",len(df))\n",
    "        \n",
    "        return df.copy()\n",
    "    def getdf(self,station):\n",
    "        return self.split_aqi[station]\n",
    "    def plot(self,station):\n",
    "        df = self.getdf(station)\n",
    "    def stats(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Model Testing as well\n",
    "def getSplitFeaturesTimeSeries(df,TIME_SERIES_LENGTH = 24):\n",
    "    features = []\n",
    "    rlist=['PM2.5','PM10','NO','NO2','CO','AQI']\n",
    "    for it in rlist:\n",
    "        print(it,np.mean(df[it]),np.std(df[it]))\n",
    "    newlist = rlist + ['Temperature','Relative Humidity','windX','windY','Year','MonthX','MonthY','hourX','hourY','isWeekend']\n",
    "    for j in range(24):\n",
    "        for i in newlist:\n",
    "            features.append(i+'_t-'+str(j))\n",
    "    predVector = []\n",
    "    for j in range(24):\n",
    "        predVector.append('PM2.5_t+'+str(j))\n",
    "    X = df[features]\n",
    "    y = df[predVector]\n",
    "    X = np.array(X).reshape(X.shape[0],TIME_SERIES_LENGTH,len(newlist))\n",
    "    scaler = StandardScaler()\n",
    "    Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "    Xtrain = scaler.fit_transform(Xtrain.reshape(Xtrain.shape[0],TIME_SERIES_LENGTH*len(newlist)))\n",
    "    Xtrain = Xtrain.reshape(Xtrain.shape[0],TIME_SERIES_LENGTH,len(newlist))\n",
    "    Xtest = scaler.transform(Xtest.reshape(Xtest.shape[0],TIME_SERIES_LENGTH*len(newlist)))\n",
    "    Xtest = Xtest.reshape(Xtest.shape[0],TIME_SERIES_LENGTH,len(newlist))\n",
    "    return Xtrain,ytrain,Xtest,ytest\n",
    "def getSplitFeatures(df):\n",
    "    features = []\n",
    "    rlist=['PM2.5','PM10','NO','NO2','CO','AQI']\n",
    "    for it in rlist:\n",
    "        print(it,np.mean(df[it]),np.std(df[it]))\n",
    "    newlist = rlist + ['Temperature','Relative Humidity','windX','windY']\n",
    "    for j in range(1,5):\n",
    "        for i in newlist:\n",
    "            features.append(i+'_lagroll'+str(j))\n",
    "    futurelist = ['Year','MonthX','MonthY','hourX','hourY','isWeekend']\n",
    "    for j in futurelist:\n",
    "        features.append(i+'_t-0')\n",
    "    print(\"features length\",len(newlist))\n",
    "    predVector = ['PM2.5_t+0']\n",
    "    X = df[features]\n",
    "    y = df[predVector]\n",
    "    X = np.array(X)\n",
    "    scaler = StandardScaler()\n",
    "    Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "    Xtrain = scaler.fit_transform(Xtrain)\n",
    "    Xtest = scaler.transform(Xtest)\n",
    "    return Xtrain,ytrain,Xtest,ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def trainModel1(Xtrain,ytrain,Xtest,ytest,TIME_SERIES_LENGTH=24):\n",
    "    model = Sequential()\n",
    "    # model.add(LSTM(50, activation='relu', return_sequences=True, input_shape=(TIME_SERIES_LENGTH,len(newlist))))\n",
    "    model.add(Conv1D(128, 3,activation='relu',input_shape=(TIME_SERIES_LENGTH,len(newlist))))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv1D(128, 6,activation='relu',input_shape=(TIME_SERIES_LENGTH,len(newlist))))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv1D(128, 6,activation='relu',input_shape=(TIME_SERIES_LENGTH,len(newlist))))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # model.add(LSTM(200,activation='relu',input_shape=(TIME_SERIES_LENGTH,len(newlist))))\n",
    "    # model.add(Bidirectional(LSTM(50, activation='relu'), input_shape=(TIME_SERIES_LENGTH,len(newlist))))\n",
    "    # model.add(Dense(200, activation='relu'))\n",
    "    \n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.summary()\n",
    "    #Fit\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['mse','mae'])\n",
    "    history = model.fit(Xtrain, ytrain, epochs=200, batch_size=256,  verbose=1, validation_split=0.2)\n",
    "    return model,history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictStats(model,Xtrain,ytrain,Xtest,ytest):\n",
    "    testPred = model.predict(Xtest)\n",
    "    trainPred = model.predict(Xtrain)\n",
    "    print(mean_squared_error(testPred, ytest,squared=False))\n",
    "    print(mean_squared_error(trainPred, ytrain,squared=False))\n",
    "    print(mean_absolute_error(testPred, ytest))\n",
    "    print(mean_absolute_error(trainPred, ytrain))\n",
    "def plothistory(history):\n",
    "    # list all data in history\n",
    "    print(history.history.keys())\n",
    "    # summarize history for loss\n",
    "    plt.plot(np.sqrt(history.history['loss']))\n",
    "    plt.plot(np.sqrt(history.history['val_loss']))\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('RMSE loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Below for time series models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged Dataset Size 44035\n",
      "Size before roll 44035\n",
      "Size after roll 12744\n",
      "PM2.5 103.03830310010925 84.5700244186484\n",
      "PM10 215.99022288982553 142.28625443338535\n",
      "NO 44.45732496511488 84.2305126668444\n",
      "NO2 59.18754284350632 40.68718062845208\n",
      "CO 1.6067181968331843 2.3779436646278365\n",
      "AQI 225.0017557281858 112.66629221036271\n",
      "Model: \"sequential_74\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_123 (Conv1D)          (None, 22, 128)           6272      \n",
      "_________________________________________________________________\n",
      "batch_normalization_105 (Bat (None, 22, 128)           512       \n",
      "_________________________________________________________________\n",
      "conv1d_124 (Conv1D)          (None, 17, 128)           98432     \n",
      "_________________________________________________________________\n",
      "batch_normalization_106 (Bat (None, 17, 128)           512       \n",
      "_________________________________________________________________\n",
      "conv1d_125 (Conv1D)          (None, 12, 128)           98432     \n",
      "_________________________________________________________________\n",
      "batch_normalization_107 (Bat (None, 12, 128)           512       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 6, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_45 (Flatten)         (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_98 (Dense)             (None, 24)                18456     \n",
      "=================================================================\n",
      "Total params: 223,128\n",
      "Trainable params: 222,360\n",
      "Non-trainable params: 768\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "27/27 [==============================] - 4s 137ms/step - loss: 16363.3125 - mse: 16363.3125 - mae: 98.3313 - val_loss: 16132.6064 - val_mse: 16132.6064 - val_mae: 98.5804\n",
      "Epoch 2/200\n",
      "27/27 [==============================] - 4s 133ms/step - loss: 12719.6875 - mse: 12719.6875 - mae: 85.7084 - val_loss: 15320.5947 - val_mse: 15320.5947 - val_mae: 95.7422\n",
      "Epoch 3/200\n",
      "27/27 [==============================] - 3s 126ms/step - loss: 9302.9072 - mse: 9302.9072 - mae: 71.8377 - val_loss: 13102.7461 - val_mse: 13102.7461 - val_mae: 88.1316\n",
      "Epoch 4/200\n",
      "27/27 [==============================] - 3s 126ms/step - loss: 7024.2036 - mse: 7024.2036 - mae: 61.3584 - val_loss: 9879.3936 - val_mse: 9879.3936 - val_mae: 73.4125\n",
      "Epoch 5/200\n",
      "27/27 [==============================] - 3s 123ms/step - loss: 5502.9160 - mse: 5502.9160 - mae: 53.6851 - val_loss: 8074.3408 - val_mse: 8074.3408 - val_mae: 63.5606\n",
      "Epoch 6/200\n",
      "27/27 [==============================] - 3s 125ms/step - loss: 4594.8833 - mse: 4594.8833 - mae: 48.8173 - val_loss: 6503.7637 - val_mse: 6503.7637 - val_mae: 54.8476\n",
      "Epoch 7/200\n",
      "27/27 [==============================] - 3s 125ms/step - loss: 4140.4790 - mse: 4140.4790 - mae: 46.1126 - val_loss: 6291.8784 - val_mse: 6291.8784 - val_mae: 52.7634\n",
      "Epoch 8/200\n",
      "27/27 [==============================] - 3s 127ms/step - loss: 3881.6343 - mse: 3881.6343 - mae: 44.3912 - val_loss: 6451.7910 - val_mse: 6451.7910 - val_mae: 53.2829\n",
      "Epoch 9/200\n",
      "27/27 [==============================] - 3s 125ms/step - loss: 3718.3638 - mse: 3718.3638 - mae: 42.7525 - val_loss: 5884.7363 - val_mse: 5884.7363 - val_mae: 50.0561\n",
      "Epoch 10/200\n",
      "27/27 [==============================] - 4s 130ms/step - loss: 3579.6716 - mse: 3579.6716 - mae: 41.6401 - val_loss: 5293.0444 - val_mse: 5293.0444 - val_mae: 48.1125\n",
      "Epoch 11/200\n",
      "27/27 [==============================] - 4s 132ms/step - loss: 3442.5405 - mse: 3442.5405 - mae: 40.5762 - val_loss: 4723.9722 - val_mse: 4723.9722 - val_mae: 45.3715\n",
      "Epoch 12/200\n",
      "27/27 [==============================] - 4s 135ms/step - loss: 3318.7036 - mse: 3318.7036 - mae: 39.6115 - val_loss: 4859.1538 - val_mse: 4859.1538 - val_mae: 45.9832\n",
      "Epoch 13/200\n",
      "27/27 [==============================] - 4s 140ms/step - loss: 3259.3440 - mse: 3259.3440 - mae: 39.0196 - val_loss: 4574.8911 - val_mse: 4574.8911 - val_mae: 43.9821\n",
      "Epoch 14/200\n",
      "27/27 [==============================] - 4s 136ms/step - loss: 3148.7815 - mse: 3148.7815 - mae: 38.1763 - val_loss: 3757.4656 - val_mse: 3757.4656 - val_mae: 39.8779\n",
      "Epoch 15/200\n",
      "27/27 [==============================] - 4s 141ms/step - loss: 3059.9944 - mse: 3059.9944 - mae: 37.2633 - val_loss: 3148.4336 - val_mse: 3148.4336 - val_mae: 36.8721\n",
      "Epoch 16/200\n",
      "27/27 [==============================] - 4s 144ms/step - loss: 2944.9722 - mse: 2944.9722 - mae: 36.2786 - val_loss: 3026.9626 - val_mse: 3026.9626 - val_mae: 35.6690\n",
      "Epoch 17/200\n",
      "27/27 [==============================] - 4s 163ms/step - loss: 2768.2810 - mse: 2768.2810 - mae: 34.6325 - val_loss: 2938.9673 - val_mse: 2938.9673 - val_mae: 35.6606\n",
      "Epoch 18/200\n",
      "27/27 [==============================] - 4s 154ms/step - loss: 2622.7935 - mse: 2622.7935 - mae: 33.4344 - val_loss: 2517.0076 - val_mse: 2517.0076 - val_mae: 32.9885\n",
      "Epoch 19/200\n",
      "27/27 [==============================] - 4s 158ms/step - loss: 2077.2617 - mse: 2077.2617 - mae: 30.8514 - val_loss: 2107.2932 - val_mse: 2107.2932 - val_mae: 31.2088\n",
      "Epoch 20/200\n",
      "27/27 [==============================] - 4s 159ms/step - loss: 1811.5236 - mse: 1811.5236 - mae: 29.2676 - val_loss: 1909.7036 - val_mse: 1909.7036 - val_mae: 29.3266\n",
      "Epoch 21/200\n",
      "27/27 [==============================] - 4s 134ms/step - loss: 1699.2651 - mse: 1699.2651 - mae: 28.4872 - val_loss: 1910.0693 - val_mse: 1910.0693 - val_mae: 29.2701\n",
      "Epoch 22/200\n",
      "27/27 [==============================] - 4s 134ms/step - loss: 1607.1486 - mse: 1607.1486 - mae: 27.5358 - val_loss: 1820.3866 - val_mse: 1820.3866 - val_mae: 28.6717\n",
      "Epoch 23/200\n",
      "27/27 [==============================] - 4s 134ms/step - loss: 1504.9385 - mse: 1504.9385 - mae: 26.7558 - val_loss: 1765.6171 - val_mse: 1765.6171 - val_mae: 28.2928\n",
      "Epoch 24/200\n",
      "27/27 [==============================] - 3s 129ms/step - loss: 1455.7233 - mse: 1455.7233 - mae: 26.4905 - val_loss: 1700.8140 - val_mse: 1700.8140 - val_mae: 27.6082\n",
      "Epoch 25/200\n",
      "27/27 [==============================] - 4s 130ms/step - loss: 1416.2510 - mse: 1416.2510 - mae: 26.2000 - val_loss: 1701.8655 - val_mse: 1701.8655 - val_mae: 28.1203\n",
      "Epoch 26/200\n",
      "27/27 [==============================] - 3s 129ms/step - loss: 1360.6952 - mse: 1360.6952 - mae: 25.7282 - val_loss: 1619.1095 - val_mse: 1619.1095 - val_mae: 26.9626\n",
      "Epoch 27/200\n",
      "27/27 [==============================] - 3s 128ms/step - loss: 1285.1598 - mse: 1285.1598 - mae: 24.9601 - val_loss: 1337.4689 - val_mse: 1337.4689 - val_mae: 24.8431\n",
      "Epoch 28/200\n",
      "27/27 [==============================] - 3s 126ms/step - loss: 1237.8444 - mse: 1237.8444 - mae: 24.6730 - val_loss: 1437.7000 - val_mse: 1437.7000 - val_mae: 26.0745\n",
      "Epoch 29/200\n",
      "27/27 [==============================] - 4s 134ms/step - loss: 1236.1816 - mse: 1236.1816 - mae: 24.6565 - val_loss: 1291.9377 - val_mse: 1291.9377 - val_mae: 25.3914\n",
      "Epoch 30/200\n",
      "27/27 [==============================] - 3s 127ms/step - loss: 1210.3639 - mse: 1210.3639 - mae: 24.5702 - val_loss: 1245.7781 - val_mse: 1245.7781 - val_mae: 24.6831\n",
      "Epoch 31/200\n",
      "27/27 [==============================] - 4s 133ms/step - loss: 1121.1238 - mse: 1121.1238 - mae: 23.3611 - val_loss: 1142.5089 - val_mse: 1142.5089 - val_mae: 23.2518\n",
      "Epoch 32/200\n",
      "27/27 [==============================] - 4s 132ms/step - loss: 1050.4672 - mse: 1050.4672 - mae: 22.5898 - val_loss: 1162.9203 - val_mse: 1162.9203 - val_mae: 23.4971\n",
      "Epoch 33/200\n",
      "27/27 [==============================] - 4s 130ms/step - loss: 1011.8589 - mse: 1011.8589 - mae: 22.1203 - val_loss: 1067.5660 - val_mse: 1067.5660 - val_mae: 22.5419\n",
      "Epoch 34/200\n",
      "27/27 [==============================] - 4s 143ms/step - loss: 996.0074 - mse: 996.0074 - mae: 22.0487 - val_loss: 1076.7762 - val_mse: 1076.7762 - val_mae: 22.6106\n",
      "Epoch 35/200\n",
      "27/27 [==============================] - 3s 128ms/step - loss: 957.4347 - mse: 957.4347 - mae: 21.7195 - val_loss: 986.5891 - val_mse: 986.5891 - val_mae: 21.7284\n",
      "Epoch 36/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 4s 136ms/step - loss: 928.4421 - mse: 928.4421 - mae: 21.2507 - val_loss: 942.8405 - val_mse: 942.8405 - val_mae: 21.2618\n",
      "Epoch 37/200\n",
      "27/27 [==============================] - 4s 137ms/step - loss: 913.4710 - mse: 913.4710 - mae: 21.0937 - val_loss: 912.6288 - val_mse: 912.6288 - val_mae: 20.8627\n",
      "Epoch 38/200\n",
      "27/27 [==============================] - 3s 123ms/step - loss: 875.2258 - mse: 875.2258 - mae: 20.5153 - val_loss: 903.0056 - val_mse: 903.0056 - val_mae: 20.6413\n",
      "Epoch 39/200\n",
      "27/27 [==============================] - 4s 142ms/step - loss: 841.7104 - mse: 841.7104 - mae: 20.2356 - val_loss: 850.9203 - val_mse: 850.9203 - val_mae: 20.0942\n",
      "Epoch 40/200\n",
      "27/27 [==============================] - 3s 120ms/step - loss: 838.7258 - mse: 838.7258 - mae: 20.2548 - val_loss: 885.7994 - val_mse: 885.7994 - val_mae: 20.6952\n",
      "Epoch 41/200\n",
      "27/27 [==============================] - 4s 146ms/step - loss: 794.4037 - mse: 794.4037 - mae: 19.5132 - val_loss: 849.0189 - val_mse: 849.0189 - val_mae: 20.0281\n",
      "Epoch 42/200\n",
      "27/27 [==============================] - 3s 128ms/step - loss: 754.5195 - mse: 754.5195 - mae: 19.0255 - val_loss: 812.9107 - val_mse: 812.9107 - val_mae: 19.5381\n",
      "Epoch 43/200\n",
      "27/27 [==============================] - 4s 131ms/step - loss: 749.4171 - mse: 749.4171 - mae: 18.8623 - val_loss: 802.1049 - val_mse: 802.1049 - val_mae: 19.6987\n",
      "Epoch 44/200\n",
      "27/27 [==============================] - 4s 135ms/step - loss: 744.3207 - mse: 744.3207 - mae: 19.0445 - val_loss: 858.9371 - val_mse: 858.9371 - val_mae: 20.5312\n",
      "Epoch 45/200\n",
      "27/27 [==============================] - 3s 127ms/step - loss: 715.3624 - mse: 715.3624 - mae: 18.6341 - val_loss: 757.1638 - val_mse: 757.1638 - val_mae: 18.8810\n",
      "Epoch 46/200\n",
      "27/27 [==============================] - 4s 144ms/step - loss: 753.9799 - mse: 753.9799 - mae: 18.9526 - val_loss: 743.8842 - val_mse: 743.8842 - val_mae: 19.0158\n",
      "Epoch 47/200\n",
      "27/27 [==============================] - 3s 126ms/step - loss: 687.4039 - mse: 687.4039 - mae: 18.1487 - val_loss: 724.2192 - val_mse: 724.2192 - val_mae: 18.4160\n",
      "Epoch 48/200\n",
      "27/27 [==============================] - 4s 134ms/step - loss: 655.2539 - mse: 655.2539 - mae: 17.6678 - val_loss: 700.6055 - val_mse: 700.6055 - val_mae: 18.1909\n",
      "Epoch 49/200\n",
      "27/27 [==============================] - 4s 143ms/step - loss: 645.5410 - mse: 645.5410 - mae: 17.6215 - val_loss: 662.3099 - val_mse: 662.3099 - val_mae: 17.6151\n",
      "Epoch 50/200\n",
      "27/27 [==============================] - 3s 126ms/step - loss: 644.7300 - mse: 644.7300 - mae: 17.6177 - val_loss: 655.4299 - val_mse: 655.4299 - val_mae: 17.6921\n",
      "Epoch 51/200\n",
      "27/27 [==============================] - 4s 136ms/step - loss: 616.0479 - mse: 616.0479 - mae: 17.0998 - val_loss: 642.2169 - val_mse: 642.2169 - val_mae: 17.3657\n",
      "Epoch 52/200\n",
      "27/27 [==============================] - 4s 133ms/step - loss: 615.3447 - mse: 615.3447 - mae: 17.0658 - val_loss: 634.5635 - val_mse: 634.5635 - val_mae: 17.3330\n",
      "Epoch 53/200\n",
      "27/27 [==============================] - 4s 135ms/step - loss: 560.0768 - mse: 560.0768 - mae: 16.2691 - val_loss: 602.2173 - val_mse: 602.2173 - val_mae: 16.7531\n",
      "Epoch 54/200\n",
      "27/27 [==============================] - 4s 146ms/step - loss: 570.5203 - mse: 570.5203 - mae: 16.5177 - val_loss: 587.5208 - val_mse: 587.5208 - val_mae: 16.5870\n",
      "Epoch 55/200\n",
      "27/27 [==============================] - 3s 125ms/step - loss: 551.9628 - mse: 551.9628 - mae: 16.2158 - val_loss: 595.0081 - val_mse: 595.0081 - val_mae: 16.7589\n",
      "Epoch 56/200\n",
      "27/27 [==============================] - 5s 174ms/step - loss: 542.2714 - mse: 542.2714 - mae: 16.1161 - val_loss: 583.2807 - val_mse: 583.2807 - val_mae: 16.4105\n",
      "Epoch 57/200\n",
      "27/27 [==============================] - 5s 167ms/step - loss: 531.8426 - mse: 531.8426 - mae: 15.8655 - val_loss: 584.1166 - val_mse: 584.1166 - val_mae: 16.7255\n",
      "Epoch 58/200\n",
      "27/27 [==============================] - 4s 149ms/step - loss: 547.6679 - mse: 547.6679 - mae: 16.0697 - val_loss: 573.6595 - val_mse: 573.6595 - val_mae: 16.3063\n",
      "Epoch 59/200\n",
      "27/27 [==============================] - 4s 150ms/step - loss: 519.8033 - mse: 519.8033 - mae: 15.7975 - val_loss: 574.5367 - val_mse: 574.5367 - val_mae: 16.4490\n",
      "Epoch 60/200\n",
      "27/27 [==============================] - 5s 191ms/step - loss: 528.8062 - mse: 528.8062 - mae: 15.8787 - val_loss: 565.2034 - val_mse: 565.2034 - val_mae: 16.2334\n",
      "Epoch 61/200\n",
      "27/27 [==============================] - 4s 154ms/step - loss: 507.4761 - mse: 507.4761 - mae: 15.5416 - val_loss: 567.5549 - val_mse: 567.5549 - val_mae: 16.4376\n",
      "Epoch 62/200\n",
      "27/27 [==============================] - 4s 144ms/step - loss: 505.1630 - mse: 505.1630 - mae: 15.4740 - val_loss: 539.3898 - val_mse: 539.3898 - val_mae: 15.9837\n",
      "Epoch 63/200\n",
      "27/27 [==============================] - 4s 130ms/step - loss: 487.0752 - mse: 487.0752 - mae: 15.2898 - val_loss: 557.6234 - val_mse: 557.6234 - val_mae: 16.1033\n",
      "Epoch 64/200\n",
      "27/27 [==============================] - 4s 152ms/step - loss: 482.7368 - mse: 482.7368 - mae: 15.1303 - val_loss: 528.3404 - val_mse: 528.3404 - val_mae: 15.9285\n",
      "Epoch 65/200\n",
      "27/27 [==============================] - 4s 134ms/step - loss: 468.5600 - mse: 468.5600 - mae: 14.9217 - val_loss: 499.6375 - val_mse: 499.6375 - val_mae: 15.1886\n",
      "Epoch 66/200\n",
      "27/27 [==============================] - 4s 140ms/step - loss: 470.6678 - mse: 470.6678 - mae: 14.9990 - val_loss: 500.6699 - val_mse: 500.6699 - val_mae: 15.2976\n",
      "Epoch 67/200\n",
      "27/27 [==============================] - 4s 150ms/step - loss: 449.9791 - mse: 449.9791 - mae: 14.6584 - val_loss: 507.7726 - val_mse: 507.7726 - val_mae: 15.5173\n",
      "Epoch 68/200\n",
      "27/27 [==============================] - 4s 137ms/step - loss: 439.5736 - mse: 439.5736 - mae: 14.4680 - val_loss: 495.2037 - val_mse: 495.2037 - val_mae: 15.1366\n",
      "Epoch 69/200\n",
      "27/27 [==============================] - 4s 132ms/step - loss: 447.9741 - mse: 447.9741 - mae: 14.6318 - val_loss: 504.1267 - val_mse: 504.1267 - val_mae: 15.1926\n",
      "Epoch 70/200\n",
      "27/27 [==============================] - 4s 149ms/step - loss: 435.5336 - mse: 435.5336 - mae: 14.3574 - val_loss: 478.9042 - val_mse: 478.9042 - val_mae: 14.8748\n",
      "Epoch 71/200\n",
      "27/27 [==============================] - 4s 143ms/step - loss: 421.8623 - mse: 421.8623 - mae: 14.1597 - val_loss: 484.3568 - val_mse: 484.3568 - val_mae: 14.9339\n",
      "Epoch 72/200\n",
      "27/27 [==============================] - 4s 132ms/step - loss: 430.0704 - mse: 430.0704 - mae: 14.3931 - val_loss: 468.4557 - val_mse: 468.4557 - val_mae: 14.8816\n",
      "Epoch 73/200\n",
      "27/27 [==============================] - 4s 148ms/step - loss: 419.7606 - mse: 419.7606 - mae: 14.2185 - val_loss: 514.0057 - val_mse: 514.0057 - val_mae: 15.3765\n",
      "Epoch 74/200\n",
      "27/27 [==============================] - 4s 135ms/step - loss: 400.5115 - mse: 400.5115 - mae: 13.8006 - val_loss: 456.3848 - val_mse: 456.3848 - val_mae: 14.6117\n",
      "Epoch 75/200\n",
      "27/27 [==============================] - 4s 139ms/step - loss: 436.9211 - mse: 436.9211 - mae: 14.4203 - val_loss: 457.3538 - val_mse: 457.3538 - val_mae: 14.6020\n",
      "Epoch 76/200\n",
      "27/27 [==============================] - 4s 145ms/step - loss: 403.4825 - mse: 403.4825 - mae: 13.8809 - val_loss: 449.8627 - val_mse: 449.8627 - val_mae: 14.4968\n",
      "Epoch 77/200\n",
      "27/27 [==============================] - 4s 163ms/step - loss: 403.7910 - mse: 403.7910 - mae: 13.8497 - val_loss: 432.5602 - val_mse: 432.5602 - val_mae: 14.0807\n",
      "Epoch 78/200\n",
      "27/27 [==============================] - 4s 153ms/step - loss: 389.1537 - mse: 389.1537 - mae: 13.6892 - val_loss: 444.2495 - val_mse: 444.2495 - val_mae: 14.2735\n",
      "Epoch 79/200\n",
      "27/27 [==============================] - 4s 144ms/step - loss: 395.7270 - mse: 395.7270 - mae: 13.6870 - val_loss: 436.0630 - val_mse: 436.0630 - val_mae: 14.1563\n",
      "Epoch 80/200\n",
      "27/27 [==============================] - 4s 152ms/step - loss: 387.4567 - mse: 387.4567 - mae: 13.6837 - val_loss: 439.9257 - val_mse: 439.9257 - val_mae: 14.3485\n",
      "Epoch 81/200\n",
      "27/27 [==============================] - 5s 188ms/step - loss: 368.3903 - mse: 368.3903 - mae: 13.2817 - val_loss: 422.8171 - val_mse: 422.8171 - val_mae: 14.0971\n",
      "Epoch 82/200\n",
      "27/27 [==============================] - 4s 140ms/step - loss: 365.1040 - mse: 365.1040 - mae: 13.2141 - val_loss: 430.8921 - val_mse: 430.8921 - val_mae: 14.1474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/200\n",
      "27/27 [==============================] - 5s 167ms/step - loss: 368.0567 - mse: 368.0567 - mae: 13.2343 - val_loss: 414.3033 - val_mse: 414.3033 - val_mae: 13.9486\n",
      "Epoch 84/200\n",
      "27/27 [==============================] - 4s 149ms/step - loss: 371.7043 - mse: 371.7043 - mae: 13.3832 - val_loss: 417.8730 - val_mse: 417.8730 - val_mae: 13.9600\n",
      "Epoch 85/200\n",
      "27/27 [==============================] - 4s 135ms/step - loss: 369.5120 - mse: 369.5120 - mae: 13.3262 - val_loss: 416.1724 - val_mse: 416.1724 - val_mae: 13.8110\n",
      "Epoch 86/200\n",
      "27/27 [==============================] - 6s 222ms/step - loss: 358.1198 - mse: 358.1198 - mae: 13.0494 - val_loss: 400.1804 - val_mse: 400.1804 - val_mae: 13.7509\n",
      "Epoch 87/200\n",
      "27/27 [==============================] - 4s 159ms/step - loss: 356.6531 - mse: 356.6531 - mae: 13.0775 - val_loss: 402.7378 - val_mse: 402.7378 - val_mae: 13.6512\n",
      "Epoch 88/200\n",
      "27/27 [==============================] - 5s 171ms/step - loss: 350.5281 - mse: 350.5281 - mae: 12.9471 - val_loss: 398.5373 - val_mse: 398.5373 - val_mae: 13.5737\n",
      "Epoch 89/200\n",
      "27/27 [==============================] - 5s 178ms/step - loss: 344.9503 - mse: 344.9503 - mae: 12.8442 - val_loss: 399.5579 - val_mse: 399.5579 - val_mae: 13.6163\n",
      "Epoch 90/200\n",
      "27/27 [==============================] - 4s 159ms/step - loss: 361.3278 - mse: 361.3278 - mae: 13.1175 - val_loss: 414.3345 - val_mse: 414.3345 - val_mae: 14.0988\n",
      "Epoch 91/200\n",
      "27/27 [==============================] - 4s 145ms/step - loss: 349.1678 - mse: 349.1678 - mae: 12.9843 - val_loss: 393.4511 - val_mse: 393.4511 - val_mae: 13.4804\n",
      "Epoch 92/200\n",
      "27/27 [==============================] - 4s 162ms/step - loss: 356.4750 - mse: 356.4750 - mae: 13.0945 - val_loss: 396.3102 - val_mse: 396.3102 - val_mae: 13.9246\n",
      "Epoch 93/200\n",
      "27/27 [==============================] - 4s 155ms/step - loss: 341.8065 - mse: 341.8065 - mae: 12.8316 - val_loss: 389.6099 - val_mse: 389.6099 - val_mae: 13.4359\n",
      "Epoch 94/200\n",
      "27/27 [==============================] - 4s 140ms/step - loss: 337.2285 - mse: 337.2285 - mae: 12.7449 - val_loss: 411.8529 - val_mse: 411.8529 - val_mae: 14.0489\n",
      "Epoch 95/200\n",
      "27/27 [==============================] - 4s 160ms/step - loss: 334.4941 - mse: 334.4941 - mae: 12.7286 - val_loss: 376.7919 - val_mse: 376.7919 - val_mae: 13.2287\n",
      "Epoch 96/200\n",
      "27/27 [==============================] - 5s 194ms/step - loss: 334.2434 - mse: 334.2434 - mae: 12.6338 - val_loss: 388.0749 - val_mse: 388.0749 - val_mae: 13.5014\n",
      "Epoch 97/200\n",
      "27/27 [==============================] - 5s 194ms/step - loss: 341.4175 - mse: 341.4175 - mae: 12.8290 - val_loss: 401.6825 - val_mse: 401.6825 - val_mae: 14.0042\n",
      "Epoch 98/200\n",
      "27/27 [==============================] - 5s 173ms/step - loss: 316.0433 - mse: 316.0433 - mae: 12.3931 - val_loss: 366.6699 - val_mse: 366.6699 - val_mae: 12.9804\n",
      "Epoch 99/200\n",
      "27/27 [==============================] - 5s 170ms/step - loss: 338.6329 - mse: 338.6329 - mae: 12.7714 - val_loss: 375.8853 - val_mse: 375.8853 - val_mae: 13.3223\n",
      "Epoch 100/200\n",
      "27/27 [==============================] - 5s 185ms/step - loss: 320.6642 - mse: 320.6642 - mae: 12.4505 - val_loss: 377.5729 - val_mse: 377.5729 - val_mae: 13.2510\n",
      "Epoch 101/200\n",
      "27/27 [==============================] - 5s 189ms/step - loss: 314.6989 - mse: 314.6989 - mae: 12.3783 - val_loss: 372.4871 - val_mse: 372.4871 - val_mae: 13.1470\n",
      "Epoch 102/200\n",
      "27/27 [==============================] - 5s 203ms/step - loss: 297.5899 - mse: 297.5899 - mae: 11.9329 - val_loss: 362.1687 - val_mse: 362.1687 - val_mae: 13.0634\n",
      "Epoch 103/200\n",
      "27/27 [==============================] - 5s 201ms/step - loss: 293.7664 - mse: 293.7664 - mae: 11.8484 - val_loss: 378.3885 - val_mse: 378.3885 - val_mae: 13.6487\n",
      "Epoch 104/200\n",
      "27/27 [==============================] - 5s 196ms/step - loss: 301.5429 - mse: 301.5429 - mae: 12.0649 - val_loss: 354.1318 - val_mse: 354.1318 - val_mae: 12.8419\n",
      "Epoch 105/200\n",
      "27/27 [==============================] - 4s 156ms/step - loss: 290.2635 - mse: 290.2635 - mae: 11.8260 - val_loss: 355.9327 - val_mse: 355.9327 - val_mae: 12.8954\n",
      "Epoch 106/200\n",
      "27/27 [==============================] - 5s 167ms/step - loss: 297.7159 - mse: 297.7159 - mae: 12.0333 - val_loss: 354.7427 - val_mse: 354.7427 - val_mae: 12.7673\n",
      "Epoch 107/200\n",
      "27/27 [==============================] - 4s 149ms/step - loss: 286.9735 - mse: 286.9735 - mae: 11.7340 - val_loss: 335.2094 - val_mse: 335.2094 - val_mae: 12.5011\n",
      "Epoch 108/200\n",
      "27/27 [==============================] - 4s 149ms/step - loss: 291.0609 - mse: 291.0609 - mae: 11.8610 - val_loss: 360.5657 - val_mse: 360.5657 - val_mae: 13.0963\n",
      "Epoch 109/200\n",
      "27/27 [==============================] - 4s 161ms/step - loss: 302.5666 - mse: 302.5666 - mae: 12.1651 - val_loss: 359.6143 - val_mse: 359.6143 - val_mae: 12.9792\n",
      "Epoch 110/200\n",
      "27/27 [==============================] - 4s 165ms/step - loss: 295.1216 - mse: 295.1216 - mae: 11.9808 - val_loss: 366.4181 - val_mse: 366.4181 - val_mae: 13.0293\n",
      "Epoch 111/200\n",
      "27/27 [==============================] - 5s 170ms/step - loss: 302.7749 - mse: 302.7749 - mae: 12.1547 - val_loss: 339.9285 - val_mse: 339.9285 - val_mae: 12.7073\n",
      "Epoch 112/200\n",
      "27/27 [==============================] - 4s 153ms/step - loss: 276.3873 - mse: 276.3873 - mae: 11.6140 - val_loss: 335.1950 - val_mse: 335.1950 - val_mae: 12.4832\n",
      "Epoch 113/200\n",
      "27/27 [==============================] - 5s 170ms/step - loss: 270.0204 - mse: 270.0204 - mae: 11.4119 - val_loss: 332.2920 - val_mse: 332.2920 - val_mae: 12.2519\n",
      "Epoch 114/200\n",
      "27/27 [==============================] - 4s 152ms/step - loss: 289.8420 - mse: 289.8420 - mae: 11.8357 - val_loss: 333.2729 - val_mse: 333.2729 - val_mae: 12.3993\n",
      "Epoch 115/200\n",
      "27/27 [==============================] - 5s 199ms/step - loss: 283.1613 - mse: 283.1613 - mae: 11.7336 - val_loss: 345.9919 - val_mse: 345.9919 - val_mae: 12.7592\n",
      "Epoch 116/200\n",
      "27/27 [==============================] - 5s 184ms/step - loss: 294.1624 - mse: 294.1624 - mae: 12.0263 - val_loss: 360.0198 - val_mse: 360.0197 - val_mae: 13.1440\n",
      "Epoch 117/200\n",
      "27/27 [==============================] - 4s 144ms/step - loss: 297.4236 - mse: 297.4236 - mae: 12.1803 - val_loss: 360.0339 - val_mse: 360.0339 - val_mae: 13.0433\n",
      "Epoch 118/200\n",
      "27/27 [==============================] - 4s 135ms/step - loss: 282.5276 - mse: 282.5276 - mae: 11.7088 - val_loss: 341.2965 - val_mse: 341.2965 - val_mae: 12.5405\n",
      "Epoch 119/200\n",
      "27/27 [==============================] - 4s 166ms/step - loss: 268.6180 - mse: 268.6180 - mae: 11.4230 - val_loss: 335.4092 - val_mse: 335.4092 - val_mae: 12.4781\n",
      "Epoch 120/200\n",
      "27/27 [==============================] - 4s 152ms/step - loss: 258.8987 - mse: 258.8987 - mae: 11.2300 - val_loss: 357.0754 - val_mse: 357.0754 - val_mae: 13.0429\n",
      "Epoch 121/200\n",
      "27/27 [==============================] - 4s 155ms/step - loss: 247.6131 - mse: 247.6131 - mae: 10.9677 - val_loss: 326.8127 - val_mse: 326.8127 - val_mae: 12.1966\n",
      "Epoch 122/200\n",
      "27/27 [==============================] - 4s 151ms/step - loss: 278.0823 - mse: 278.0823 - mae: 11.6695 - val_loss: 334.2970 - val_mse: 334.2970 - val_mae: 12.4097\n",
      "Epoch 123/200\n",
      "27/27 [==============================] - 5s 189ms/step - loss: 262.9181 - mse: 262.9181 - mae: 11.3312 - val_loss: 337.7545 - val_mse: 337.7545 - val_mae: 12.4817\n",
      "Epoch 124/200\n",
      "27/27 [==============================] - 5s 197ms/step - loss: 297.0241 - mse: 297.0241 - mae: 12.1294 - val_loss: 346.9709 - val_mse: 346.9709 - val_mae: 12.7255\n",
      "Epoch 125/200\n",
      "27/27 [==============================] - 5s 191ms/step - loss: 264.0740 - mse: 264.0740 - mae: 11.3953 - val_loss: 321.8547 - val_mse: 321.8547 - val_mae: 12.2589\n",
      "Epoch 126/200\n",
      "27/27 [==============================] - 4s 153ms/step - loss: 260.0873 - mse: 260.0873 - mae: 11.2804 - val_loss: 317.3250 - val_mse: 317.3250 - val_mae: 12.0980\n",
      "Epoch 127/200\n",
      "27/27 [==============================] - 4s 165ms/step - loss: 272.1037 - mse: 272.1037 - mae: 11.6544 - val_loss: 350.5814 - val_mse: 350.5814 - val_mae: 13.0369\n",
      "Epoch 128/200\n",
      "27/27 [==============================] - 5s 176ms/step - loss: 270.9413 - mse: 270.9413 - mae: 11.6430 - val_loss: 341.9272 - val_mse: 341.9272 - val_mae: 12.8056\n",
      "Epoch 129/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 4s 159ms/step - loss: 264.5331 - mse: 264.5331 - mae: 11.4276 - val_loss: 331.7875 - val_mse: 331.7875 - val_mae: 12.5493\n",
      "Epoch 130/200\n",
      "27/27 [==============================] - 4s 155ms/step - loss: 261.7282 - mse: 261.7282 - mae: 11.3657 - val_loss: 325.4361 - val_mse: 325.4361 - val_mae: 12.5045\n",
      "Epoch 131/200\n",
      "27/27 [==============================] - 6s 210ms/step - loss: 241.5936 - mse: 241.5936 - mae: 10.9320 - val_loss: 342.4722 - val_mse: 342.4722 - val_mae: 12.8733\n",
      "Epoch 132/200\n",
      "27/27 [==============================] - 6s 204ms/step - loss: 243.8777 - mse: 243.8777 - mae: 11.0587 - val_loss: 311.9563 - val_mse: 311.9563 - val_mae: 11.9081\n",
      "Epoch 133/200\n",
      "27/27 [==============================] - 5s 168ms/step - loss: 247.4283 - mse: 247.4283 - mae: 11.0607 - val_loss: 324.8291 - val_mse: 324.8291 - val_mae: 12.3670\n",
      "Epoch 134/200\n",
      "27/27 [==============================] - 5s 182ms/step - loss: 241.1359 - mse: 241.1359 - mae: 10.9021 - val_loss: 310.2482 - val_mse: 310.2482 - val_mae: 12.0962\n",
      "Epoch 135/200\n",
      "27/27 [==============================] - 5s 194ms/step - loss: 234.8243 - mse: 234.8243 - mae: 10.6923 - val_loss: 308.7726 - val_mse: 308.7726 - val_mae: 12.0084\n",
      "Epoch 136/200\n",
      "27/27 [==============================] - 5s 171ms/step - loss: 239.5192 - mse: 239.5192 - mae: 10.9376 - val_loss: 305.6809 - val_mse: 305.6809 - val_mae: 11.9310\n",
      "Epoch 137/200\n",
      "27/27 [==============================] - 5s 177ms/step - loss: 231.4560 - mse: 231.4560 - mae: 10.7372 - val_loss: 296.5582 - val_mse: 296.5582 - val_mae: 11.6230\n",
      "Epoch 138/200\n",
      "27/27 [==============================] - 4s 148ms/step - loss: 243.4544 - mse: 243.4544 - mae: 10.9864 - val_loss: 316.6202 - val_mse: 316.6202 - val_mae: 12.0199\n",
      "Epoch 139/200\n",
      "27/27 [==============================] - 5s 184ms/step - loss: 224.3984 - mse: 224.3984 - mae: 10.5785 - val_loss: 301.0490 - val_mse: 301.0490 - val_mae: 11.5932\n",
      "Epoch 140/200\n",
      "27/27 [==============================] - 4s 162ms/step - loss: 230.0550 - mse: 230.0550 - mae: 10.7759 - val_loss: 308.5629 - val_mse: 308.5629 - val_mae: 11.9020\n",
      "Epoch 141/200\n",
      "27/27 [==============================] - 4s 144ms/step - loss: 237.0145 - mse: 237.0145 - mae: 10.9622 - val_loss: 321.2121 - val_mse: 321.2121 - val_mae: 12.2726\n",
      "Epoch 142/200\n",
      "27/27 [==============================] - 4s 134ms/step - loss: 235.0919 - mse: 235.0919 - mae: 10.8704 - val_loss: 309.7472 - val_mse: 309.7472 - val_mae: 11.9039\n",
      "Epoch 143/200\n",
      "27/27 [==============================] - 4s 138ms/step - loss: 228.1346 - mse: 228.1346 - mae: 10.7279 - val_loss: 315.7856 - val_mse: 315.7856 - val_mae: 11.9584\n",
      "Epoch 144/200\n",
      "27/27 [==============================] - 4s 141ms/step - loss: 231.5420 - mse: 231.5420 - mae: 10.7672 - val_loss: 294.3847 - val_mse: 294.3847 - val_mae: 11.6897\n",
      "Epoch 145/200\n",
      "27/27 [==============================] - 4s 143ms/step - loss: 220.1757 - mse: 220.1757 - mae: 10.4936 - val_loss: 285.5237 - val_mse: 285.5237 - val_mae: 11.4132\n",
      "Epoch 146/200\n",
      "27/27 [==============================] - 4s 138ms/step - loss: 210.0393 - mse: 210.0393 - mae: 10.2384 - val_loss: 292.6048 - val_mse: 292.6048 - val_mae: 11.7250\n",
      "Epoch 147/200\n",
      "27/27 [==============================] - 4s 137ms/step - loss: 232.8418 - mse: 232.8418 - mae: 10.8685 - val_loss: 322.6497 - val_mse: 322.6497 - val_mae: 12.3210\n",
      "Epoch 148/200\n",
      "27/27 [==============================] - 4s 152ms/step - loss: 229.3404 - mse: 229.3404 - mae: 10.7889 - val_loss: 291.8537 - val_mse: 291.8537 - val_mae: 11.5829\n",
      "Epoch 149/200\n",
      "27/27 [==============================] - 5s 174ms/step - loss: 214.1893 - mse: 214.1893 - mae: 10.3520 - val_loss: 285.4406 - val_mse: 285.4406 - val_mae: 11.5133\n",
      "Epoch 150/200\n",
      "27/27 [==============================] - 5s 201ms/step - loss: 220.5564 - mse: 220.5564 - mae: 10.5555 - val_loss: 284.4950 - val_mse: 284.4950 - val_mae: 11.4162\n",
      "Epoch 151/200\n",
      "27/27 [==============================] - 4s 163ms/step - loss: 220.4692 - mse: 220.4692 - mae: 10.5357 - val_loss: 316.1896 - val_mse: 316.1896 - val_mae: 12.4318\n",
      "Epoch 152/200\n",
      "27/27 [==============================] - 4s 154ms/step - loss: 224.9619 - mse: 224.9619 - mae: 10.6809 - val_loss: 324.7116 - val_mse: 324.7116 - val_mae: 12.1493\n",
      "Epoch 153/200\n",
      "27/27 [==============================] - 5s 168ms/step - loss: 233.6250 - mse: 233.6250 - mae: 10.9288 - val_loss: 300.8227 - val_mse: 300.8227 - val_mae: 11.8577\n",
      "Epoch 154/200\n",
      "27/27 [==============================] - 4s 149ms/step - loss: 207.3620 - mse: 207.3620 - mae: 10.2657 - val_loss: 303.1213 - val_mse: 303.1213 - val_mae: 11.7987\n",
      "Epoch 155/200\n",
      "27/27 [==============================] - 4s 158ms/step - loss: 218.4017 - mse: 218.4017 - mae: 10.5955 - val_loss: 331.7731 - val_mse: 331.7731 - val_mae: 12.3479\n",
      "Epoch 156/200\n",
      "27/27 [==============================] - 4s 138ms/step - loss: 203.7905 - mse: 203.7905 - mae: 10.1733 - val_loss: 288.6123 - val_mse: 288.6123 - val_mae: 11.4697\n",
      "Epoch 157/200\n",
      "27/27 [==============================] - 4s 142ms/step - loss: 214.6207 - mse: 214.6207 - mae: 10.4512 - val_loss: 348.2709 - val_mse: 348.2709 - val_mae: 12.7844\n",
      "Epoch 158/200\n",
      "27/27 [==============================] - 4s 141ms/step - loss: 210.3196 - mse: 210.3196 - mae: 10.3689 - val_loss: 304.4236 - val_mse: 304.4236 - val_mae: 11.8678\n",
      "Epoch 159/200\n",
      "27/27 [==============================] - 4s 149ms/step - loss: 209.6787 - mse: 209.6787 - mae: 10.3009 - val_loss: 312.0365 - val_mse: 312.0365 - val_mae: 12.1779\n",
      "Epoch 160/200\n",
      "27/27 [==============================] - 4s 152ms/step - loss: 199.9976 - mse: 199.9976 - mae: 10.1222 - val_loss: 289.5364 - val_mse: 289.5364 - val_mae: 11.6559\n",
      "Epoch 161/200\n",
      "27/27 [==============================] - 4s 148ms/step - loss: 193.7266 - mse: 193.7266 - mae: 9.9486 - val_loss: 278.8056 - val_mse: 278.8056 - val_mae: 11.1906\n",
      "Epoch 162/200\n",
      "27/27 [==============================] - 4s 145ms/step - loss: 184.1152 - mse: 184.1152 - mae: 9.7148 - val_loss: 287.6308 - val_mse: 287.6308 - val_mae: 11.5483\n",
      "Epoch 163/200\n",
      "27/27 [==============================] - 4s 165ms/step - loss: 202.5504 - mse: 202.5504 - mae: 10.1658 - val_loss: 283.6790 - val_mse: 283.6790 - val_mae: 11.2989\n",
      "Epoch 164/200\n",
      "27/27 [==============================] - 4s 159ms/step - loss: 183.6184 - mse: 183.6184 - mae: 9.6639 - val_loss: 278.3773 - val_mse: 278.3773 - val_mae: 11.3274\n",
      "Epoch 165/200\n",
      "27/27 [==============================] - 4s 142ms/step - loss: 190.0338 - mse: 190.0338 - mae: 9.8183 - val_loss: 274.4702 - val_mse: 274.4702 - val_mae: 11.2108\n",
      "Epoch 166/200\n",
      "27/27 [==============================] - 4s 146ms/step - loss: 181.2250 - mse: 181.2250 - mae: 9.6143 - val_loss: 298.8949 - val_mse: 298.8949 - val_mae: 12.0345\n",
      "Epoch 167/200\n",
      "27/27 [==============================] - 4s 150ms/step - loss: 182.5764 - mse: 182.5764 - mae: 9.6876 - val_loss: 274.4539 - val_mse: 274.4539 - val_mae: 11.1451\n",
      "Epoch 168/200\n",
      "27/27 [==============================] - 4s 151ms/step - loss: 215.2553 - mse: 215.2553 - mae: 10.5731 - val_loss: 304.7022 - val_mse: 304.7022 - val_mae: 11.8360\n",
      "Epoch 169/200\n",
      "27/27 [==============================] - 3s 127ms/step - loss: 199.3503 - mse: 199.3503 - mae: 10.0975 - val_loss: 298.1919 - val_mse: 298.1919 - val_mae: 11.7674\n",
      "Epoch 170/200\n",
      "27/27 [==============================] - 4s 143ms/step - loss: 182.6055 - mse: 182.6055 - mae: 9.6894 - val_loss: 311.0313 - val_mse: 311.0313 - val_mae: 12.0843\n",
      "Epoch 171/200\n",
      "27/27 [==============================] - 4s 148ms/step - loss: 197.4935 - mse: 197.4935 - mae: 10.1786 - val_loss: 308.9218 - val_mse: 308.9218 - val_mae: 12.0810\n",
      "Epoch 172/200\n",
      "27/27 [==============================] - 4s 160ms/step - loss: 186.7051 - mse: 186.7051 - mae: 9.8189 - val_loss: 314.3700 - val_mse: 314.3700 - val_mae: 12.2814\n",
      "Epoch 173/200\n",
      "27/27 [==============================] - 4s 134ms/step - loss: 194.4597 - mse: 194.4597 - mae: 10.0868 - val_loss: 284.9505 - val_mse: 284.9505 - val_mae: 11.6262\n",
      "Epoch 174/200\n",
      "27/27 [==============================] - 5s 178ms/step - loss: 208.1939 - mse: 208.1939 - mae: 10.3938 - val_loss: 273.8570 - val_mse: 273.8570 - val_mae: 11.2439\n",
      "Epoch 175/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 6s 217ms/step - loss: 189.7531 - mse: 189.7531 - mae: 9.8979 - val_loss: 277.0836 - val_mse: 277.0836 - val_mae: 11.3588\n",
      "Epoch 176/200\n",
      "27/27 [==============================] - 5s 196ms/step - loss: 184.0633 - mse: 184.0633 - mae: 9.7505 - val_loss: 271.2028 - val_mse: 271.2028 - val_mae: 11.0572\n",
      "Epoch 177/200\n",
      "27/27 [==============================] - 5s 180ms/step - loss: 169.9123 - mse: 169.9123 - mae: 9.3937 - val_loss: 311.3127 - val_mse: 311.3127 - val_mae: 12.4901\n",
      "Epoch 178/200\n",
      "27/27 [==============================] - 5s 192ms/step - loss: 191.7982 - mse: 191.7982 - mae: 10.0134 - val_loss: 300.0620 - val_mse: 300.0620 - val_mae: 11.7496\n",
      "Epoch 179/200\n",
      "27/27 [==============================] - 5s 185ms/step - loss: 195.4051 - mse: 195.4051 - mae: 10.0657 - val_loss: 291.8334 - val_mse: 291.8334 - val_mae: 11.7050\n",
      "Epoch 180/200\n",
      "27/27 [==============================] - 5s 187ms/step - loss: 187.8250 - mse: 187.8250 - mae: 9.8635 - val_loss: 287.6248 - val_mse: 287.6248 - val_mae: 11.3398\n",
      "Epoch 181/200\n",
      "27/27 [==============================] - 5s 191ms/step - loss: 163.8454 - mse: 163.8454 - mae: 9.2432 - val_loss: 282.3344 - val_mse: 282.3344 - val_mae: 11.1330\n",
      "Epoch 182/200\n",
      "27/27 [==============================] - 5s 177ms/step - loss: 173.8710 - mse: 173.8710 - mae: 9.4914 - val_loss: 267.1898 - val_mse: 267.1898 - val_mae: 10.9412\n",
      "Epoch 183/200\n",
      "27/27 [==============================] - 5s 170ms/step - loss: 188.4430 - mse: 188.4430 - mae: 9.9488 - val_loss: 279.2036 - val_mse: 279.2036 - val_mae: 11.2360\n",
      "Epoch 184/200\n",
      "27/27 [==============================] - 5s 179ms/step - loss: 193.9194 - mse: 193.9194 - mae: 10.1124 - val_loss: 272.6953 - val_mse: 272.6953 - val_mae: 11.1074\n",
      "Epoch 185/200\n",
      "27/27 [==============================] - 5s 182ms/step - loss: 175.5893 - mse: 175.5893 - mae: 9.6311 - val_loss: 282.8047 - val_mse: 282.8047 - val_mae: 11.3866\n",
      "Epoch 186/200\n",
      "27/27 [==============================] - 4s 164ms/step - loss: 185.1493 - mse: 185.1493 - mae: 9.8783 - val_loss: 281.1656 - val_mse: 281.1656 - val_mae: 11.2397\n",
      "Epoch 187/200\n",
      "27/27 [==============================] - 5s 172ms/step - loss: 170.5454 - mse: 170.5454 - mae: 9.5174 - val_loss: 287.1190 - val_mse: 287.1190 - val_mae: 11.2521\n",
      "Epoch 188/200\n",
      "27/27 [==============================] - 5s 174ms/step - loss: 177.0961 - mse: 177.0961 - mae: 9.6480 - val_loss: 310.2849 - val_mse: 310.2849 - val_mae: 11.8027\n",
      "Epoch 189/200\n",
      "27/27 [==============================] - 5s 182ms/step - loss: 166.3874 - mse: 166.3874 - mae: 9.3536 - val_loss: 255.3946 - val_mse: 255.3946 - val_mae: 10.7064\n",
      "Epoch 190/200\n",
      "27/27 [==============================] - 5s 173ms/step - loss: 164.6134 - mse: 164.6134 - mae: 9.2768 - val_loss: 266.4412 - val_mse: 266.4412 - val_mae: 10.8816\n",
      "Epoch 191/200\n",
      "27/27 [==============================] - 5s 173ms/step - loss: 160.0121 - mse: 160.0121 - mae: 9.1018 - val_loss: 273.4770 - val_mse: 273.4770 - val_mae: 11.0117\n",
      "Epoch 192/200\n",
      "27/27 [==============================] - 4s 161ms/step - loss: 154.4386 - mse: 154.4386 - mae: 9.0341 - val_loss: 267.8731 - val_mse: 267.8731 - val_mae: 10.8931\n",
      "Epoch 193/200\n",
      "27/27 [==============================] - 5s 188ms/step - loss: 153.0796 - mse: 153.0796 - mae: 8.9788 - val_loss: 259.8620 - val_mse: 259.8620 - val_mae: 10.7335\n",
      "Epoch 194/200\n",
      "27/27 [==============================] - 5s 180ms/step - loss: 166.0622 - mse: 166.0622 - mae: 9.3443 - val_loss: 276.7295 - val_mse: 276.7295 - val_mae: 11.2615\n",
      "Epoch 195/200\n",
      "27/27 [==============================] - 5s 189ms/step - loss: 184.2868 - mse: 184.2868 - mae: 9.9599 - val_loss: 264.6650 - val_mse: 264.6650 - val_mae: 10.9683\n",
      "Epoch 196/200\n",
      "27/27 [==============================] - 5s 195ms/step - loss: 163.3778 - mse: 163.3778 - mae: 9.2978 - val_loss: 270.3346 - val_mse: 270.3346 - val_mae: 11.0862\n",
      "Epoch 197/200\n",
      "27/27 [==============================] - 5s 181ms/step - loss: 167.4074 - mse: 167.4074 - mae: 9.3788 - val_loss: 272.6312 - val_mse: 272.6312 - val_mae: 11.1843\n",
      "Epoch 198/200\n",
      "27/27 [==============================] - 5s 195ms/step - loss: 186.3520 - mse: 186.3520 - mae: 9.9889 - val_loss: 283.0189 - val_mse: 283.0188 - val_mae: 11.2939\n",
      "Epoch 199/200\n",
      "27/27 [==============================] - 5s 193ms/step - loss: 170.3845 - mse: 170.3845 - mae: 9.5373 - val_loss: 259.7074 - val_mse: 259.7074 - val_mae: 10.8964\n",
      "Epoch 200/200\n",
      "27/27 [==============================] - 5s 190ms/step - loss: 161.5028 - mse: 161.5028 - mae: 9.1889 - val_loss: 267.6213 - val_mse: 267.6213 - val_mae: 11.0599\n",
      "INFO:tensorflow:Assets written to: 1daypm1_366\\assets\n",
      "16.855441276690538\n",
      "12.818625697313342\n",
      "11.401290980835519\n",
      "8.987629222288025\n",
      "dict_keys(['loss', 'mse', 'mae', 'val_loss', 'val_mse', 'val_mae'])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEXCAYAAABGeIg9AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAABDu0lEQVR4nO3dd3xV9f3H8de5+97scRNCwg7IEEFB9hCVEQGV0RZsRYr+xGqtpa0bigtx0NJH66xVW0erVBRHFVBRFAIiCMjeISSB7J27z/f3x4UIhoCJ5N6QfJ7/yL333HvfObned75nfI+mlFIIIYQQJzGEO4AQQojmR8pBCCFEHVIOQggh6pByEEIIUYeUgxBCiDqkHIQQQtQh5SDEjzR79mzefvvtMy7z1VdfMWHChB98vxDhJuUghBCiDlO4AwgRSl999RV//vOfSUpKYt++fdjtdm6//XZeffVVDh06xJgxY7jvvvsAePPNN3n11VcxGAwkJiYyb948OnXqRH5+Pvfccw8FBQW0bduW4uLi2tc/cOAACxYsoKysjEAgwPXXX8/UqVN/ULbKykoefPBBdu/ejaZpDB8+nN/97neYTCb++te/8vHHH2M2m4mLi2PhwoUkJSXVe78QP5oSohVZv3696tGjh9qxY4dSSqkbb7xR/exnP1Mej0cVFxerXr16qWPHjqnMzEx15ZVXquLiYqWUUkuXLlUZGRlK13V16623qsWLFyullMrKylJ9+/ZVS5cuVT6fT1111VVq+/btSimlKioqVEZGhtq8ebNav369Gj9+/GnznLj/rrvuUg8//LDSdV15PB41a9Ys9fzzz6u8vDx1ySWXKI/Ho5RS6sUXX1Qff/xxvfcLcS7IyEG0OmlpafTs2ROA9u3bExUVhcViIT4+noiICMrLy/nyyy+56qqriI+PB2Dy5MksWLCAnJwcMjMzufvuuwHo0KEDAwcOBCArK4vs7OzakQeA2+1m586ddOnS5ay5vvjiC/7zn/+gaRoWi4Vp06bxr3/9i5tuuonu3bszadIkRowYwYgRIxg8eDC6rp/2fiHOBSkH0epYLJZTbptMdf83UKeZckwphd/vR9O0Ux4/8fxAIEB0dDTvvvtu7WNFRUVERUWxZcuWs+bSdb3Obb/fj8Fg4LXXXmPbtm2sW7eORx99lIEDBzJ37tx67xfix5Id0kKcxrBhw/jwww8pKSkBYOnSpcTGxtKhQweGDx/Om2++CUBeXh5fffUVAJ06dcJqtdaWw9GjR5kwYQLbt2//we/5+uuvo5TC6/WyZMkShgwZwu7du5kwYQJdunRh9uzZzJw5kz179tR7vxDngowchDiNoUOHMnPmTG644QZ0XSc+Pp7nn38eg8HA/Pnzuffee8nIyKBNmzZ0794dCI5InnnmGRYsWMA//vEP/H4/d9xxB/369astkDOZO3cujzzyCBMnTsTn8zF8+HBuueUWLBYLGRkZTJkyBYfDgc1mY+7cuXTv3v209wtxLmjqdONnIYQQrZpsVhJCCFGHlIMQQog6pByEEELUIeUghBCiDikHIYQQdUg5CCGEqKPFnOdQWlqNrjf8qNyEhEiKi6uaINGP11yzSa6GkVwN11yztaRcBoNGXFxEvY+3mHLQddWocjjx3OaquWaTXA0juRquuWZrLblks5IQQog6pByEEELU0WI2KwkhRH2UUpSWFuL1uoHGb34pKDDUmT23Oag/l4bFYiMuzommaQ16TSkHIUSLV1VVjqZpJCenoWmN32BiMhnw+5tfOdSXSymdsrIiqqrKiYqKbdBrymYlIUSL53JVERUV+6OK4XykaQaiouJwuRp+hFXrWlNCiFZJ1wMYja1zQ4nRaELXAw1+Xqsuh637i/jNnz7DH2h+w0QhxLnV0G3uLUVjf+7WWaXHlRWXYCnYTbX7ImIiLGd/ghBC/Eh/+tPjbNu2Fb/fR07OETp27AzAT34yjfHjrz7r82fOvI5//vPfTR2zdZdDUvU+bo3+hJqSEcREtA93HCFEK/D7398NwNGjedx+++wGf9GHohiglZeDHt8p+N+cHdBOykEIET5Tp06kZ88L2bdvD8888w+WLPkPmzZ9TUVFBbGxsSxY8AQJCYkMG9afNWs28uKLz1NUVMiRI9nk5x9jwoRruOGGG89ZnlZdDqa4NpQEIrDl7wQywh1HCBECa7cdZc23Rxv1XE2DM11YedhFKQztndLIZDBo0BAeemghOTlHyM7O4rnnXsJgMPDww39k5crlTJ/+i1OW379/H8888w9crmqmTLmayZN/SlRUVKPf/2StuhzsNjO7fG0ZWLwPpfvRDK16dQghwqxnzwsBSEtrx69/PYf3319GdvZhduzYRmpqWp3lL7mkP2azGbs9nujoaKqrq6QczgWHzcRuX1uGBPYRKDiIqU23cEcSQjSxob0b/9d9U58EZ7VaAdi9excPPHA/06Zdx6hRV2A0GlCnGbJYLN8dSKNp2mmXaaxWfSirw2pir78NCo1Azo5wxxFCCAC2bNnExRf349prp9KxY2c2bPgq5NN2tOqRg81iwqWsuM0xmCsKwh1HCCEAuOKKMdx3353ccMM0jEYTXbqkc/RoXkgztOpyMBg07FYTLkMEkTVl4Y4jhGhFUlLa8tZb79fePvnfTmcSL7zwr9M+b82ajQDceOPsU+4/+fnnQqverAQQYTNRozlQrvJwRxFCiGajScuhqqqKCRMmkJOTA8Cbb77JhAkTmDhxIvfeey9erxeAXbt2MWXKFMaOHcv999+P3+9vylincNjNVOFAr5FyEEKIE5qsHLZu3cr06dPJysoC4NChQ7z44ou88cYbvPfee+i6zr//HTzT784772TevHmsWLECpRRLlixpqlh1RNjMVOgO8FSjAr6Qva8QQjRnTVYOS5YsYf78+SQlJQHBQ64eeOABIiMj0TSNbt26kZeXR25uLm63m759+wIwefJkli9f3lSx6nDYTJQFbAAoGT0IIQTQhDukFyxYcMrt1NRUUlNTASgpKeH1119n4cKFFBQU4HQ6a5dzOp3k5+c3+P0SEiIblTPCbqZctwMQY/Vhc56bE0jOFWczy3OC5GoYydVw5zJbQYEBk+nc/C18rl7nXDtTLoPB0OD1GfKjlfLz87npppuYMmUKAwcO5JtvvqmzTGOmmC0urkLXG34CSITNzAG3GaxQkpeH2dL4U9/PNaczisLCynDHqENyNYzkarhznU3X9XNy8tr5diW4E3Rdr7M+DQbtjH9Uh7QCDxw4wPTp05k0aRK33XYbAMnJyRQVFdUuU1hYWLspKhQcNhOF7uBZhrJZSQghgkI2cqiqquLGG29kzpw5XHPNNbX3p6amYrVa2bRpE/369WPZsmWMGDEiVLGCm5UCVkCTchBCNLkfez2HqqoqFiyYz8KFf2rSnCErh7feeouioiJeeuklXnrpJQAuv/xy7rjjDhYtWsTcuXOprq6mZ8+ezJgxI1SxcNjM6BhQtkiUnAgnhGhiP/Z6DpWVFezbt7cpop2iycth1apVAMycOZOZM2eedpnu3bvz1ltvNXWU04qwBVeBbo1Gl3IQosXz7V2Lb88XjXru2Sa3M18wAnO3oQ1+3ZycIyxatJCKinKsVhtz5txJt27dWblyOf/+9ysYDAbatm3LvHkP85e/PElRUSH33vsHFi5c1Kif44donrvdQ8hhNwMQsESjXBVhTiOEaI0WLJjPrbf+hpdeep277rqf+fPvA+CFF55l8eKneOml12jfviPZ2Vn89rd3kpjobNJigFY+txIEj1YC8JojsZYfC3MaIURTM3cb2qi/7qFpjlaqqalh166dPProQ7X3uVwuysvLGDp0OL/61Y0MH34ZI0deTteuF4RsAj4ph+MjB48xksiaCpTS0bRWP6ASQoSIrutYLNZT9j0UFOQTHR3Db3/7B/bvv4Z169bw8MPzmDXrZi66qG9IcrX6b0HH8X0OLkMEqADKXRXmREKI1iQyMpK0tHasWPEhAF9/vZ7bbruZQCDAtGmTiI2N5frrf8m4cePZu3cPRqORQCDQ5Llk5HB8s1KNCl6BCU8N2KPDmEgI0drMn/8ITz75KP/+9yuYTGYeeuhRTCYTN944m9/+9lasVhuRkVHMnfsAcXHxJCe34fbbZ/O3vz3fZJlafTnYrSY0oEYPloTy1oQ3kBCiVTj5eg4dOnTkqaf+XmeZ0aPHMXr0uDr3P/fcS02er9VvVjIYNGxWI9WBYE8qryvMiYQQIvxa/cgBgqOH6uOb8GTkIIQQUg4A2C0mKo9fX0jKQYiWSSnVqEk9z3dnOmnvTFr9ZiUAm9VIpc8YvCGblYRocQwGI4FA6K4w2ZwEAn4MBmODnyflQHDkUOENrgrZ5yBEy2O3R1JZWYZSzW+67aaklE5lZSl2e8OvdyOblQCb1URxhRvMdtmsJEQLFBkZQ2lpIfn5OUDjNrNA8KI5ut78Cqb+XBoWi43IyJgGv6aUA2C3GHF7A2gOu4wchGiBNE0jPv7HXyemuV4gqSlyyWYlgkcruTx+NIsDZOQghBBSDgC24yMHLDJyEEIIkHIAgiMHAGWyyT4HIYRAygH4rhwCRpuMHIQQAikHILhZCcBvtMl5DkIIgZQD8N3IwWeworw1jT6jUAghWgopB4InwQF4NQvoAQh4w5xICCHCS8qB4PQZAB6C13SQ/Q5CiNZOyoHvRg5uJdd0EEIIkHIAwH585OA6fsEf2SkthGjtpBwA2/GRQ42yALJZSQghpBwIXg3OajZS5T9xNTjZrCSEaN2atByqqqqYMGECOTk5AGRmZjJx4kTGjBnD4sWLa5fbtWsXU6ZMYezYsdx///34/aGfd/2US4V6pByEEK1bk5XD1q1bmT59OllZWQC43W7uu+8+nnnmGT788EO2b9/O6tWrAbjzzjuZN28eK1asQCnFkiVLmipWvewWE5W+45PUymYlIUQr12TlsGTJEubPn09SUnCa3G+//ZYOHTrQrl07TCYTEydOZPny5eTm5uJ2u+nbty8AkydPZvny5U0Vq152q4kKnwHQZLOSEKLVa7LrOSxYsOCU2wUFBTidztrbSUlJ5Ofn17nf6XSSn5/fVLHqZbcacXt1sMj8SkIIEbKL/ZxuSgpN0+q9v6ESEhp+GbwTnM4oYqJs5BVWYbJHYtW8OJ1RjX69c6m55Pg+ydUwkqvhmmu21pIrZOWQnJxMUVFR7e2CggKSkpLq3F9YWFi7Kaohiour0PWGz4l04gpKBhRVNV70mChcZSXN4mpPremqU+eC5GqY5poLmm+2lpTLYNDO+Ed1yA5l7dOnD4cOHeLw4cMEAgE++OADRowYQWpqKlarlU2bNgGwbNkyRowYEapYtewWEy5PAIMjBlVTFvL3F0KI5iRkIwer1cpjjz3G7bffjsfjYeTIkYwbNw6ARYsWMXfuXKqrq+nZsyczZswIVaxaNqsJl9cP9hhU/v6Qv78QQjQnTV4Oq1atqv334MGDee+99+os0717d956662mjnJGdqsRpUDZolHuSpTuRzOErDuFEKJZkTOkj6udttsc3KmjXM1vu6IQQoSKlMNxDluwHDyGCADZ7yCEaNWkHI6LsAdnZK2pLYfycMYRQoiwknI4LtIWLIcqZQdAd0k5CCFaLymH4yKOb1Yq122AbFYSQrRuUg7HndisVOUBrBGyWUkI0apJORxnsxgxGjSq3T4MjlgpByFEqyblcJymaUTYTFS7fGiOGNnnIIRo1aQcThJhN1Pl9qPZY2TkIIRo1aQcThJhM9eOHFRN2WlnjBVCiNZAyuEkETbT8X0OMRDwgU+u6yCEaJ2kHE4SYTdT7fKjOWIB0OVwViFEKyXlcJIIm5lqtw/NHgPIWdJCiNZLyuEkEXYTbm8A3RYNSDkIIVovKYeTRByfQsOtyfxKQojWTcrhJBH24BQa1boZjCaUnOsghGilpBxOcmLyvWp3AM0eIzukhRCtlpTDSWrnV6o910FGDkKI1ums5XDgwAH++9//opTi1ltv5YorrmD9+vWhyBZyJ2ZmlfmVhBCt3VnLYf78+VitVj777DNKS0t59NFHWbx4cSiyhdyJkUO1K3g4q+xzEEK0VmctB4/Hw9VXX83atWvJyMhg4MCB+Hy+UGQLObvVhKYRnF/JEYtyV6J0f7hjCSFEyJ21HLxeL0VFRXz++ecMGTKEoqIiPB5PKLKFnEHTvjsRznHiRLiKMKcSQojQO2s5/OxnP2PUqFH069eP9PR0pk6dyg033BCKbGERYTdTWXN8fiWQTUtCiFbJdLYFrrvuOqZNm4bBEOyRd955h7i4uCYPFi4xERYqqr1o9lhALhcqhGidftDRSkuXLq09Wmnq1Kkt9mglgOgIC+XV3trNSrocsSSEaIXCcrTSu+++y/jx4xk/fjyPP/44ALt27WLKlCmMHTuW+++/H78/PDuCvxs5yGYlIUTrFfKjlVwuFwsWLODVV1/l3XffZePGjWRmZnLnnXcyb948VqxYgVKKJUuWNPo9foyYCAsujx+frqFZI+VcByFEqxTyo5UCgQC6ruNyufD7/fj9fkwmE263m759+wIwefJkli9f3uj3+DFiIiwAwdFDZByBgoMopePL2oQ/+9uwZBJCiFA76w7pE0crZWRkkJ6ezmWXXcatt97a6DeMjIzkjjvuICMjA5vNxoABAzCbzTidztplnE4n+fn5jX6PHyMmMlgO5dVeYnqPw/35C7g/eQb/oU1oMUlEtr8oLLmEECKUQn600u7du1m6dCmfffYZUVFR/OEPf2Dt2rV1ltM0rUGvm5AQ2ehMTmdU7b87uAMAKKORlCFjyc/dTM2+jWgmC6o8nzibD1NUfKPf68dka04kV8NIroZrrtlaS66zlkNNTQ1PPPEEX3zxBX6/n6FDh3L//fcTGdm4L+M1a9YwePBgEhISgOAmpBdffJGioqLaZQoLC0lKSmrQ6xYXV6HrqsF5nM4oCgsra2/rvuCO8Jyj5RS1iUQbNANLZArGNt1wffQnCrZvwpw+qMHv0xjfz9ZcSK6GkVwN11yztaRcBoN2xj+qz7rPYeHChXi9Xp5++mmeeeYZNE3j4YcfblCIk3Xv3p3MzExqampQSrFq1SoGDBiA1Wpl06ZNACxbtowRI0Y0+j1+jChHcH6l8movAAZ7NNZLp2BM7QVmO4Gju8OSSwghQumsI4etW7fy3nvv1d5+5JFHGD9+fKPfcNiwYezcuZPJkydjNpvp3bs3N998M6NHj2bu3LlUV1fTs2dPZsyY0ej3+DFMRgORdnNtOZygGQwYU7oRyJNyEEK0fGcthxNHF53Y56DrOkaj8Ue96c0338zNN998yn3du3fnrbfe+lGve67ERFgor6p7RJYppTue7K3oNWUYHLGhDyaEECFy1nIYPHgwv/3tb5k+fToA//nPfxg4cGCTBwun6AgLFTXeOvcbUy4AIHBsL4bOA0IdSwghQuas+xzuuece0tPT+fOf/8yiRYvo1KkTd911VyiyhU1MpIXyqrrlYEhoD0YzgfwDYUglhBChc9aRg8lk4je/+Q2/+c1vQpGnWTgxhYZS6pRDajWjCaOzE4H8/WFMJ4QQTa/ecrj44ovPeK7BN9980ySBmoPoCAtev47bG8BuPXUVGZPT8W5bifJ70UyWMCUUQoimVW85fPDBB6HM0aycmEKjrMpTpxwMyV1gqx+96DDGNl3DEU8IIZpcveWQmpoayhzNSmKMHYCicjcpCRGnPGZMSgcgULBfykEI0WKddYd0a5QcFyyHglJXnccMjhi0KCeBY7LfQQjRckk5nEZ0hAWr2Uh+ac1pHzcmpxMoOIBSDZ+uQwghzgeNKoeSkpJznaNZ0TQNZ6ydwtOMHCBYDqqmDFVVHOJkQggRGvWWw6xZs2r//fzzz5/y2I033th0iZqJ5Dg7BWX1lwMgh7QKIVqsesvh5NHB9y+80xo2pyTF2Sksc512pldDfBqYrFIOQogWq95yOPkch++XQUOvtXA+Soqz4w8oSirddR7TDMbgyXAFB9ArCwkUZoU+oBBCNKF6y+HkQmgNZfB9SXEOgDPud9CLDlPzzkPUvL8Q5T39zmshhDgf/aCRQ2t04nDW/DPtd1A6SveD34Nvz5ehjCeEEE2q3pPgDh48yMSJEwHIzs6u/TfAkSNHmj5ZmMVGWTEZDac91wHA2LYHpi6DsPQZh2ft63i3f4K512g0gxwdLIQ4/9VbDi+88EIoczQ7Bk0jKc7O0aLq0z6uma3Yr7gFAPOFo3F/+gyBnG2Y2vcJZUwhhGgS9ZbDgAF1r1dQVlZGTExMq9nk1KVtNJv2FKLrCoOh/p/Z1KEvoBEozJJyEEK0CPVuA6mqquIPf/gDGzZsAOB3v/sdgwcPZsyYMRw+fDhkAcOpR8c4ajx+Duef+cLdmsmCFhGHXpEfomRCCNG06i2Hxx9/nIiICNLT01m9ejXr1q1j1apVPPjggzz++OOhzBg2PdrHAbD7cOlZlzXEJKNXFDR1JCGECIl6y2HLli088MADxMfH88UXXzB69GhSUlIYMmQIWVlZIYwYPjGRVlITI9j5Q8ohOglVLiMHIUTLUG85GI3G2n0LmzdvPmUfRGs4Q/qE7h3i2HekDJ9fP+NyWnQyyl0p5zsIIVqEesvBYDBQWVlJfn4+e/bsYeDAgQDk5+djNptDFjDcenaMw+vX2Zl15skGDTFJALJpSQjRItR7tNIvfvELJk2ahFKKjIwMnE4nq1at4k9/+hO/+MUvQpkxrHp3TiA20sInm3Lok55Y73KG6GQA9PJ8jIkdQ5ROCCGaRr3lMHnyZNLT0ykqKmLEiBEAlJaWctNNNzFp0qSQBQw3k9HA5Zek8fYXB8ktrCLVGXna5QzRx0cOst9BCNEC1FsOABdddNEpt6dMmdKkYZqrkX3b8n5mFss3ZHPj+J6nXUYzW9EcsbJZSQjRItRbDidPl3E677//fqPfdNWqVTz11FPU1NQwbNgw5s6dS2ZmJgsXLsTj8ZCRkcGcOXMa/frnWpTDwqiLU1n59RGGXphC9w5xp13OEJ2EknIQQrQA9ZZDTU0NHo+Hq6++muHDh2M0Gs/JGx45coT58+fz3//+l4SEBG644QZWr17N/PnzefXVV0lJSWH27NmsXr2akSNHnpP3PBcmDe/Mlv1FvPThLh6cNQC7te6q06KTCRzZGoZ0QghxbtV7tNKnn37KX/7yF8rLy3nwwQf5/PPPiY+PZ8CAAaedWuOH+vjjj7nqqqto06YNZrOZxYsXY7fb6dChA+3atcNkMjFx4sQ6FxgKN6vFyI3je1Bc4ealD3ed9nBeY1xblKsC5a4KQ0IhhDh3zrjPoX///vTv3x+3283HH3/MwoULqaqq4pprruG6665r1BsePnwYs9nMjTfeSGFhIaNGjaJr1644nc7aZZKSksjPb347drumxfKTy9JZ8tl+Plx/mPGDO57yuCE+FYBAaS6mlAvCkFAIIc6NM5bDCTabjauuugqHw8HLL7/M4sWLG10OgUCAjRs38uqrr+JwOLj11lux2+11lmvo5H4JCac/iuiHcDqjfvCyvxjfk2OlLt7+4iC9uyXRr3ty7WN+a3eyAYe3iBhn/0bnaWy2UJJcDSO5Gq65Zmstuc5aDlu2bGHZsmV8/PHH9OrVi+nTp3PllVc2+g0TExMZPHgw8fHxAFxxxRUsX778lH0aBQUFJCUlNeh1i4urTnu957NxOqMoLDzzxHrfN/3ydA7mlvPEKxv548z+tVeNU8oMFjsVRw7g7dCw1zxX2UJBcjWM5Gq45pqtJeUyGLQz/lFd7z6Hv/3tb4wZM4YHHniAjh078u677/L3v/+d8ePHY7VaGxTiZKNGjWLNmjVUVFQQCAT48ssvGTduHIcOHeLw4cMEAgE++OCD2nMrmiOrxcivp/RG0+Cpt7fh8QaA4GjHGJeGXpIT5oRCCPHj1DtyePrpp2nbti1t2rRh/fr1rF+//pTHn3vuuUa9YZ8+fbjpppu47rrr8Pl8DB06lOnTp9O5c2duv/12PB4PI0eOZNy4cY16/VBJirUz++peLF6ylWff3c4vM7oTE2nFEJ+K7+DXKKVazXUvhBAtT73lsHDhwiZ706lTpzJ16tRT7hs8eDDvvfdek71nU7iwcwLXje7GG5/u496/r+fmq3vRMz4Ndn2OqilDizj9+RBCCNHc1VsOZ5oiY+3atU0S5nx0Rb80LuwUz3Pv7eDZZdu598pYEgC9JAeDlIMQ4jxV7z6HHTt2MG3aNG655RZKSoIzkubl5XHbbbfxq1/9KmQBzwfJ8Q5+99M+JETbeGpVMQoN34H1rWpqcyFEy1JvOTzwwAOMGTOGtLQ0nn32WT788EPGjx+P2+3m3XffDWXG80KUw8Kd0y/GERPLJ+4L8e9di2/Hp+GOJYQQjVLvZqXKykpmzZpFIBBg7NixfPTRRzzyyCOMHz8+lPnOK3FRVu75eT/+/KaBtlVl9Mx8HQwGLD0vD3c0IYRokHpHDidOTDMajXg8Hl544QUphh/AYTNx+9Q+vKuNZq+ehmfNK/j2ZYY7lhBCNEi95XDy9vL4+Hh69OgRkkAtQUyEhZuu6cNzZSNxmWPwZ30T7khCCNEg9W5W0nWd8vJylFIopWr/fUJsbGwo8p23uqTGcPEFyezOS6Bv/gHqThAihBDNV73lsHfvXgYNGlRbCCeuIQ3BM4F37drV9OnOc1NGdmHlKwlcXHMQvbpUDm0VQpw36i2H3bt3hzJHi9Qm3kGbbr0g52uyd3xLxwHN5/oUQghxJvXucxDnxohRgwgoA3s3f9OoiQGFECIcpByamM1uwx2ZQqL/GEXlrnDHEUKIH0TKIQS0xM60NxWTVyRXiBNCnB+kHELAkdYNq+anIjcr3FGEEOIHkXIIgYi0bgAECg6EOYkQQvwwUg4hoEUn4cKGvfJIuKMIIcQPIuUQApqmUW5rS4LvqMzUKoQ4L0g5hIgvtgNJWimV5RXhjiKEEGcl5RAilpR0DBoUH9oT7ihCCHFWUg4hEtehOwCevH1hTiKEEGcn5RAi8c54cgIJRBZslf0OQohmT8ohRDRNIzvqYmJ8BQTy5ZBWIUTzJuUQQvbuQ3ArE+VbPg53FCGEOCMphxC6sFtbNno6Y87eQOU/b8W9/g3ZxCSEaJbqnbJbnHvx0TZ2RAwiUnPQP0XD9+1yNFs01r5XhTuaEEKcQkYOIda5a0f+VdiHqgE3Yeo8AO+GJbjX/QcV8IU7mhBC1ApbOTz++OPcc889AOzatYspU6YwduxY7r//fvx+f7hiNbmRfdpiMRv45/K9WC+7CXPPy/FtW0HN+4+hu+QEOSFE8xCWcli3bh3vvPNO7e0777yTefPmsWLFCpRSLFmyJByxQiIhxsZPL09n1+FSPttagG3YDGxX3oZenE3N2w9Q88HjeLd+FO6YQohWLuTlUFZWxuLFi7nlllsAyM3Nxe1207dvXwAmT57M8uXLQx0rpEb2acuFneN5c9U+DuZVYO58KY4Jd2OISUa5K/F89SbebSvDHVMI0YqFvBz++Mc/MmfOHKKjowEoKCjA6XTWPu50OsnPzw91rJDSNI2bJ/YiNtLK0+9so7LGizE5HceEu3FMfghTx3541v0Hz9GD4Y4qhGilQnq00n//+19SUlIYPHgwb7/9NsBpD+XUNK3Br52QENnoXE5nVKOf2+j3BObOGsgf/voFSz4/yN0z+tf+3IFJt3N48S+pOfANzmFTQ57thwjHOvshJFfDNNdc0HyztZZcIS2HDz/8kMLCQq655hrKy8upqalB0zSKiopqlyksLCQpKanBr11cXIWuN/ycAaczisLCygY/71yIthq5dnhn3vr8AEs/2cPIvqm1jxni03Bn76CwcGxYsp1JONfZmUiuhmmuuaD5ZmtJuQwG7Yx/VIe0HF5++eXaf7/99tts2LCBhQsXMmHCBDZt2kS/fv1YtmwZI0aMCGWssBo3oD3bDxbzr+V7OHyskp+MSsduNWFM6YZ77xoidD9oxkaNpoQQorGaxXkOixYtYuHChWRkZOByuZgxY0a4I4WMwaAx56d9GTewPau35HHv39ezYVc+xpTuKJ8H3+4vqP737/HnbA93VCFEK6KpFjJ/w/m4Wen7Dh2t4PWP93Iwr4Jr+ycw6uBi0DRQCmNyVxzX3B/uiEDzWmcnk1wN01xzQfPN1pJynW2zUrMYOYigTinR3PPzSxjZty3LNhZTbU0MFkP7vgTy9xHI3x/uiEKIVkLKoZkxGQ3MGHsBoy5J5d3ibuSmjMJ+xS1gceDZtEym2RBChISUQzOkaRrXXdkVPX0oT+xox0srD3Is7XICOdupWfYwelVJuCMKIVo4KYdmymgwcP/MAYwf3IHMbcdYuDGev1eOwl2cR9mnL4Y7nhCihZMpu5sxo9HAlJFduKJfGtUuH/tyLmBVZjXj8jfg2v4pRhXA1Kk/hsj4cEcVQrQwUg7ngdhIK7GRVlKdkXwbOZWjH+8hJfNV/IA/axP2CffIeRBCiHNKNiudZy7qmsSOdj9hSfVACjtlEDi6B9+2lfhzd6K8rnDHE0K0EFIO56EJGYM5lnApT25NwhPbCc/6/+D63xO4P/u7XHZUCHFOSDmch8wmI7+echGx0XYWHOrHxrgMKjpchv/wZvyHNoY7nhCiBZByOE/FRFh4YOal9L+kG/8+lMT8zWnka05qvnyFQFFWuOMJIc5zUg7nMavFyM9Hd+Nvvx3OrAm9+I9rOOWuAJXvPELxV++jAi33cqtCiKYlRyu1ADaLiSEXptCr41j+93kqXQ6/Tc+tS8nbuhxTm64k9BuDKbUnSg+ApqFp8jeBEOLMpBxakJhIK9dN6EdRaQ/2bF4H+7+kTd4eqo99i/niq9H3foFmsWO7fDbGhPa1z9PdlRDwY4iIC2N6IURzIn9CtkCJcQ76X34FvX85j3WdbiXbH09g8zKKaxTuynJq3nkQz5YPUbqOXlVMzdI/UrPsIZTfG+7oQohmQkYOLZjVbOQnY3uz9+Dv+Xz9Gja70igsLOVXKZtpt2EJvl2rQNdR7moIePHt/gLLhVeGO7YQohmQcmgFunVOoVvnnzBBKT5Yd5hFX1jJSO7EldEFmD3l2C+fjXfj23i3fIAxfTB+gw2rxRju2EKIMJJyaEU0TWPikI6kxDv4xwdGPspvgwZE5RYyzNmHsTVLqHzldrL0FLpcNpHIbpeiGaQkhGiNpBxaof7dk0hLimRPdimllR5KKj18ujvATj2DS+w59DYehC+eo3rL21j7T8acPijckYUQISbl0Eq1iXfQJt5Re3vS8M58uTWPARdezZY9+exY8zlTzLuIW/UcgWN7sfS7Fs0WJRP8CdFKSDkIAOKirFw9rBMAVw7oQLlrGA+tT2NS1FZG7FyFb+cqvMYIrB0uwtrtQgKONIyJHcKcWgjRVKQcRB0GTeMnl6XTp0siX25ty2tH00n0HyPRc4yeB75BHVwHgKnXldgG/hTNZAlzYiHEuSblIOrVrV0s3drFAj0BOJBXzouf7iNQVUJf32Yu2/EJVXvXYkzugqoqRrNFYWzTDQwGNEccxqTOYDCh2aMw2KPD+rMIIRpGykH8YF3axnDf9f1xOqN48pX2/HVHe0ZGZZF0OAcV6SRRd2HZ8j+g7rThWkQcKIUhNgXrkOvQi4+AHsDUdSiaQc7FFKK5kXIQjfLz0d34u8vHxkBPImxmNu4pwOfXMRs1OqVEo1fkE+U+Sue2MfRIVKjSHBJiHBiPfkvNW/NqX8e4dw22UTdjiEwI408jhPg+KQfRKGaTgdsm9669fd3oruzJLmPvkTL25ZQRkdiWpMSuvLsph6VZCkjAaNDo3+FCLrLuIrFLTzpEB/Bkvk71W3OxXJSBZjSjlx9D+T2Y0i5Eed0onxtLr8vRLN8dWeU7+DXeTcuwjb4NY2zbMPz0QrR8Ug7inIiwmbmkm5NLujlPuX/UJWmUVXqIibSwcsMR9uWUsb2yG9VZbmIiLHSO/gnXGj4jduPbwSdYI9AMRvz719e+hm/HJxgS2oPPjTG1J94tH0DAj/uzF3BcM1dO1BOiCYSlHJ566ik++ugjAEaOHMldd91FZmYmCxcuxOPxkJGRwZw5c8IRTZxjSbF2kmLtAFw/9gIA/AGdr3bmszu7lPwSFw/kjsKheQhgwGJ3MG5Aey52uqlWVtxlpbTP/QhVUwZKx7tpGVpMMpbeY/GseYWaZQ+jWSMwxLWlpuelqJh0lKsCfG606GQ4ftlU2a8hRMOEvBwyMzNZs2YN77zzDpqmcdNNN/HBBx+waNEiXn31VVJSUpg9ezarV69m5MiRoY4nQsBkNDC0dwpDe6cAUFLhJreoGl1XrNiQzZLPD7LkpOUt5qH0TU+ke7tYKm2HMETE0j22PamXlFN5YCslRYW0ObaPY9s/RrPHoFzlwSeabeD3olkjMPe4LDjrrNIx97gMgz0G5XOB2YZmjZST+4T4npCXg9Pp5J577sFiCR4b36VLF7KysujQoQPt2rUDYOLEiSxfvlzKoZWIj7YRH20DoE96IvklNezOLiXSbsZhNfH1nkI27i5gw64CrGYjXn8h6qtC0lPbc+hoLAAGdGb0rCa5YgeRFwwjJjGRmqNZ2CMjoSwH7+b3wWgGwLf941Pe3xDfDnPXwSg9gGa2odljIODFn7WZQMEBzD0vx9S5P/i8GGKS8B/ciHfHp1gH/iS4b0QPEMjdAWiY2vVGiJZAU0rVPe4wRLKyspg2bRrXX389hw4dYtGiRUBwdPGPf/yDl156KVzRRDPjD+gUlNaQHB9BjdvHZxuPsOTTvSTHO7j7+kt5ZulWtu0vwq8rdF1h0EBXYDJqXNTVyfUjU+nSJQXd46Jy2xe4PX78RivmgAvvri/wFR2p854GRzQWZ3vch7fXeUwzW0HXieg1jJr9m9BrKgBIzJiNNbUb3sJslNeNMTIOS1J7zLHJeIty8BXn4ejaT/aTiGYvbDuk9+3bx+zZs7n77rsxmUwcOnTolMcbOswvLq5C1xvec05nFIWFlQ1+Xig012zhymUGSoqrABjcI4lLuyWiaaAFAtx27YU4nVFk55Syfmc+pZUekuPs5BZVs+bbo8x5voAI2zZMJgOV1Tq60gAvYCQ5LoN+He3Y7JEkREBalI4zLpIdhRrfZpXT76LhRAXKKXMrOkXUEJGQjLFdb1z/W0TVjjWYOlyMpctAfLtXU/TR86fNrkUloiqLAYWxTTdM6YMh4AOjGWPbCzBEt8G3bQWB4mxQCr0kB0NsG2zDZ6LZIuu8nvLW4Nu3jkDBQdA0zJ36Y2zfB03T0MuOEsjfj6nb0DqXhFVK1f6/5XRGUXC0GOV1YXDEnMPf1I93ts9Y8JK3hpBvDmxJ/08aDBoJCXU/WyeEpRw2bdrEb37zG+677z7Gjx/Phg0bKCoqqn28oKCApKSkcEQT5xGTse5OZrvVxKiLU0+5b8Lgjqz5No/8Mhc+v05MhIXYSCtWs5Eql4/th4r5eFs5Pn9pndczmwx86dc58b+K0RBLl1QbCbuziY//OZFtNXzKTPbWKqKtY5jUvzu5FXDAE8/QizsSQTX2mlzK92zG0HUIWkQ8nvVvEDi297s30QwYYlPQS3PRIhOCt2OS8R/eQnXBHzG1uxAtMgGDI3gZ10D+PnwHvgruT3HEQsCPf+8aDM7OGOJS8B/4CgJ+TEe2Ye4+gkDRYVRVCYH8veiVRVgH/BRzj8vwlRVQvWQuqqoYQ0wbjG17YGzbA0NcCr4dn6J8nmA5ma3n7Hf2YwRKcoKXsjVZqVn2IIboZGxX3ib7i5pIyDcrHT16lEmTJrF48WIGDx4MgMfjYcyYMbzyyiukpaUxe/ZspkyZQkZGxg9+XRk5hE5LzeUP6BSWucgtrCa3qJqUBAeXdHNyMK8Cf0AnJtLKmm/zOJhXQUmFh7IqD4Hjn7mEaCullV7sViPVbj8ANouRXp3iuaR7MpekJ2A1BzclKa8L5XOjGc0orwvv1g/xHdyAbdA0zBcMr80TKDiIZ/0bwXM/XBXfBTVZMacPxNxjFEZnJ5Tux7fzc3z7MtHLj2JKuxBDXCreTcu+e47FXnvd8MDRPRgS2qN5qgh43VguGkeg4ACBo3vA5w4ubzCC0jEkdgQ0lKscc4/LMCano9kigzP0Ht8349v5OZo9ClOXgegV+fj3ZhI4thdjWq/gAQLVpcGDAE66RnmgMAv3mn9h6XMV5s6X1t6vdD/eTe+icrbi97ixjfglprY98O76HM+af2FI7Ig5fTCedf8O/liXTsGY0A5DdDKG2JQG/b5PjKKUrhM4tgf8HkBDL8/HmNoLY3xqneckJjg4tmU9xqQuaNaIBr1f7fv6POhVRRjj6r5+YzXFyCHk5fDII4+wdOlS2rf/7gL306ZNo2PHjrWHso4cOZJ77723QX8RSDmEjuQK0pXC6wugoWG1GNmTXcrrH++l/wVJ9LvAyYfrszmQW05BmYvYSAt90hMxGjQO5FaQEGOjR4c4jhRUER1h5qJOCVS6fGzeX8Tuw6W0TQx+8eSX1HDN8E4M7BaPqimnyu2nyGMmJSmWXYdL2ZdTRkW1l+4d4hjcq80po6nAsX0ovwdjUmc0i4OCMheff3OEq2L3YTi6ExM+jAOmY3QGZ+NVegC9KItA0WFM7S4iUHgI96rnMcQkoTliCeTuPOXn1xyxYDCiqoqDdxhNEPCDpmGIS0UvyfluYWsE1kuuwZjaE704G/eaV4NFpGlYB0/H1KEvmjUS9xcv4z+4AXvH3rhLC1EV+cGCqSnDkNABvfgwoGFs0xXNFok/65vg6xtMWAdMxdzrimCmikI0ezSaxf7d76uqGM+GtzCnD0azR+Fa+VdAA0377mc4wWzDPm4OppQLTrnb8O3blK9/DwwmjGm9MKX2xJDQHkNsCpo95ozfWUop9KLDuFc9h16ej2PyfIyJHc/6OfshWkQ5NBUph9CRXA1TUOnllf/tIDu/Cq8/QOeUaPKKa6io9uKwmnB5/SdOx8BqMdKzQxzHSmoAMBoM5BRWcdnFqThjbfwv8zA1Hn/ta5tNBuxWExXVXhKirYwb2IFhvVNqL/OadayCVd/kMqBHEq+t3EtBqYuuaTH8/md9aZsSQ+bmHArLXNgsRtolR+GMsZ3yBae8xw/31TT0igL0yiKUpwpVU0Hg6G6UqwJLv2tRPjeBnO0YEtpjancRhsh49OrS4H4VPYBr9Yvo+ftrX9cQ0wbbmN/gWfOv4IjlJNaBPyP1yp9SkFuAd/P7KHcVhtgUzL1H48n8N76dn+G4+j4M8an49q7FENsW345P8B/eDBYHmtFUO9LS7NEYYtqgRScTyN6CclcCGpitaNYIjMnpKJ8bc9chGKKcoAfA6sD98VPoFQWY2vdFi0oEbw1YI/B9uxxT+mA0WxT+7K2oivyTgkdgvmA4xoT2+A9tDJanpuE/vCX4ugE/ylMVLDvdjzGxA/ar7kRV5BM4tg8MRgyJHYObGIuz8e1fjymtV/B2SS7KVU7g2F78x/Zi6XE55t5jwFuDZo2QcjgTKYfQkVwNc3KuE5syArpOSYWHhBgblTU+DuSWExtpJdUZUbv5CcDn13lt5R4ytx8joCu6psVw+SVp5JfU0DElml6d4jBoGtsOFvNB5mH255ZjtRjp381Jz47x/PuTvbWbucwmA2MHtOd/mVlEOsw4bGbyj5fQCRazgfgoG1cN6sDQ3m3QNA1/QEcphdnU+COslFKo8mME8vdjiE3BkNgx+CWu6+jF2QSKssDrQotJwtyxX72/S6X04D6SKGed1w/k7sC3fx0EAhhTLkB5q1Hl+ejl+ejlx9AcsdhG3oh32wr0osPYx83BEJV42ry6uxLv5g/w71+H8rrRLDaUqwJrSjrmq+5GO35YtF5dil6ad/wggH34D24ApdAi4oLFqvsxpfVGs0cBWrA8O1+K/8BXeDJfR4tJRpXnn/Lemj0a5arkdBNYYrZhiGmDXpSFZotCuSuxX/UHUi4eLOVQHymH0JFcDXMucvn8AYrK3STHOTAYTr/pQinF/txy1nx7lK93F+D2BoiLsvLbn/Rhx6ES2idH0rNjPJv3FbJ5XxFev6JXx1guaBdLtdtP1rFK8ktq2J9bzsG8CpLjHbjcPipqfBg0jXZJkaSnxhAfY6Wg1IUz1k60w8L6ncdonxzFtcM64fYG2J9bTm5hFTaLieR4B53bRlPt8hFhNxNpN9fmzc6vxGg0kBRrx2w69eACpzOKo8eCOdKcEThs5u//uCFx4utR0zR0VwXOlESKy7z1Lq+XH0OvKcfYpmvwu10FaovklNcN+Kl5bwEA5q5DMab2BKUIFOwnkLsLQ2Q85t5jCeTtQnmqgpuuHHFoEbFgMOHbtpJAwX6MbXtg7j6SpORYKYf6SDmEjuRqmHDk8voCbDtYQofkSBJj7addpr5cuq5Y+fUR9mSXEhNpJS7KSkDXOZBbwcG8Cjy+ABE2U+2IJC7KSmmlB7vViMsTqDeT0aDR7wInI/u05Zu9RXz6TXCfRKTdzMShHUFBQakLHUV5jY/dh0qo8fhJdUbw26l9OHS0gqQ4O+2ToyipcOML6Dhj7PWW5dnkl9QQG2U9ZaR2Ni3pM9YsD2UVQjQti9lIvwucZ1/wNAwGjXED2zNuYPs6jwV0HZcnQKTdTEmFm9JKD53aRrPzUAnrdhwjzRlJ17RY2iVH4vPrHM6vJDu/kii7hSMFVazddpQNuwoAuLJfGp3aRrN6Sx7/+WQfEDwU2aBBcnwEl3Rz0i45kqWfH+DOZzMB0IAObaLIOhb8IjRoGmaTgVRnBEMubEPvzgkkHt9vsisrWC79Lqh7WPzOrBL+9OYWEmNszBzXnR4d4/EHdCprfMRFBQ/dPZhXwbtrDpEcZ2fqZV2wfK9ElFJ4fXrt/p2WRkYOzfQvAWi+2SRXw0iu73h9ATbuKcBkNDCgRzIQ/JLNOlZJTISldhqVk7PtyS5l7fZjDOiexM7DpWw/WEK/C5zER1spLHPh8ersPFxCbmE1ACkJDjq0iWL9juC2/IE9kzEbDfgDOv0uSMJqMfDC+zuDm6qUIr/UxeBeyWQdq+RocQ3paTH4/TpZxypxWE3UePykJDjof0ESY4Z0woLi9Y/38PXuAlyeAKnOCC6/OJVRl6QBUFTm4v3MLPZkl2E0atw4vieaBiUVHi7umtigkU5OQRUKaJcU/Avf4w3wxdY8Lu6aeMqIUHZIn4GUQ+hIroaRXA3X0GxKKfKKa9iVVcKGXQXszy3nsotTibSb+V9mFg6bCU3TqHL5gOBRYX+8oT8J0TbeXXOI5RuySYi2MahXGzbtKcBmMdG/u5PL+qayP7ect1cfJLugEg1IjndwtLiGob3b4Iyxs+1gMQfyKhg/uAO6Uny6MQc0uLBTAoePVVJc4a7N2T45EofVRGmVl+EXpXBhp3hiIq3ERFjq/DyfbMphyar9aBrMuqoH8dE2Xlu5h5zCaqwWI9cO68Twi1Jw2MxSDmci5RA6kqthJFfDnYsTGk+c81Hj9mGzmNCV4kBuObqCtgkOYiK/O/O7pMJNlMN8xiOyqlw+Vm7K4ZMN2fx8dLfaWYV1XfHi/3axbscxNA0u7Z7ETy5LJyHGRrXbx/KvsomPtmGzGHnni4PYLEYcNjN7j5TVvnZCtJUuqTF0bBNNhM3El9uOsj+nnL7piVS5fezPCc40HGEzMf3KrqzbfowdWaVYTAbu/UU/+vduK+VQHymH0JFcDSO5Gq65ZnM6oygoqKhzspuuKzbsyic9NabeAwC+L7eommPFNRSXuziQV8GBvHJKKjwARDvMTB7ZhWEXpeD3B69/Emk30yUthmiHBaUUh/Mr2XagmBF92pLeKVF2SAshRDid7ixog0FjUK82DXqd1MQIUhNPnYKjyuWjxu0jJvK7o6gsZiPD+5x6OVxN0+jYJpqObaIbmP6Hk3IQQohmIvJ754KEk1w7UQghRB1SDkIIIeqQchBCCFGHlIMQQog6pByEEELUIeUghBCijhZzKGtjZ2b8sc9tas01m+RqGMnVcM01W0vJdbblW8wZ0kIIIc4d2awkhBCiDikHIYQQdUg5CCGEqEPKQQghRB1SDkIIIeqQchBCCFGHlIMQQog6pByEEELUIeUghBCijhYzfUZjvP/++zz77LP4fD5mzpzJz3/+87Bleeqpp/joo48AGDlyJHfddRf33nsvmzZtwm4PXpP217/+NaNHjw5prhkzZlBcXIzJFPyoPPTQQ2RnZ4d1vf33v//ltddeq72dk5PDNddcg8vlCtv6qqqqYtq0aTz33HOkpaWRmZnJwoUL8Xg8ZGRkMGfOHAB27drF3Llzqaqqon///jz44IO16zZU2d58801effVVNE3jwgsv5MEHH8RisfDUU0+xdOlSoqODl5786U9/2qS/2+/nqu/zXt+6DEWuAwcO8Oc//7n2sfz8fPr06cPzzz8f0vV1uu+HJv+MqVbq2LFjatSoUaq0tFRVV1eriRMnqn379oUly9q1a9XPfvYz5fF4lNfrVTNmzFArV65UEyZMUPn5+WHJpJRSuq6roUOHKp/PV3tfc1pvSim1d+9eNXr0aFVcXBy29bVlyxY1YcIE1atXL3XkyBHlcrnUyJEjVXZ2tvL5fGrWrFnq888/V0opNX78eLV582allFL33nuvev3110Oa7eDBg2r06NGqsrJS6bqu7rrrLvXyyy8rpZSaPXu2+uabb5o0T325lFKn/f2daV2GKtcJBQUF6oorrlCHDh1SSoVufZ3u++H9999v8s9Yq92slJmZyaBBg4iNjcXhcDB27FiWL18elixOp5N77rkHi8WC2WymS5cu5OXlkZeXx7x585g4cSJ//etf0XU9pLkOHjyIpmn83//9H1dffTWvvfZas1pvAA888ABz5szBZrOFbX0tWbKE+fPnk5SUBMC3335Lhw4daNeuHSaTiYkTJ7J8+XJyc3Nxu9307dsXgMmTJzf5uvt+NovFwgMPPEBkZCSaptGtWzfy8vIA2L59Oy+88AITJ07koYcewuPxhCxXTU3NaX9/9a3LUOU62RNPPMG0adPo2LEjELr1dbrvh6ysrCb/jLXacigoKMDpdNbeTkpKIj8/PyxZunbtWvvLzMrK4sMPP2T48OEMGjSIRx99lCVLlrBx40beeuutkOaqqKhg8ODBPP300/zzn//kjTfeIC8vr9mst8zMTNxuNxkZGRQXF4dtfS1YsID+/fvX3q7vs/X9+51OZ5Ovu+9nS01NZciQIQCUlJTw+uuvc8UVV1BdXU2PHj24++67eeedd6ioqOCZZ54JWa76fn+h/v/0+7lOyMrKYsOGDcyYMQMgpOvrdN8PmqY1+Wes1ZaDOs1ktJoW3ql49+3bx6xZs7j77rvp3LkzTz/9NAkJCdjtdq6//npWr14d0jwXX3wxTzzxBA6Hg/j4eKZOncpf//rXOsuFa7298cYb/PKXvwSgXbt2YV9fJ9T32WpOn7n8/HxuuOEGpkyZwsCBA4mIiOCFF16gQ4cOmEwmZs2aFdL1V9/vr7msszfffJPrrrsOi8UCEJb1dfL3Q/v27es8fq4/Y622HJKTkykqKqq9XVBQcNqhZKhs2rSJmTNn8vvf/55JkyaxZ88eVqxYUfu4UqrJd1x+38aNG1m3bt0pGVJTU5vFevN6vXz99ddcfvnlAM1ifZ1Q32fr+/cXFhaGZd0dOHCA6dOnM2nSJG677TYA8vLyThlphXr91ff7ay7/n3766adcddVVtbdDvb6+//0Qis9Yqy2HIUOGsG7dOkpKSnC5XKxcuZIRI0aEJcvRo0e57bbbWLRoEePHjweCH7ZHH32U8vJyfD4fb775ZsiPVKqsrOSJJ57A4/FQVVXFO++8w5NPPtks1tuePXvo2LEjDocDaB7r64Q+ffpw6NAhDh8+TCAQ4IMPPmDEiBGkpqZitVrZtGkTAMuWLQv5uquqquLGG2/kjjvuYNasWbX322w2nnzySY4cOYJSitdffz2k66++31996zKUSkpKcLvdtGvXrva+UK6v030/hOIz1moPZU1OTmbOnDnMmDEDn8/H1KlTueiii8KS5cUXX8Tj8fDYY4/V3jdt2jRuvvlmpk+fjt/vZ8yYMUyYMCGkuUaNGsXWrVu59tpr0XWd6667jn79+jWL9XbkyBHatGlTe7t79+5hX18nWK1WHnvsMW6//XY8Hg8jR45k3LhxACxatIi5c+dSXV1Nz549a7dhh8pbb71FUVERL730Ei+99BIAl19+OXfccQcPPfQQv/rVr/D5fFxyySW1m+xC4Uy/v/rWZajk5OSc8lkDiI+PD9n6qu/7oak/Y3IlOCGEEHW02s1KQggh6iflIIQQog4pByGEEHVIOQghhKhDykEIIUQdUg5CNANfffVV2A69FeJ0pByEEELU0WpPghOiIVatWlV7DQubzcbdd9/NmjVr2LdvH0VFRRQXF9O9e3cWLFhAZGQk+/bt46GHHqKsrAxN05g1axbXXnstEDwR7eWXX8ZgMBAXF8fjjz8OBGcmnTNnDgcPHsTj8fDII4+cdhI4IUKicTOMC9F6HDp0SE2YMEGVlJQopYLXkBg6dKh67LHH1IgRI1RhYaEKBALqd7/7nXrssceUz+dTV1xxhVqxYoVSKngNjOHDh6tvvvlG7dq1Sw0cOFDl5eUppZR6+eWX1bx589T69etVjx491JYtW2rvnzFjRnh+YCGUUjJyEOIs1q5dS0FBATNnzqy9T9M0srOzGTduHImJiQBMnTqVRx99lClTpuDxeBgzZgwQnKplzJgxfPnll0RFRTFs2DBSUlIAal/zq6++ol27dvTp0wcITiexdOnS0P2QQnyPlIMQZ6HrOoMHD+Yvf/lL7X1Hjx7lzTffxOv1nrKcwWA47UWGlFL4/X6MRuMpUyi73W5yc3MBMJvNtffXN/2yEKEiO6SFOItBgwaxdu1aDhw4AMDq1au5+uqr8Xg8fPrpp1RWVqLrOkuWLGHUqFF06tQJs9nMypUrgeC1E1asWMGQIUMYOHAg69ato6CgAAhek+LJJ58M288mRH1k5CDEWXTt2pWHHnqI3/3ud7Xz9j/77LOsW7eOxMRE/u///o/S0lIuvfRSbrnlFsxmM8888wyPPPIIf/vb3wgEAtx2220MGjQIgDvvvJObbroJCF6p69FHHyUrKyuMP6EQdcmsrEI00t/+9jdKS0v54x//GO4oQpxzsllJCCFEHTJyEEIIUYeMHIQQQtQh5SCEEKIOKQchhBB1SDkIIYSoQ8pBCCFEHVIOQggh6vh/0WBbyTDBx9cAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dat = dataset(met,aqi,split_aqi)\n",
    "df = dat.mergedData('DL031',roll=48,shift=24)\n",
    "Xtrain,ytrain,Xtest,ytest = getSplitFeaturesTimeSeries(df,TIME_SERIES_LENGTH = 24)\n",
    "model,history = trainModel1(Xtrain,ytrain,Xtest,ytest,TIME_SERIES_LENGTH=24)\n",
    "model.save('1daypm1_366')\n",
    "\n",
    "# reconstructed_model = keras.models.load_model(\"2daypm1_366\")\n",
    "# predictStats(reconstructed_model,Xtrain,ytrain,Xtest,ytest)\n",
    "\n",
    "predictStats(model,Xtrain,ytrain,Xtest,ytest)\n",
    "\n",
    "plothistory(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run below for non time series model (rolling mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PM2.5 101.89062177480847 82.2323519352033\n",
      "PM10 215.52704148344912 140.97411211440493\n",
      "NO 43.47371008555342 83.06738197153933\n",
      "NO2 58.66296373444158 40.64878421077701\n",
      "CO 1.5791442652531407 2.3089151644232633\n",
      "AQI 224.23316853135833 111.67767090442183\n",
      "features length 10\n",
      "(8503, 46) (8503, 1) (4189, 46) (4189, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\confusement\\miniconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51.55932984050624\n",
      "50.435895807351066\n",
      "32.92133170715936\n",
      "30.093099655909757\n"
     ]
    }
   ],
   "source": [
    "# dat = dataset(met,aqi,split_aqi)\n",
    "# df = dat.mergedData('DL031',roll=48,shift=48)\n",
    "Xtrain,ytrain,Xtest,ytest = getSplitFeatures(df)\n",
    "print(Xtrain.shape,ytrain.shape,Xtest.shape,ytest.shape)\n",
    "reg = SVR(C=100, epsilon=0.2).fit(Xtrain, ytrain)\n",
    "mean_squared_error(reg.predict(Xtest), ytest, squared=False)\n",
    "predictStats(reg,Xtrain,ytrain,Xtest,ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif, f_regression , mutual_info_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.svm import SVR\n",
    "dfTrain = df[:]\n",
    "features = ['Temperature','Relative Humidity','windX','windY','Year','MonthX','MonthY','hourX','hourY','isWeekend']\n",
    "features.append(\"PM2.5_lag1\")\n",
    "features.append(\"PM2.5_lag2\")\n",
    "\n",
    "features.append(\"PM10_lag1\")\n",
    "features.append(\"PM10_lag2\")\n",
    "\n",
    "features.append(\"NO2_lag1\")\n",
    "features.append(\"NO2_lag2\")\n",
    "\n",
    "features.append(\"SO2_lag1\")\n",
    "features.append(\"SO2_lag2\")\n",
    "\n",
    "features.append(\"NO_lag1\")\n",
    "features.append(\"NO_lag2\")\n",
    "\n",
    "features.append(\"NOx_lag1\")\n",
    "features.append(\"NOx_lag2\")\n",
    "\n",
    "# features.append(\"CO_lag1\")\n",
    "# features.append(\"CO_lag2\")\n",
    "\n",
    "# features.append(\"O3_lag1\")\n",
    "# features.append(\"O3_lag2\")\n",
    "\n",
    "# features.append(\"NH3_lag1\")\n",
    "# features.append(\"NH3_lag2\")\n",
    "\n",
    "# features = []\n",
    "# rlist=['PM2.5','PM10','NO','NO2','CO']\n",
    "# newlist = rlist + ['Temperature','Relative Humidity','windX','windY','Year','MonthX','MonthY','hourX','hourY','isWeekend']\n",
    "# for i in newlist:\n",
    "#     for j in range(24):\n",
    "#         features.append(i+'_t-'+str(j))\n",
    "        \n",
    "X = df[features]\n",
    "y = df['PM2.5_pred3']\n",
    "scaler = StandardScaler()\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "scaler.fit(Xtrain)\n",
    "\n",
    "reg = LinearRegression().fit(scaler.transform(Xtrain), ytrain)\n",
    "\n",
    "testPred = reg.predict(scaler.transform(Xtest))\n",
    "trainPred = reg.predict(scaler.transform(Xtrain))\n",
    "mse = np.mean((testPred - np.array(ytest))*(testPred - np.array(ytest)))\n",
    "print(reg.score(scaler.transform(Xtest), ytest))\n",
    "print(mean_squared_error(testPred, ytest,squared=False))\n",
    "print(mean_squared_error(trainPred, ytrain,squared=False))\n",
    "print(mean_absolute_error(testPred, ytest))\n",
    "print(mean_absolute_error(trainPred, ytrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif, f_regression , mutual_info_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "dfTrain = df[:]\n",
    "features = ['Temperature','Relative Humidity','windX','windY','Year','MonthX','MonthY','hourX','hourY','isWeekend']\n",
    "features.append(\"PM2.5_lag1\")\n",
    "features.append(\"PM2.5_lag2\")\n",
    "\n",
    "features.append(\"PM10_lag1\")\n",
    "features.append(\"PM10_lag2\")\n",
    "\n",
    "features.append(\"NO2_lag1\")\n",
    "features.append(\"NO2_lag2\")\n",
    "\n",
    "features.append(\"SO2_lag1\")\n",
    "features.append(\"SO2_lag2\")\n",
    "\n",
    "features.append(\"NO_lag1\")\n",
    "features.append(\"NO_lag2\")\n",
    "\n",
    "features.append(\"NOx_lag1\")\n",
    "features.append(\"NOx_lag2\")\n",
    "\n",
    "features.append(\"CO_lag1\")\n",
    "features.append(\"CO_lag2\")\n",
    "\n",
    "features.append(\"O3_lag1\")\n",
    "features.append(\"O3_lag2\")\n",
    "\n",
    "features.append(\"NH3_lag1\")\n",
    "features.append(\"NH3_lag2\")\n",
    "\n",
    "features = []\n",
    "rlist=['PM2.5','PM10','NO','NO2','CO']\n",
    "newlist = rlist + ['Temperature','Relative Humidity','windX','windY','Year','MonthX','MonthY','hourX','hourY','isWeekend']\n",
    "for i in newlist:\n",
    "    for j in range(24):\n",
    "        features.append(i+'_t-'+str(j))\n",
    "        \n",
    "X = df[features]\n",
    "y = df['PM2.5_pred1']\n",
    "scaler = StandardScaler()\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "scaler.fit(Xtrain)\n",
    "\n",
    "reg = MLPRegressor(random_state=1, max_iter=100).fit(scaler.transform(Xtrain), ytrain)\n",
    "\n",
    "testPred = reg.predict(scaler.transform(Xtest))\n",
    "trainPred = reg.predict(scaler.transform(Xtrain))\n",
    "mse = np.mean((testPred - np.array(ytest))*(testPred - np.array(ytest)))\n",
    "print(reg.score(scaler.transform(Xtest), ytest))\n",
    "print(mean_squared_error(testPred, ytest,squared=False))\n",
    "print(mean_squared_error(trainPred, ytrain,squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7769, 360) (7769,)\n",
      "0.3379685723175532\n",
      "65.78093011534429\n",
      "67.0801815314519\n",
      "43.41983467544137\n",
      "43.25189223855267\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif, f_regression , mutual_info_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.svm import SVR\n",
    "dfTrain = df[:]\n",
    "features = ['Temperature','Relative Humidity','windX','windY','Year','MonthX','MonthY','hourX','hourY','isWeekend']\n",
    "features.append(\"PM2.5_lag1\")\n",
    "features.append(\"PM2.5_lag2\")\n",
    "\n",
    "features.append(\"PM10_lag1\")\n",
    "features.append(\"PM10_lag2\")\n",
    "\n",
    "features.append(\"NO2_lag1\")\n",
    "features.append(\"NO2_lag2\")\n",
    "\n",
    "features.append(\"SO2_lag1\")\n",
    "features.append(\"SO2_lag2\")\n",
    "\n",
    "features.append(\"NO_lag1\")\n",
    "features.append(\"NO_lag2\")\n",
    "\n",
    "features.append(\"NOx_lag1\")\n",
    "features.append(\"NOx_lag2\")\n",
    "\n",
    "features.append(\"CO_lag1\")\n",
    "features.append(\"CO_lag2\")\n",
    "\n",
    "features.append(\"O3_lag1\")\n",
    "features.append(\"O3_lag2\")\n",
    "\n",
    "features.append(\"NH3_lag1\")\n",
    "features.append(\"NH3_lag2\")\n",
    "\n",
    "features = []\n",
    "rlist=['PM2.5','PM10','NO','NO2','CO']\n",
    "newlist = rlist + ['Temperature','Relative Humidity','windX','windY','Year','MonthX','MonthY','hourX','hourY','isWeekend']\n",
    "for i in newlist:\n",
    "    for j in range(24):\n",
    "        features.append(i+'_t-'+str(j))\n",
    "\n",
    "X = df[features]\n",
    "y = df['PM2.5_pred1']\n",
    "scaler = StandardScaler()\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "print(Xtrain.shape,ytrain.shape)\n",
    "# scaler.fit(Xtrain)\n",
    "\n",
    "reg = SVR(C=3.0, epsilon=0.2).fit(Xtrain, ytrain)\n",
    "\n",
    "testPred = reg.predict(Xtest)\n",
    "trainPred = reg.predict(Xtrain)\n",
    "mse = np.mean((testPred - np.array(ytest))*(testPred - np.array(ytest)))\n",
    "print(reg.score(Xtest, ytest))\n",
    "print(mean_squared_error(testPred, ytest,squared=False))\n",
    "print(mean_squared_error(trainPred, ytrain,squared=False))\n",
    "print(mean_absolute_error(testPred, ytest))\n",
    "print(mean_absolute_error(trainPred, ytrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf  \n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "features = ['Temperature','Relative Humidity','windX','windY','Year','MonthX','MonthY','hourX','hourY','isWeekend']\n",
    "features = []\n",
    "rlist=['PM2.5','PM10','NO','NO2','CO']\n",
    "newlist = rlist + ['Temperature','Relative Humidity','windX','windY','Year','MonthX','MonthY','hourX','hourY','isWeekend']\n",
    "for i in newlist:\n",
    "    for j in range(24):\n",
    "        features.append(i+'_t-'+str(j))\n",
    "predVector = []\n",
    "for j in range(24):\n",
    "    predVector.append('PM2.5_t+'+str(j))\n",
    "X = df[features]\n",
    "y = df[predVector]\n",
    "scaler = StandardScaler()\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "scaler.fit(Xtrain)\n",
    "print(Xtrain.shape)\n",
    "model = Sequential()\n",
    "model.add(Dense(200, input_dim=360, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(24, activation='linear'))\n",
    "model.summary()\n",
    "#Fit\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['mse','mae'])\n",
    "history = model.fit(scaler.transform(Xtrain), ytrain, epochs=100, batch_size=50,  verbose=1, validation_split=0.2)\n",
    "#Print Accuracy\n",
    "testPred = model.predict(scaler.transform(Xtest))\n",
    "trainPred = model.predict(scaler.transform(Xtrain))\n",
    "print(mean_squared_error(testPred, ytest,squared=False))\n",
    "print(mean_squared_error(trainPred, ytrain,squared=False))\n",
    "print(mean_absolute_error(testPred, ytest))\n",
    "print(mean_absolute_error(trainPred, ytrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for loss\n",
    "plt.plot(np.sqrt(history.history['loss']))\n",
    "plt.plot(np.sqrt(history.history['val_loss']))\n",
    "plt.title('model loss')\n",
    "plt.ylabel('RMSE loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
