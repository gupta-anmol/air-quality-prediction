{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "RKP = \"DL031\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "sns.set_theme(style=\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\confusement\\miniconda3\\envs\\mlc\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3263: DtypeWarning: Columns (15) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['StationId', 'Datetime', 'PM2.5', 'PM10', 'NO', 'NO2', 'NOx', 'NH3',\n",
      "       'CO', 'SO2', 'O3', 'Benzene', 'Toluene', 'Xylene', 'AQI', 'AQI_Bucket'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Load datasets and rename columns, load all aqi data but specify metro data name\n",
    "def loadcsv(city=\"./data/rkpuram.csv\"):\n",
    "    met = pd.read_csv(city,delimiter=';',skiprows=24)\n",
    "    aqi = pd.read_csv('./data/station_hour.csv')\n",
    "    print(aqi.columns)\n",
    "    met.rename(columns={'# Date': 'Date',}, inplace=True)\n",
    "    met.rename(columns={'UT time': 'Time',}, inplace=True)\n",
    "    aqi['Time'] = aqi['Datetime'].str[-8:-3]\n",
    "    aqi['Date'] = aqi['Datetime'].str[0:10]\n",
    "    stations = [\"DL\"+str(x).zfill(3) for x in range(1,39)]\n",
    "    split_aqi = {}\n",
    "    for i in range(len(stations)):\n",
    "        split_aqi[stations[i]] = (aqi[aqi['StationId'] == stations[i]])\n",
    "    return met,aqi,split_aqi\n",
    "met,aqi,split_aqi = loadcsv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged Dataset Size 44035\n",
      "Size before roll 44035\n",
      "Size after roll 14861\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PM2.5</th>\n",
       "      <th>PM10</th>\n",
       "      <th>NO</th>\n",
       "      <th>NO2</th>\n",
       "      <th>NOx</th>\n",
       "      <th>NH3</th>\n",
       "      <th>CO</th>\n",
       "      <th>SO2</th>\n",
       "      <th>O3</th>\n",
       "      <th>AQI</th>\n",
       "      <th>...</th>\n",
       "      <th>O3_t-19</th>\n",
       "      <th>O3_t+19</th>\n",
       "      <th>O3_t-20</th>\n",
       "      <th>O3_t+20</th>\n",
       "      <th>O3_t-21</th>\n",
       "      <th>O3_t+21</th>\n",
       "      <th>O3_t-22</th>\n",
       "      <th>O3_t+22</th>\n",
       "      <th>O3_t-23</th>\n",
       "      <th>O3_t+23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>33.00</td>\n",
       "      <td>102.17</td>\n",
       "      <td>6.33</td>\n",
       "      <td>14.45</td>\n",
       "      <td>22.22</td>\n",
       "      <td>23.65</td>\n",
       "      <td>0.17</td>\n",
       "      <td>10.27</td>\n",
       "      <td>18.08</td>\n",
       "      <td>429.0</td>\n",
       "      <td>...</td>\n",
       "      <td>26.83</td>\n",
       "      <td>39.17</td>\n",
       "      <td>19.83</td>\n",
       "      <td>34.33</td>\n",
       "      <td>18.75</td>\n",
       "      <td>32.58</td>\n",
       "      <td>19.58</td>\n",
       "      <td>32.58</td>\n",
       "      <td>21.33</td>\n",
       "      <td>34.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>95.17</td>\n",
       "      <td>184.83</td>\n",
       "      <td>6.72</td>\n",
       "      <td>15.63</td>\n",
       "      <td>23.86</td>\n",
       "      <td>22.06</td>\n",
       "      <td>20.00</td>\n",
       "      <td>11.97</td>\n",
       "      <td>18.58</td>\n",
       "      <td>429.0</td>\n",
       "      <td>...</td>\n",
       "      <td>35.33</td>\n",
       "      <td>34.33</td>\n",
       "      <td>26.83</td>\n",
       "      <td>32.58</td>\n",
       "      <td>19.83</td>\n",
       "      <td>32.58</td>\n",
       "      <td>18.75</td>\n",
       "      <td>34.42</td>\n",
       "      <td>19.58</td>\n",
       "      <td>34.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>77.50</td>\n",
       "      <td>164.67</td>\n",
       "      <td>8.15</td>\n",
       "      <td>32.28</td>\n",
       "      <td>41.18</td>\n",
       "      <td>26.91</td>\n",
       "      <td>8.00</td>\n",
       "      <td>17.28</td>\n",
       "      <td>20.33</td>\n",
       "      <td>318.0</td>\n",
       "      <td>...</td>\n",
       "      <td>43.58</td>\n",
       "      <td>32.58</td>\n",
       "      <td>35.33</td>\n",
       "      <td>32.58</td>\n",
       "      <td>26.83</td>\n",
       "      <td>34.42</td>\n",
       "      <td>19.83</td>\n",
       "      <td>34.75</td>\n",
       "      <td>18.75</td>\n",
       "      <td>38.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>65.50</td>\n",
       "      <td>154.33</td>\n",
       "      <td>7.64</td>\n",
       "      <td>90.45</td>\n",
       "      <td>94.00</td>\n",
       "      <td>30.74</td>\n",
       "      <td>17.00</td>\n",
       "      <td>18.81</td>\n",
       "      <td>29.83</td>\n",
       "      <td>318.0</td>\n",
       "      <td>...</td>\n",
       "      <td>52.17</td>\n",
       "      <td>32.58</td>\n",
       "      <td>43.58</td>\n",
       "      <td>34.42</td>\n",
       "      <td>35.33</td>\n",
       "      <td>34.75</td>\n",
       "      <td>26.83</td>\n",
       "      <td>38.33</td>\n",
       "      <td>19.83</td>\n",
       "      <td>46.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>60.50</td>\n",
       "      <td>310.17</td>\n",
       "      <td>14.19</td>\n",
       "      <td>116.38</td>\n",
       "      <td>121.56</td>\n",
       "      <td>28.73</td>\n",
       "      <td>15.00</td>\n",
       "      <td>15.16</td>\n",
       "      <td>33.33</td>\n",
       "      <td>318.0</td>\n",
       "      <td>...</td>\n",
       "      <td>59.83</td>\n",
       "      <td>34.75</td>\n",
       "      <td>56.42</td>\n",
       "      <td>38.33</td>\n",
       "      <td>52.17</td>\n",
       "      <td>46.50</td>\n",
       "      <td>43.58</td>\n",
       "      <td>55.92</td>\n",
       "      <td>35.33</td>\n",
       "      <td>56.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43959</th>\n",
       "      <td>45.00</td>\n",
       "      <td>173.00</td>\n",
       "      <td>1.97</td>\n",
       "      <td>10.47</td>\n",
       "      <td>7.20</td>\n",
       "      <td>26.23</td>\n",
       "      <td>0.60</td>\n",
       "      <td>14.25</td>\n",
       "      <td>8.98</td>\n",
       "      <td>96.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.30</td>\n",
       "      <td>8.80</td>\n",
       "      <td>6.05</td>\n",
       "      <td>11.10</td>\n",
       "      <td>11.80</td>\n",
       "      <td>12.57</td>\n",
       "      <td>10.88</td>\n",
       "      <td>12.40</td>\n",
       "      <td>10.25</td>\n",
       "      <td>14.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43960</th>\n",
       "      <td>66.00</td>\n",
       "      <td>250.00</td>\n",
       "      <td>2.55</td>\n",
       "      <td>19.05</td>\n",
       "      <td>12.20</td>\n",
       "      <td>23.60</td>\n",
       "      <td>0.65</td>\n",
       "      <td>14.43</td>\n",
       "      <td>7.52</td>\n",
       "      <td>102.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.30</td>\n",
       "      <td>11.10</td>\n",
       "      <td>7.30</td>\n",
       "      <td>12.57</td>\n",
       "      <td>6.05</td>\n",
       "      <td>12.40</td>\n",
       "      <td>11.80</td>\n",
       "      <td>14.65</td>\n",
       "      <td>10.88</td>\n",
       "      <td>11.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43961</th>\n",
       "      <td>56.00</td>\n",
       "      <td>183.00</td>\n",
       "      <td>3.25</td>\n",
       "      <td>28.25</td>\n",
       "      <td>17.70</td>\n",
       "      <td>28.57</td>\n",
       "      <td>0.80</td>\n",
       "      <td>18.93</td>\n",
       "      <td>10.67</td>\n",
       "      <td>105.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.10</td>\n",
       "      <td>12.57</td>\n",
       "      <td>7.30</td>\n",
       "      <td>12.40</td>\n",
       "      <td>7.30</td>\n",
       "      <td>14.65</td>\n",
       "      <td>6.05</td>\n",
       "      <td>11.53</td>\n",
       "      <td>11.80</td>\n",
       "      <td>10.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43962</th>\n",
       "      <td>63.00</td>\n",
       "      <td>190.00</td>\n",
       "      <td>4.03</td>\n",
       "      <td>44.83</td>\n",
       "      <td>27.15</td>\n",
       "      <td>38.50</td>\n",
       "      <td>0.82</td>\n",
       "      <td>18.10</td>\n",
       "      <td>7.38</td>\n",
       "      <td>108.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.43</td>\n",
       "      <td>12.40</td>\n",
       "      <td>8.10</td>\n",
       "      <td>14.65</td>\n",
       "      <td>7.30</td>\n",
       "      <td>11.53</td>\n",
       "      <td>7.30</td>\n",
       "      <td>10.25</td>\n",
       "      <td>6.05</td>\n",
       "      <td>10.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43963</th>\n",
       "      <td>55.00</td>\n",
       "      <td>177.00</td>\n",
       "      <td>4.90</td>\n",
       "      <td>52.40</td>\n",
       "      <td>31.88</td>\n",
       "      <td>57.32</td>\n",
       "      <td>1.17</td>\n",
       "      <td>20.85</td>\n",
       "      <td>8.20</td>\n",
       "      <td>110.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.85</td>\n",
       "      <td>14.65</td>\n",
       "      <td>9.43</td>\n",
       "      <td>11.53</td>\n",
       "      <td>8.10</td>\n",
       "      <td>10.25</td>\n",
       "      <td>7.30</td>\n",
       "      <td>10.05</td>\n",
       "      <td>7.30</td>\n",
       "      <td>10.57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14861 rows × 497 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       PM2.5    PM10     NO     NO2     NOx    NH3     CO    SO2     O3  \\\n",
       "165    33.00  102.17   6.33   14.45   22.22  23.65   0.17  10.27  18.08   \n",
       "166    95.17  184.83   6.72   15.63   23.86  22.06  20.00  11.97  18.58   \n",
       "167    77.50  164.67   8.15   32.28   41.18  26.91   8.00  17.28  20.33   \n",
       "168    65.50  154.33   7.64   90.45   94.00  30.74  17.00  18.81  29.83   \n",
       "170    60.50  310.17  14.19  116.38  121.56  28.73  15.00  15.16  33.33   \n",
       "...      ...     ...    ...     ...     ...    ...    ...    ...    ...   \n",
       "43959  45.00  173.00   1.97   10.47    7.20  26.23   0.60  14.25   8.98   \n",
       "43960  66.00  250.00   2.55   19.05   12.20  23.60   0.65  14.43   7.52   \n",
       "43961  56.00  183.00   3.25   28.25   17.70  28.57   0.80  18.93  10.67   \n",
       "43962  63.00  190.00   4.03   44.83   27.15  38.50   0.82  18.10   7.38   \n",
       "43963  55.00  177.00   4.90   52.40   31.88  57.32   1.17  20.85   8.20   \n",
       "\n",
       "         AQI  ...  O3_t-19  O3_t+19  O3_t-20  O3_t+20  O3_t-21  O3_t+21  \\\n",
       "165    429.0  ...    26.83    39.17    19.83    34.33    18.75    32.58   \n",
       "166    429.0  ...    35.33    34.33    26.83    32.58    19.83    32.58   \n",
       "167    318.0  ...    43.58    32.58    35.33    32.58    26.83    34.42   \n",
       "168    318.0  ...    52.17    32.58    43.58    34.42    35.33    34.75   \n",
       "170    318.0  ...    59.83    34.75    56.42    38.33    52.17    46.50   \n",
       "...      ...  ...      ...      ...      ...      ...      ...      ...   \n",
       "43959   96.0  ...     7.30     8.80     6.05    11.10    11.80    12.57   \n",
       "43960  102.0  ...     7.30    11.10     7.30    12.57     6.05    12.40   \n",
       "43961  105.0  ...     8.10    12.57     7.30    12.40     7.30    14.65   \n",
       "43962  108.0  ...     9.43    12.40     8.10    14.65     7.30    11.53   \n",
       "43963  110.0  ...     7.85    14.65     9.43    11.53     8.10    10.25   \n",
       "\n",
       "       O3_t-22  O3_t+22 O3_t-23  O3_t+23  \n",
       "165      19.58    32.58   21.33    34.42  \n",
       "166      18.75    34.42   19.58    34.75  \n",
       "167      19.83    34.75   18.75    38.33  \n",
       "168      26.83    38.33   19.83    46.50  \n",
       "170      43.58    55.92   35.33    56.92  \n",
       "...        ...      ...     ...      ...  \n",
       "43959    10.88    12.40   10.25    14.65  \n",
       "43960    11.80    14.65   10.88    11.53  \n",
       "43961     6.05    11.53   11.80    10.25  \n",
       "43962     7.30    10.25    6.05    10.05  \n",
       "43963     7.30    10.05    7.30    10.57  \n",
       "\n",
       "[14861 rows x 497 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pre - processing and loading data\n",
    "class dataset:\n",
    "    def __init__(self,met,aqi,split_aqi):\n",
    "            self.metro_data = met\n",
    "            self.aqi_data = aqi\n",
    "            self.split_aqi = split_aqi\n",
    "    def mergedData(self,station,rlist=['PM2.5','PM10','NO','NO2','NOx','NH3','CO','SO2','O3'],roll=48,shift=48):\n",
    "        df_aqi = self.getdf(station)\n",
    "        df = pd.merge(df_aqi, self.metro_data, how='inner', on=['Date', 'Time'])\n",
    "        print(\"Merged Dataset Size\",len(df))\n",
    "        \n",
    "        #Pre Processing merged Data\n",
    "        df['Year'] = df['Date'].str[0:4]\n",
    "        df['Month'] = df['Date'].str[5:7].astype(np.float64)\n",
    "        df['Day'] = df['Date'].str[8:10].astype(np.float64)\n",
    "        df['Hour'] = df['Time'].str[0:2]\n",
    "        \n",
    "        # TRIG TRANSFORMATIONS\n",
    "        df['windX'] = np.cos(np.deg2rad(df['Wind direction'])) * df['Wind speed']\n",
    "        df['windY'] = np.sin(np.deg2rad(df['Wind direction'])) * df['Wind speed']\n",
    "        df['hourX'] = np.cos((df['Hour'].astype(np.float64)-1)*np.pi/24)\n",
    "        df['hourY'] = np.sin((df['Hour'].astype(np.float64)-1)*np.pi/24)\n",
    "        df['MonthX'] = np.cos((df['Month'].astype(np.float64)-1)*np.pi/12)\n",
    "        df['MonthY'] = np.sin((df['Month'].astype(np.float64)-1)*np.pi/12)\n",
    "        \n",
    "        import datetime\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "        df['isWeekend'] =  (df['Date'].dt.dayofweek>=5).astype(int)\n",
    "        \n",
    "        df.interpolate(method='linear', limit=5,inplace=True)\n",
    "        \n",
    "        # Drop Additional columns\n",
    "        df.drop('Benzene', axis=1, inplace=True)\n",
    "        df.drop('Toluene',axis=1, inplace=True)\n",
    "        df.drop('Xylene', axis=1,inplace=True)\n",
    "        df.drop('AQI_Bucket',axis=1,inplace=True)\n",
    "        df.drop('Datetime',axis=1,inplace=True)\n",
    "        df.drop('StationId',axis=1,inplace=True)\n",
    "        df.drop('Short-wave irradiation',axis=1,inplace=True)\n",
    "        df.drop('Date',axis=1,inplace=True)\n",
    "        df.drop('Time',axis=1,inplace=True)\n",
    "        \n",
    "        # Rolling and shifting \n",
    "        print(\"Size before roll\",len(df))\n",
    "        for i in rlist:\n",
    "            df[i+'_lag1'] = df[i].shift(24)\n",
    "            df[i+'_lag2'] = df[i].shift(48)\n",
    "        for i in rlist:\n",
    "            df[i+\"_pred1\"] = df[i].shift(-24)\n",
    "            df[i+\"_pred2\"] = df[i].shift(-48)\n",
    "        for i in rlist:\n",
    "            for j in range(24):\n",
    "                df[i+\"_t-\"+str(j)] = df[i].shift(j)\n",
    "                df[i+\"_t+\"+str(j)] = df[i].shift(-j-shift)\n",
    "        df.dropna(inplace=True)\n",
    "        print(\"Size after roll\",len(df))\n",
    "        \n",
    "        return df.copy()\n",
    "    def getdf(self,station):\n",
    "        return self.split_aqi[station]\n",
    "    def plot(self,station):\n",
    "        df = self.getdf(station)\n",
    "    def stats(self):\n",
    "        pass\n",
    "dat = dataset(met,aqi,split_aqi)\n",
    "df = dat.mergedData('DL031')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4613403329888863\n",
      "60.96974796032171\n",
      "63.05849255749911\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif, f_regression , mutual_info_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "dfTrain = df[:]\n",
    "features = ['Temperature','Relative Humidity','windX','windY','Year','MonthX','MonthY','hourX','hourY','isWeekend']\n",
    "features.append(\"PM2.5_lag1\")\n",
    "features.append(\"PM2.5_lag2\")\n",
    "\n",
    "features.append(\"PM10_lag1\")\n",
    "features.append(\"PM10_lag2\")\n",
    "\n",
    "features.append(\"NO2_lag1\")\n",
    "features.append(\"NO2_lag2\")\n",
    "\n",
    "features.append(\"SO2_lag1\")\n",
    "features.append(\"SO2_lag2\")\n",
    "\n",
    "features.append(\"CO_lag1\")\n",
    "features.append(\"CO_lag2\")\n",
    "X = df[features]\n",
    "y = df['PM2.5_pred1']\n",
    "scaler = StandardScaler()\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "scaler.fit(Xtrain)\n",
    "\n",
    "reg = LinearRegression().fit(scaler.transform(Xtrain), ytrain)\n",
    "\n",
    "testPred = reg.predict(scaler.transform(Xtest))\n",
    "trainPred = reg.predict(scaler.transform(Xtrain))\n",
    "mse = np.mean((testPred - np.array(ytest))*(testPred - np.array(ytest)))\n",
    "print(reg.score(scaler.transform(Xtest), ytest))\n",
    "print(mean_squared_error(testPred, ytest,squared=False))\n",
    "print(mean_squared_error(trainPred, ytrain,squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5366752515515851\n",
      "56.54574363812652\n",
      "57.906774033167665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\confusement\\miniconda3\\envs\\mlc\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif, f_regression , mutual_info_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "dfTrain = df[:]\n",
    "features = ['Temperature','Relative Humidity','windX','windY','Year','MonthX','MonthY','hourX','hourY','isWeekend']\n",
    "features.append(\"PM2.5_lag1\")\n",
    "features.append(\"PM2.5_lag2\")\n",
    "\n",
    "features.append(\"PM10_lag1\")\n",
    "features.append(\"PM10_lag2\")\n",
    "\n",
    "features.append(\"NO2_lag1\")\n",
    "features.append(\"NO2_lag2\")\n",
    "\n",
    "features.append(\"SO2_lag1\")\n",
    "features.append(\"SO2_lag2\")\n",
    "\n",
    "features.append(\"CO_lag1\")\n",
    "features.append(\"CO_lag2\")\n",
    "X = df[features]\n",
    "y = df['PM2.5_pred1']\n",
    "scaler = StandardScaler()\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "scaler.fit(Xtrain)\n",
    "\n",
    "reg = MLPRegressor(random_state=1, max_iter=100).fit(scaler.transform(Xtrain), ytrain)\n",
    "\n",
    "testPred = reg.predict(scaler.transform(Xtest))\n",
    "trainPred = reg.predict(scaler.transform(Xtrain))\n",
    "mse = np.mean((testPred - np.array(ytest))*(testPred - np.array(ytest)))\n",
    "print(reg.score(scaler.transform(Xtest), ytest))\n",
    "print(mean_squared_error(testPred, ytest,squared=False))\n",
    "print(mean_squared_error(trainPred, ytrain,squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3778398185894405\n",
      "65.52518818123579\n",
      "66.79772066299263\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif, f_regression , mutual_info_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "dfTrain = df[:]\n",
    "features = ['Temperature','Relative Humidity','windX','windY','Year','MonthX','MonthY','hourX','hourY','isWeekend']\n",
    "features.append(\"PM2.5_lag1\")\n",
    "features.append(\"PM2.5_lag2\")\n",
    "\n",
    "features.append(\"PM10_lag1\")\n",
    "features.append(\"PM10_lag2\")\n",
    "\n",
    "features.append(\"NO2_lag1\")\n",
    "features.append(\"NO2_lag2\")\n",
    "\n",
    "features.append(\"SO2_lag1\")\n",
    "features.append(\"SO2_lag2\")\n",
    "\n",
    "features.append(\"CO_lag1\")\n",
    "features.append(\"CO_lag2\")\n",
    "X = df[features]\n",
    "y = df['PM2.5_pred1']\n",
    "scaler = StandardScaler()\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "scaler.fit(Xtrain)\n",
    "\n",
    "reg = SVR(C=1.0, epsilon=0.2).fit(scaler.transform(Xtrain), ytrain)\n",
    "\n",
    "testPred = reg.predict(scaler.transform(Xtest))\n",
    "trainPred = reg.predict(scaler.transform(Xtrain))\n",
    "mse = np.mean((testPred - np.array(ytest))*(testPred - np.array(ytest)))\n",
    "print(reg.score(scaler.transform(Xtest), ytest))\n",
    "print(mean_squared_error(testPred, ytest,squared=False))\n",
    "print(mean_squared_error(trainPred, ytrain,squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_27 (Dense)             (None, 50)                1750      \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 24)                1224      \n",
      "=================================================================\n",
      "Total params: 8,074\n",
      "Trainable params: 8,074\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 9168 samples, validate on 2293 samples\n",
      "Epoch 1/250\n",
      "9168/9168 [==============================] - 0s 51us/step - loss: 10501.2033 - mse: 10501.2080 - mae: 72.9954 - val_loss: 5702.4121 - val_mse: 5702.4111 - val_mae: 51.5703\n",
      "Epoch 2/250\n",
      "9168/9168 [==============================] - 0s 35us/step - loss: 4607.2749 - mse: 4607.2754 - mae: 46.6721 - val_loss: 4011.9436 - val_mse: 4011.9434 - val_mae: 44.2378\n",
      "Epoch 3/250\n",
      "9168/9168 [==============================] - 0s 33us/step - loss: 4053.8322 - mse: 4053.8325 - mae: 43.9911 - val_loss: 3873.4642 - val_mse: 3873.4636 - val_mae: 43.1683\n",
      "Epoch 4/250\n",
      "9168/9168 [==============================] - 0s 34us/step - loss: 3987.2027 - mse: 3987.2029 - mae: 43.5427 - val_loss: 3828.1427 - val_mse: 3828.1421 - val_mae: 42.6187\n",
      "Epoch 5/250\n",
      "9168/9168 [==============================] - 0s 32us/step - loss: 3946.3836 - mse: 3946.3840 - mae: 43.2286 - val_loss: 3766.2968 - val_mse: 3766.2966 - val_mae: 42.3478\n",
      "Epoch 6/250\n",
      "9168/9168 [==============================] - 0s 34us/step - loss: 3885.4507 - mse: 3885.4512 - mae: 42.8819 - val_loss: 3706.9081 - val_mse: 3706.9084 - val_mae: 42.3124\n",
      "Epoch 7/250\n",
      "9168/9168 [==============================] - 0s 33us/step - loss: 3822.0673 - mse: 3822.0681 - mae: 42.3995 - val_loss: 3650.5673 - val_mse: 3650.5674 - val_mae: 41.3649\n",
      "Epoch 8/250\n",
      "9168/9168 [==============================] - 0s 34us/step - loss: 3780.5469 - mse: 3780.5471 - mae: 42.0777 - val_loss: 3616.4775 - val_mse: 3616.4773 - val_mae: 41.7048\n",
      "Epoch 9/250\n",
      "9168/9168 [==============================] - 0s 33us/step - loss: 3756.8464 - mse: 3756.8455 - mae: 41.9046 - val_loss: 3680.7000 - val_mse: 3680.6995 - val_mae: 41.0720\n",
      "Epoch 10/250\n",
      "9168/9168 [==============================] - 0s 33us/step - loss: 3704.6071 - mse: 3704.6077 - mae: 41.5899 - val_loss: 3618.7338 - val_mse: 3618.7339 - val_mae: 40.5106\n",
      "Epoch 11/250\n",
      "9168/9168 [==============================] - 0s 34us/step - loss: 3652.7180 - mse: 3652.7175 - mae: 41.2826 - val_loss: 3515.5140 - val_mse: 3515.5139 - val_mae: 40.2515\n",
      "Epoch 12/250\n",
      "9168/9168 [==============================] - 0s 33us/step - loss: 3610.8075 - mse: 3610.8071 - mae: 40.9435 - val_loss: 3518.4742 - val_mse: 3518.4746 - val_mae: 40.3326\n",
      "Epoch 13/250\n",
      "9168/9168 [==============================] - 0s 33us/step - loss: 3566.9043 - mse: 3566.9031 - mae: 40.6748 - val_loss: 3456.3940 - val_mse: 3456.3948 - val_mae: 39.8032\n",
      "Epoch 14/250\n",
      "9168/9168 [==============================] - 0s 34us/step - loss: 3554.7374 - mse: 3554.7380 - mae: 40.5672 - val_loss: 3450.1237 - val_mse: 3450.1233 - val_mae: 39.7068\n",
      "Epoch 15/250\n",
      "9168/9168 [==============================] - 0s 33us/step - loss: 3528.6818 - mse: 3528.6831 - mae: 40.3382 - val_loss: 3417.9877 - val_mse: 3417.9875 - val_mae: 39.9410\n",
      "Epoch 16/250\n",
      "9168/9168 [==============================] - 0s 33us/step - loss: 3507.2027 - mse: 3507.2029 - mae: 40.2317 - val_loss: 3432.4529 - val_mse: 3432.4536 - val_mae: 39.3016\n",
      "Epoch 17/250\n",
      "9168/9168 [==============================] - 0s 32us/step - loss: 3479.0245 - mse: 3479.0244 - mae: 40.0523 - val_loss: 3379.3646 - val_mse: 3379.3643 - val_mae: 39.7335\n",
      "Epoch 18/250\n",
      "9168/9168 [==============================] - 0s 34us/step - loss: 3457.5959 - mse: 3457.5959 - mae: 39.9146 - val_loss: 3376.9222 - val_mse: 3376.9221 - val_mae: 40.0131\n",
      "Epoch 19/250\n",
      "9168/9168 [==============================] - 0s 33us/step - loss: 3445.7374 - mse: 3445.7375 - mae: 39.8366 - val_loss: 3354.9496 - val_mse: 3354.9495 - val_mae: 39.5913\n",
      "Epoch 20/250\n",
      "9168/9168 [==============================] - 0s 32us/step - loss: 3421.7238 - mse: 3421.7236 - mae: 39.6870 - val_loss: 3340.2665 - val_mse: 3340.2664 - val_mae: 39.1797\n",
      "Epoch 21/250\n",
      "9168/9168 [==============================] - 0s 34us/step - loss: 3384.7791 - mse: 3384.7793 - mae: 39.4482 - val_loss: 3291.5090 - val_mse: 3291.5085 - val_mae: 39.4303\n",
      "Epoch 22/250\n",
      "9168/9168 [==============================] - 0s 33us/step - loss: 3370.5923 - mse: 3370.5930 - mae: 39.3977 - val_loss: 3260.5655 - val_mse: 3260.5654 - val_mae: 39.0395\n",
      "Epoch 23/250\n",
      "9168/9168 [==============================] - 0s 33us/step - loss: 3338.7301 - mse: 3338.7295 - mae: 39.2522 - val_loss: 3274.3347 - val_mse: 3274.3352 - val_mae: 39.1915\n",
      "Epoch 24/250\n",
      "9168/9168 [==============================] - 0s 33us/step - loss: 3300.6126 - mse: 3300.6128 - mae: 39.0190 - val_loss: 3245.5471 - val_mse: 3245.5471 - val_mae: 38.3917\n",
      "Epoch 25/250\n",
      "9168/9168 [==============================] - 0s 34us/step - loss: 3266.4617 - mse: 3266.4619 - mae: 38.8074 - val_loss: 3155.7878 - val_mse: 3155.7878 - val_mae: 38.4181\n",
      "Epoch 26/250\n",
      "9168/9168 [==============================] - 0s 33us/step - loss: 3229.4536 - mse: 3229.4534 - mae: 38.6956 - val_loss: 3227.2742 - val_mse: 3227.2742 - val_mae: 38.0926\n",
      "Epoch 27/250\n",
      "9168/9168 [==============================] - 0s 33us/step - loss: 3211.8237 - mse: 3211.8237 - mae: 38.4871 - val_loss: 3155.9854 - val_mse: 3155.9854 - val_mae: 38.3643\n",
      "Epoch 28/250\n",
      "9168/9168 [==============================] - 0s 35us/step - loss: 3191.9342 - mse: 3191.9338 - mae: 38.3960 - val_loss: 3120.5353 - val_mse: 3120.5356 - val_mae: 38.2522\n",
      "Epoch 29/250\n",
      "9168/9168 [==============================] - 0s 33us/step - loss: 3191.1628 - mse: 3191.1641 - mae: 38.3622 - val_loss: 3122.0350 - val_mse: 3122.0349 - val_mae: 38.0685\n",
      "Epoch 30/250\n",
      "9168/9168 [==============================] - 0s 33us/step - loss: 3155.9680 - mse: 3155.9688 - mae: 38.2040 - val_loss: 3142.4267 - val_mse: 3142.4265 - val_mae: 38.0174\n",
      "Epoch 31/250\n",
      "9168/9168 [==============================] - 0s 34us/step - loss: 3147.0684 - mse: 3147.0681 - mae: 38.1051 - val_loss: 3087.6265 - val_mse: 3087.6267 - val_mae: 38.1406\n",
      "Epoch 32/250\n",
      "9168/9168 [==============================] - 0s 37us/step - loss: 3115.5487 - mse: 3115.5486 - mae: 37.9971 - val_loss: 3070.7494 - val_mse: 3070.7495 - val_mae: 37.7171\n",
      "Epoch 33/250\n",
      "9168/9168 [==============================] - 0s 42us/step - loss: 3107.3041 - mse: 3107.3042 - mae: 37.9151 - val_loss: 3076.6060 - val_mse: 3076.6060 - val_mae: 37.8842\n",
      "Epoch 34/250\n",
      "9168/9168 [==============================] - 0s 45us/step - loss: 3089.0106 - mse: 3089.0112 - mae: 37.8352 - val_loss: 3043.5895 - val_mse: 3043.5894 - val_mae: 37.6761\n",
      "Epoch 35/250\n",
      "9168/9168 [==============================] - 0s 40us/step - loss: 3081.6293 - mse: 3081.6292 - mae: 37.7987 - val_loss: 3031.6747 - val_mse: 3031.6748 - val_mae: 37.9851\n",
      "Epoch 36/250\n",
      "9168/9168 [==============================] - 0s 43us/step - loss: 3058.7315 - mse: 3058.7319 - mae: 37.7307 - val_loss: 3033.6561 - val_mse: 3033.6565 - val_mae: 37.3777\n",
      "Epoch 37/250\n",
      "9168/9168 [==============================] - 1s 56us/step - loss: 3041.2607 - mse: 3041.2598 - mae: 37.5029 - val_loss: 3012.7707 - val_mse: 3012.7705 - val_mae: 37.6637\n",
      "Epoch 38/250\n",
      "9168/9168 [==============================] - 1s 92us/step - loss: 3035.3683 - mse: 3035.3684 - mae: 37.5327 - val_loss: 3023.6208 - val_mse: 3023.6211 - val_mae: 37.4912\n",
      "Epoch 39/250\n",
      "9168/9168 [==============================] - 0s 46us/step - loss: 3022.3479 - mse: 3022.3484 - mae: 37.4792 - val_loss: 3032.4608 - val_mse: 3032.4607 - val_mae: 37.8492\n",
      "Epoch 40/250\n",
      "9168/9168 [==============================] - 1s 65us/step - loss: 3010.2364 - mse: 3010.2366 - mae: 37.3770 - val_loss: 3002.2970 - val_mse: 3002.2969 - val_mae: 37.9480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/250\n",
      "9168/9168 [==============================] - 0s 49us/step - loss: 3003.1233 - mse: 3003.1235 - mae: 37.3832 - val_loss: 3010.6117 - val_mse: 3010.6121 - val_mae: 37.4309\n",
      "Epoch 42/250\n",
      "9168/9168 [==============================] - 1s 60us/step - loss: 2988.1373 - mse: 2988.1382 - mae: 37.2508 - val_loss: 3026.2134 - val_mse: 3026.2136 - val_mae: 38.2478\n",
      "Epoch 43/250\n",
      "9168/9168 [==============================] - 0s 48us/step - loss: 2962.5517 - mse: 2962.5522 - mae: 37.0384 - val_loss: 3014.5280 - val_mse: 3014.5276 - val_mae: 38.1166\n",
      "Epoch 44/250\n",
      "9168/9168 [==============================] - 0s 39us/step - loss: 2948.8334 - mse: 2948.8330 - mae: 37.0295 - val_loss: 2951.5256 - val_mse: 2951.5256 - val_mae: 37.2895\n",
      "Epoch 45/250\n",
      "9168/9168 [==============================] - 0s 36us/step - loss: 2940.9697 - mse: 2940.9702 - mae: 36.9538 - val_loss: 2961.4965 - val_mse: 2961.4968 - val_mae: 37.3088\n",
      "Epoch 46/250\n",
      "9168/9168 [==============================] - 0s 36us/step - loss: 2938.7361 - mse: 2938.7361 - mae: 36.9860 - val_loss: 2967.8801 - val_mse: 2967.8801 - val_mae: 37.9382\n",
      "Epoch 47/250\n",
      "9168/9168 [==============================] - 0s 49us/step - loss: 2930.5528 - mse: 2930.5527 - mae: 36.9117 - val_loss: 3056.6092 - val_mse: 3056.6091 - val_mae: 36.9247\n",
      "Epoch 48/250\n",
      "9168/9168 [==============================] - ETA: 0s - loss: 2910.7413 - mse: 2910.7407 - mae: 36.78 - 0s 43us/step - loss: 2908.4819 - mse: 2908.4814 - mae: 36.7602 - val_loss: 2943.5509 - val_mse: 2943.5513 - val_mae: 37.6783\n",
      "Epoch 49/250\n",
      "9168/9168 [==============================] - 0s 41us/step - loss: 2882.2574 - mse: 2882.2585 - mae: 36.5749 - val_loss: 2933.7423 - val_mse: 2933.7422 - val_mae: 37.1647\n",
      "Epoch 50/250\n",
      "9168/9168 [==============================] - 0s 35us/step - loss: 2882.8286 - mse: 2882.8289 - mae: 36.6281 - val_loss: 2933.4924 - val_mse: 2933.4922 - val_mae: 37.3721\n",
      "Epoch 51/250\n",
      "9168/9168 [==============================] - 0s 53us/step - loss: 2874.7420 - mse: 2874.7432 - mae: 36.5564 - val_loss: 2953.2231 - val_mse: 2953.2234 - val_mae: 37.2362\n",
      "Epoch 52/250\n",
      "9168/9168 [==============================] - 1s 65us/step - loss: 2871.6595 - mse: 2871.6589 - mae: 36.5560 - val_loss: 2915.2361 - val_mse: 2915.2356 - val_mae: 36.7501\n",
      "Epoch 53/250\n",
      "9168/9168 [==============================] - 1s 56us/step - loss: 2856.3339 - mse: 2856.3333 - mae: 36.4953 - val_loss: 2895.6835 - val_mse: 2895.6831 - val_mae: 37.3141\n",
      "Epoch 54/250\n",
      "9168/9168 [==============================] - 0s 48us/step - loss: 2869.7969 - mse: 2869.7979 - mae: 36.5472 - val_loss: 2932.0957 - val_mse: 2932.0952 - val_mae: 36.9335\n",
      "Epoch 55/250\n",
      "9168/9168 [==============================] - 1s 65us/step - loss: 2852.1331 - mse: 2852.1323 - mae: 36.4304 - val_loss: 2927.7531 - val_mse: 2927.7532 - val_mae: 37.3161\n",
      "Epoch 56/250\n",
      "9168/9168 [==============================] - 1s 84us/step - loss: 2830.4936 - mse: 2830.4927 - mae: 36.3187 - val_loss: 2897.1774 - val_mse: 2897.1775 - val_mae: 36.8569\n",
      "Epoch 57/250\n",
      "9168/9168 [==============================] - 1s 82us/step - loss: 2816.7499 - mse: 2816.7510 - mae: 36.2624 - val_loss: 2909.4364 - val_mse: 2909.4360 - val_mae: 37.3802\n",
      "Epoch 58/250\n",
      "9168/9168 [==============================] - 0s 43us/step - loss: 2806.3583 - mse: 2806.3577 - mae: 36.1984 - val_loss: 2925.8845 - val_mse: 2925.8840 - val_mae: 37.2139\n",
      "Epoch 59/250\n",
      "9168/9168 [==============================] - 0s 45us/step - loss: 2818.0567 - mse: 2818.0564 - mae: 36.2508 - val_loss: 2954.8116 - val_mse: 2954.8118 - val_mae: 36.7112\n",
      "Epoch 60/250\n",
      "9168/9168 [==============================] - 0s 49us/step - loss: 2803.5989 - mse: 2803.5989 - mae: 36.1372 - val_loss: 2950.4878 - val_mse: 2950.4880 - val_mae: 37.7889\n",
      "Epoch 61/250\n",
      "9168/9168 [==============================] - 1s 56us/step - loss: 2802.3371 - mse: 2802.3376 - mae: 36.1431 - val_loss: 2936.7550 - val_mse: 2936.7551 - val_mae: 37.0135\n",
      "Epoch 62/250\n",
      "9168/9168 [==============================] - 0s 44us/step - loss: 2790.9064 - mse: 2790.9067 - mae: 36.0912 - val_loss: 2900.4295 - val_mse: 2900.4299 - val_mae: 36.7009\n",
      "Epoch 63/250\n",
      "9168/9168 [==============================] - 0s 42us/step - loss: 2794.5687 - mse: 2794.5681 - mae: 36.1323 - val_loss: 2893.2059 - val_mse: 2893.2063 - val_mae: 37.3718\n",
      "Epoch 64/250\n",
      "9168/9168 [==============================] - 0s 39us/step - loss: 2766.3645 - mse: 2766.3645 - mae: 35.9709 - val_loss: 2861.3263 - val_mse: 2861.3264 - val_mae: 36.5356\n",
      "Epoch 65/250\n",
      "9168/9168 [==============================] - 0s 38us/step - loss: 2764.1583 - mse: 2764.1582 - mae: 35.9483 - val_loss: 2926.8333 - val_mse: 2926.8335 - val_mae: 36.6260\n",
      "Epoch 66/250\n",
      "9168/9168 [==============================] - 0s 38us/step - loss: 2750.6488 - mse: 2750.6487 - mae: 35.8606 - val_loss: 2888.1103 - val_mse: 2888.1099 - val_mae: 36.7228\n",
      "Epoch 67/250\n",
      "9168/9168 [==============================] - 0s 46us/step - loss: 2736.6936 - mse: 2736.6934 - mae: 35.7829 - val_loss: 2876.0431 - val_mse: 2876.0432 - val_mae: 36.9829\n",
      "Epoch 68/250\n",
      "9168/9168 [==============================] - 0s 37us/step - loss: 2759.7427 - mse: 2759.7432 - mae: 35.9019 - val_loss: 2893.6013 - val_mse: 2893.6013 - val_mae: 37.1726\n",
      "Epoch 69/250\n",
      "9168/9168 [==============================] - 0s 35us/step - loss: 2727.7892 - mse: 2727.7883 - mae: 35.7392 - val_loss: 2902.7052 - val_mse: 2902.7053 - val_mae: 37.5905\n",
      "Epoch 70/250\n",
      "9168/9168 [==============================] - 1s 61us/step - loss: 2730.6203 - mse: 2730.6206 - mae: 35.7685 - val_loss: 2871.1615 - val_mse: 2871.1616 - val_mae: 37.0421\n",
      "Epoch 71/250\n",
      "9168/9168 [==============================] - 1s 59us/step - loss: 2724.7897 - mse: 2724.7896 - mae: 35.7200 - val_loss: 2865.1319 - val_mse: 2865.1318 - val_mae: 36.4684\n",
      "Epoch 72/250\n",
      "9168/9168 [==============================] - ETA: 0s - loss: 2717.6657 - mse: 2717.6660 - mae: 35.54 - 0s 37us/step - loss: 2708.2435 - mse: 2708.2441 - mae: 35.6104 - val_loss: 2907.9039 - val_mse: 2907.9031 - val_mae: 36.5184\n",
      "Epoch 73/250\n",
      "9168/9168 [==============================] - 0s 48us/step - loss: 2696.2589 - mse: 2696.2583 - mae: 35.5558 - val_loss: 2870.1923 - val_mse: 2870.1924 - val_mae: 36.5201\n",
      "Epoch 74/250\n",
      "9168/9168 [==============================] - 1s 64us/step - loss: 2686.8397 - mse: 2686.8398 - mae: 35.5010 - val_loss: 2882.5525 - val_mse: 2882.5530 - val_mae: 37.1339\n",
      "Epoch 75/250\n",
      "9168/9168 [==============================] - 1s 65us/step - loss: 2688.7338 - mse: 2688.7346 - mae: 35.4895 - val_loss: 2895.6827 - val_mse: 2895.6826 - val_mae: 36.6634\n",
      "Epoch 76/250\n",
      "9168/9168 [==============================] - 1s 66us/step - loss: 2675.3937 - mse: 2675.3940 - mae: 35.4083 - val_loss: 2872.7523 - val_mse: 2872.7527 - val_mae: 36.7379\n",
      "Epoch 77/250\n",
      "9168/9168 [==============================] - 1s 68us/step - loss: 2681.0093 - mse: 2681.0100 - mae: 35.4313 - val_loss: 2851.0985 - val_mse: 2851.0986 - val_mae: 36.8488\n",
      "Epoch 78/250\n",
      "9168/9168 [==============================] - 1s 62us/step - loss: 2673.8014 - mse: 2673.8013 - mae: 35.4575 - val_loss: 2897.8062 - val_mse: 2897.8062 - val_mae: 36.4127\n",
      "Epoch 79/250\n",
      "9168/9168 [==============================] - ETA: 0s - loss: 2658.0513 - mse: 2658.0527 - mae: 35.36 - 1s 60us/step - loss: 2650.6515 - mse: 2650.6528 - mae: 35.3275 - val_loss: 2837.4790 - val_mse: 2837.4790 - val_mae: 36.7680\n",
      "Epoch 80/250\n",
      "9168/9168 [==============================] - 1s 57us/step - loss: 2646.4303 - mse: 2646.4309 - mae: 35.2705 - val_loss: 2842.2342 - val_mse: 2842.2349 - val_mae: 36.5613\n",
      "Epoch 81/250\n",
      "9168/9168 [==============================] - 1s 61us/step - loss: 2653.1067 - mse: 2653.1072 - mae: 35.3153 - val_loss: 2862.3175 - val_mse: 2862.3174 - val_mae: 36.7574\n",
      "Epoch 82/250\n",
      "9168/9168 [==============================] - 1s 72us/step - loss: 2643.6493 - mse: 2643.6499 - mae: 35.2817 - val_loss: 2856.6396 - val_mse: 2856.6389 - val_mae: 36.5313\n",
      "Epoch 83/250\n",
      "9168/9168 [==============================] - 1s 73us/step - loss: 2632.5343 - mse: 2632.5347 - mae: 35.1924 - val_loss: 2836.6223 - val_mse: 2836.6218 - val_mae: 36.9739\n",
      "Epoch 84/250\n",
      "9168/9168 [==============================] - 1s 71us/step - loss: 2625.1477 - mse: 2625.1475 - mae: 35.1148 - val_loss: 2831.0153 - val_mse: 2831.0151 - val_mae: 36.8044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/250\n",
      "9168/9168 [==============================] - 1s 73us/step - loss: 2618.6328 - mse: 2618.6331 - mae: 35.1310 - val_loss: 2877.4049 - val_mse: 2877.4050 - val_mae: 37.2140\n",
      "Epoch 86/250\n",
      "9168/9168 [==============================] - 1s 73us/step - loss: 2613.7019 - mse: 2613.7024 - mae: 35.0753 - val_loss: 2836.6697 - val_mse: 2836.6697 - val_mae: 36.4057\n",
      "Epoch 87/250\n",
      "9168/9168 [==============================] - 1s 70us/step - loss: 2602.9707 - mse: 2602.9700 - mae: 35.0452 - val_loss: 2846.9951 - val_mse: 2846.9949 - val_mae: 36.6187\n",
      "Epoch 88/250\n",
      "9168/9168 [==============================] - 1s 68us/step - loss: 2622.0805 - mse: 2622.0813 - mae: 35.1701 - val_loss: 2836.7969 - val_mse: 2836.7969 - val_mae: 36.4585\n",
      "Epoch 89/250\n",
      "9168/9168 [==============================] - 1s 61us/step - loss: 2602.2658 - mse: 2602.2659 - mae: 35.0589 - val_loss: 2939.7009 - val_mse: 2939.7009 - val_mae: 36.4563\n",
      "Epoch 90/250\n",
      "9168/9168 [==============================] - 1s 58us/step - loss: 2595.9103 - mse: 2595.9094 - mae: 34.9749 - val_loss: 2827.0944 - val_mse: 2827.0947 - val_mae: 36.4497\n",
      "Epoch 91/250\n",
      "9168/9168 [==============================] - 1s 61us/step - loss: 2577.0728 - mse: 2577.0735 - mae: 34.8604 - val_loss: 2820.1273 - val_mse: 2820.1272 - val_mae: 36.8122\n",
      "Epoch 92/250\n",
      "9168/9168 [==============================] - 1s 61us/step - loss: 2566.7933 - mse: 2566.7932 - mae: 34.8459 - val_loss: 2827.7794 - val_mse: 2827.7795 - val_mae: 36.3621\n",
      "Epoch 93/250\n",
      "9168/9168 [==============================] - 1s 58us/step - loss: 2565.3963 - mse: 2565.3962 - mae: 34.8056 - val_loss: 2863.1122 - val_mse: 2863.1121 - val_mae: 36.8430\n",
      "Epoch 94/250\n",
      "9168/9168 [==============================] - 1s 63us/step - loss: 2567.3972 - mse: 2567.3967 - mae: 34.8351 - val_loss: 2908.2065 - val_mse: 2908.2065 - val_mae: 36.8808\n",
      "Epoch 95/250\n",
      "9168/9168 [==============================] - 1s 67us/step - loss: 2552.3163 - mse: 2552.3157 - mae: 34.7336 - val_loss: 2803.8988 - val_mse: 2803.8987 - val_mae: 36.8019\n",
      "Epoch 96/250\n",
      "9168/9168 [==============================] - 1s 71us/step - loss: 2543.9213 - mse: 2543.9211 - mae: 34.7198 - val_loss: 2818.5353 - val_mse: 2818.5354 - val_mae: 36.2174\n",
      "Epoch 97/250\n",
      "9168/9168 [==============================] - 1s 71us/step - loss: 2542.0514 - mse: 2542.0520 - mae: 34.6748 - val_loss: 2858.8058 - val_mse: 2858.8057 - val_mae: 37.2715\n",
      "Epoch 98/250\n",
      "9168/9168 [==============================] - 1s 77us/step - loss: 2546.5116 - mse: 2546.5117 - mae: 34.7577 - val_loss: 2842.0407 - val_mse: 2842.0410 - val_mae: 36.6330\n",
      "Epoch 99/250\n",
      "9168/9168 [==============================] - 1s 72us/step - loss: 2537.2446 - mse: 2537.2446 - mae: 34.6554 - val_loss: 2828.4914 - val_mse: 2828.4915 - val_mae: 36.4821\n",
      "Epoch 100/250\n",
      "9168/9168 [==============================] - 1s 70us/step - loss: 2535.3643 - mse: 2535.3638 - mae: 34.6557 - val_loss: 2814.0631 - val_mse: 2814.0637 - val_mae: 36.8658\n",
      "Epoch 101/250\n",
      "9168/9168 [==============================] - 1s 71us/step - loss: 2533.3751 - mse: 2533.3750 - mae: 34.6511 - val_loss: 2837.9719 - val_mse: 2837.9722 - val_mae: 36.3335\n",
      "Epoch 102/250\n",
      "9168/9168 [==============================] - 1s 73us/step - loss: 2520.1862 - mse: 2520.1870 - mae: 34.5736 - val_loss: 2775.0464 - val_mse: 2775.0461 - val_mae: 36.5368\n",
      "Epoch 103/250\n",
      "9168/9168 [==============================] - 1s 70us/step - loss: 2539.6528 - mse: 2539.6531 - mae: 34.6301 - val_loss: 2770.4919 - val_mse: 2770.4919 - val_mae: 36.0218\n",
      "Epoch 104/250\n",
      "9168/9168 [==============================] - 1s 72us/step - loss: 2509.4701 - mse: 2509.4712 - mae: 34.4811 - val_loss: 2838.7517 - val_mse: 2838.7517 - val_mae: 36.4926\n",
      "Epoch 105/250\n",
      "9168/9168 [==============================] - 1s 72us/step - loss: 2509.1513 - mse: 2509.1523 - mae: 34.4937 - val_loss: 2804.3818 - val_mse: 2804.3821 - val_mae: 36.3106\n",
      "Epoch 106/250\n",
      "9168/9168 [==============================] - 1s 70us/step - loss: 2504.5219 - mse: 2504.5232 - mae: 34.4511 - val_loss: 2777.4556 - val_mse: 2777.4556 - val_mae: 36.2979\n",
      "Epoch 107/250\n",
      "9168/9168 [==============================] - 1s 70us/step - loss: 2491.2501 - mse: 2491.2498 - mae: 34.3963 - val_loss: 2780.6711 - val_mse: 2780.6709 - val_mae: 36.6142\n",
      "Epoch 108/250\n",
      "9168/9168 [==============================] - 1s 63us/step - loss: 2471.8819 - mse: 2471.8818 - mae: 34.2948 - val_loss: 2782.4153 - val_mse: 2782.4153 - val_mae: 36.3923\n",
      "Epoch 109/250\n",
      "9168/9168 [==============================] - 1s 70us/step - loss: 2475.6879 - mse: 2475.6890 - mae: 34.2934 - val_loss: 2772.9090 - val_mse: 2772.9087 - val_mae: 36.5377\n",
      "Epoch 110/250\n",
      "9168/9168 [==============================] - 1s 71us/step - loss: 2477.8993 - mse: 2477.8989 - mae: 34.2731 - val_loss: 2782.8327 - val_mse: 2782.8325 - val_mae: 36.7940\n",
      "Epoch 111/250\n",
      "9168/9168 [==============================] - 1s 72us/step - loss: 2461.1462 - mse: 2461.1458 - mae: 34.2163 - val_loss: 2777.1342 - val_mse: 2777.1338 - val_mae: 36.3658\n",
      "Epoch 112/250\n",
      "9168/9168 [==============================] - 1s 69us/step - loss: 2487.9375 - mse: 2487.9375 - mae: 34.3514 - val_loss: 2760.0488 - val_mse: 2760.0493 - val_mae: 36.2354\n",
      "Epoch 113/250\n",
      "9168/9168 [==============================] - 1s 71us/step - loss: 2473.5084 - mse: 2473.5085 - mae: 34.2874 - val_loss: 2785.6329 - val_mse: 2785.6326 - val_mae: 36.2238\n",
      "Epoch 114/250\n",
      "9168/9168 [==============================] - 1s 69us/step - loss: 2453.6239 - mse: 2453.6240 - mae: 34.1378 - val_loss: 2747.3499 - val_mse: 2747.3501 - val_mae: 36.2504\n",
      "Epoch 115/250\n",
      "9168/9168 [==============================] - 1s 72us/step - loss: 2454.3098 - mse: 2454.3103 - mae: 34.1657 - val_loss: 2754.4267 - val_mse: 2754.4268 - val_mae: 36.3237\n",
      "Epoch 116/250\n",
      "9168/9168 [==============================] - 1s 71us/step - loss: 2432.8410 - mse: 2432.8408 - mae: 34.0628 - val_loss: 2769.5749 - val_mse: 2769.5754 - val_mae: 36.1181\n",
      "Epoch 117/250\n",
      "9168/9168 [==============================] - 1s 72us/step - loss: 2440.2367 - mse: 2440.2366 - mae: 34.0883 - val_loss: 2729.7240 - val_mse: 2729.7239 - val_mae: 35.9792\n",
      "Epoch 118/250\n",
      "9168/9168 [==============================] - 1s 71us/step - loss: 2439.2819 - mse: 2439.2810 - mae: 34.0818 - val_loss: 2777.1853 - val_mse: 2777.1848 - val_mae: 35.9553\n",
      "Epoch 119/250\n",
      "9168/9168 [==============================] - 1s 70us/step - loss: 2428.8931 - mse: 2428.8938 - mae: 33.9761 - val_loss: 2807.8293 - val_mse: 2807.8296 - val_mae: 35.9304\n",
      "Epoch 120/250\n",
      "9168/9168 [==============================] - 1s 70us/step - loss: 2422.3766 - mse: 2422.3774 - mae: 33.9712 - val_loss: 2744.9787 - val_mse: 2744.9790 - val_mae: 35.9431\n",
      "Epoch 121/250\n",
      "9168/9168 [==============================] - 1s 70us/step - loss: 2421.6952 - mse: 2421.6956 - mae: 33.9754 - val_loss: 2815.5122 - val_mse: 2815.5122 - val_mae: 36.8867\n",
      "Epoch 122/250\n",
      "9168/9168 [==============================] - 1s 70us/step - loss: 2408.1517 - mse: 2408.1521 - mae: 33.8868 - val_loss: 2747.2962 - val_mse: 2747.2961 - val_mae: 36.1351\n",
      "Epoch 123/250\n",
      "9168/9168 [==============================] - 1s 71us/step - loss: 2411.4718 - mse: 2411.4722 - mae: 33.9058 - val_loss: 2827.9776 - val_mse: 2827.9775 - val_mae: 35.9801\n",
      "Epoch 124/250\n",
      "9168/9168 [==============================] - 1s 70us/step - loss: 2398.1797 - mse: 2398.1799 - mae: 33.8442 - val_loss: 2730.6110 - val_mse: 2730.6106 - val_mae: 35.9469\n",
      "Epoch 125/250\n",
      "9168/9168 [==============================] - 1s 68us/step - loss: 2397.8402 - mse: 2397.8411 - mae: 33.8336 - val_loss: 2739.4961 - val_mse: 2739.4956 - val_mae: 35.8029\n",
      "Epoch 126/250\n",
      "9168/9168 [==============================] - 1s 73us/step - loss: 2406.0343 - mse: 2406.0337 - mae: 33.8749 - val_loss: 2782.7965 - val_mse: 2782.7966 - val_mae: 36.3565\n",
      "Epoch 127/250\n",
      "9168/9168 [==============================] - 1s 72us/step - loss: 2390.1323 - mse: 2390.1318 - mae: 33.7450 - val_loss: 2762.6910 - val_mse: 2762.6909 - val_mae: 36.0115\n",
      "Epoch 128/250\n",
      "9168/9168 [==============================] - 1s 69us/step - loss: 2388.0530 - mse: 2388.0522 - mae: 33.7574 - val_loss: 2830.1395 - val_mse: 2830.1399 - val_mae: 36.4702\n",
      "Epoch 129/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9168/9168 [==============================] - 1s 73us/step - loss: 2385.6269 - mse: 2385.6265 - mae: 33.7667 - val_loss: 2731.3303 - val_mse: 2731.3306 - val_mae: 36.0417\n",
      "Epoch 130/250\n",
      "9168/9168 [==============================] - 1s 70us/step - loss: 2373.2565 - mse: 2373.2563 - mae: 33.6930 - val_loss: 2754.0957 - val_mse: 2754.0955 - val_mae: 35.7909\n",
      "Epoch 131/250\n",
      "9168/9168 [==============================] - 1s 71us/step - loss: 2379.2059 - mse: 2379.2061 - mae: 33.7027 - val_loss: 2722.4283 - val_mse: 2722.4282 - val_mae: 35.6957\n",
      "Epoch 132/250\n",
      "9168/9168 [==============================] - 1s 71us/step - loss: 2385.4101 - mse: 2385.4099 - mae: 33.7058 - val_loss: 2781.5071 - val_mse: 2781.5073 - val_mae: 36.2383\n",
      "Epoch 133/250\n",
      "9168/9168 [==============================] - 1s 70us/step - loss: 2369.8431 - mse: 2369.8438 - mae: 33.6093 - val_loss: 2765.4204 - val_mse: 2765.4204 - val_mae: 36.4668\n",
      "Epoch 134/250\n",
      "9168/9168 [==============================] - 1s 74us/step - loss: 2351.2236 - mse: 2351.2239 - mae: 33.5217 - val_loss: 2756.8135 - val_mse: 2756.8132 - val_mae: 36.0000\n",
      "Epoch 135/250\n",
      "9168/9168 [==============================] - 1s 71us/step - loss: 2359.5460 - mse: 2359.5461 - mae: 33.5772 - val_loss: 2953.1848 - val_mse: 2953.1848 - val_mae: 37.9776\n",
      "Epoch 136/250\n",
      "9168/9168 [==============================] - 1s 72us/step - loss: 2360.7360 - mse: 2360.7358 - mae: 33.6033 - val_loss: 2744.0199 - val_mse: 2744.0200 - val_mae: 35.7695\n",
      "Epoch 137/250\n",
      "9168/9168 [==============================] - 1s 72us/step - loss: 2354.1242 - mse: 2354.1240 - mae: 33.5141 - val_loss: 2753.1507 - val_mse: 2753.1509 - val_mae: 36.4406\n",
      "Epoch 138/250\n",
      "9168/9168 [==============================] - 1s 72us/step - loss: 2342.0823 - mse: 2342.0830 - mae: 33.5177 - val_loss: 2706.7360 - val_mse: 2706.7361 - val_mae: 35.6173\n",
      "Epoch 139/250\n",
      "9168/9168 [==============================] - 1s 71us/step - loss: 2345.1532 - mse: 2345.1538 - mae: 33.5020 - val_loss: 2715.2822 - val_mse: 2715.2827 - val_mae: 35.8219\n",
      "Epoch 140/250\n",
      "9168/9168 [==============================] - 1s 73us/step - loss: 2339.8132 - mse: 2339.8125 - mae: 33.4698 - val_loss: 2749.2793 - val_mse: 2749.2798 - val_mae: 36.0176\n",
      "Epoch 141/250\n",
      "9168/9168 [==============================] - 1s 72us/step - loss: 2332.4838 - mse: 2332.4836 - mae: 33.3948 - val_loss: 2783.3517 - val_mse: 2783.3521 - val_mae: 35.8191\n",
      "Epoch 142/250\n",
      "9168/9168 [==============================] - 1s 70us/step - loss: 2342.0254 - mse: 2342.0256 - mae: 33.4569 - val_loss: 2707.5659 - val_mse: 2707.5662 - val_mae: 35.7609\n",
      "Epoch 143/250\n",
      "9168/9168 [==============================] - 1s 71us/step - loss: 2338.8977 - mse: 2338.8982 - mae: 33.5032 - val_loss: 2708.2812 - val_mse: 2708.2812 - val_mae: 35.5618\n",
      "Epoch 144/250\n",
      "9168/9168 [==============================] - 1s 72us/step - loss: 2342.8527 - mse: 2342.8535 - mae: 33.4345 - val_loss: 2738.0193 - val_mse: 2738.0193 - val_mae: 35.9789\n",
      "Epoch 145/250\n",
      "9168/9168 [==============================] - 1s 72us/step - loss: 2330.6328 - mse: 2330.6328 - mae: 33.4082 - val_loss: 2828.4168 - val_mse: 2828.4167 - val_mae: 36.9630\n",
      "Epoch 146/250\n",
      "9168/9168 [==============================] - 1s 59us/step - loss: 2323.0438 - mse: 2323.0444 - mae: 33.3523 - val_loss: 2700.1394 - val_mse: 2700.1396 - val_mae: 35.7044\n",
      "Epoch 147/250\n",
      "9168/9168 [==============================] - 0s 48us/step - loss: 2316.8576 - mse: 2316.8574 - mae: 33.3077 - val_loss: 2740.3560 - val_mse: 2740.3560 - val_mae: 35.7123\n",
      "Epoch 148/250\n",
      "9168/9168 [==============================] - 0s 49us/step - loss: 2303.9402 - mse: 2303.9397 - mae: 33.2297 - val_loss: 2738.8781 - val_mse: 2738.8782 - val_mae: 36.1790\n",
      "Epoch 149/250\n",
      "9168/9168 [==============================] - 0s 42us/step - loss: 2310.8163 - mse: 2310.8164 - mae: 33.2555 - val_loss: 2696.0699 - val_mse: 2696.0703 - val_mae: 35.6716\n",
      "Epoch 150/250\n",
      "9168/9168 [==============================] - 0s 49us/step - loss: 2304.9501 - mse: 2304.9500 - mae: 33.1909 - val_loss: 2721.4149 - val_mse: 2721.4146 - val_mae: 36.0474\n",
      "Epoch 151/250\n",
      "9168/9168 [==============================] - 0s 47us/step - loss: 2293.6376 - mse: 2293.6367 - mae: 33.1633 - val_loss: 2752.8728 - val_mse: 2752.8728 - val_mae: 35.6443\n",
      "Epoch 152/250\n",
      "9168/9168 [==============================] - 0s 45us/step - loss: 2296.1796 - mse: 2296.1797 - mae: 33.1713 - val_loss: 2698.0035 - val_mse: 2698.0034 - val_mae: 35.4321\n",
      "Epoch 153/250\n",
      "9168/9168 [==============================] - 0s 42us/step - loss: 2301.7679 - mse: 2301.7678 - mae: 33.2039 - val_loss: 2718.7695 - val_mse: 2718.7693 - val_mae: 35.7454\n",
      "Epoch 154/250\n",
      "9168/9168 [==============================] - 0s 51us/step - loss: 2273.7395 - mse: 2273.7400 - mae: 33.0384 - val_loss: 2743.6427 - val_mse: 2743.6426 - val_mae: 35.5010\n",
      "Epoch 155/250\n",
      "9168/9168 [==============================] - 0s 43us/step - loss: 2293.5515 - mse: 2293.5515 - mae: 33.1491 - val_loss: 2749.8268 - val_mse: 2749.8267 - val_mae: 35.9204\n",
      "Epoch 156/250\n",
      "9168/9168 [==============================] - 0s 44us/step - loss: 2277.2874 - mse: 2277.2878 - mae: 33.0354 - val_loss: 2710.9953 - val_mse: 2710.9956 - val_mae: 35.8965\n",
      "Epoch 157/250\n",
      "9168/9168 [==============================] - 0s 43us/step - loss: 2279.3810 - mse: 2279.3813 - mae: 33.0677 - val_loss: 2738.0140 - val_mse: 2738.0142 - val_mae: 35.5033\n",
      "Epoch 158/250\n",
      "9168/9168 [==============================] - 0s 42us/step - loss: 2278.3061 - mse: 2278.3054 - mae: 33.0551 - val_loss: 2729.6548 - val_mse: 2729.6545 - val_mae: 35.6718\n",
      "Epoch 159/250\n",
      "9168/9168 [==============================] - 0s 43us/step - loss: 2267.4817 - mse: 2267.4817 - mae: 33.0175 - val_loss: 2661.4397 - val_mse: 2661.4397 - val_mae: 35.4702\n",
      "Epoch 160/250\n",
      "9168/9168 [==============================] - 0s 44us/step - loss: 2277.4587 - mse: 2277.4592 - mae: 33.0016 - val_loss: 2786.0309 - val_mse: 2786.0308 - val_mae: 36.1412\n",
      "Epoch 161/250\n",
      "9168/9168 [==============================] - 0s 44us/step - loss: 2272.1902 - mse: 2272.1897 - mae: 33.0102 - val_loss: 2759.5225 - val_mse: 2759.5227 - val_mae: 35.7541\n",
      "Epoch 162/250\n",
      "9168/9168 [==============================] - 0s 53us/step - loss: 2270.7977 - mse: 2270.7976 - mae: 32.9990 - val_loss: 2698.6091 - val_mse: 2698.6091 - val_mae: 35.4398\n",
      "Epoch 163/250\n",
      "9168/9168 [==============================] - 1s 55us/step - loss: 2273.7614 - mse: 2273.7612 - mae: 33.0230 - val_loss: 2752.9590 - val_mse: 2752.9585 - val_mae: 35.6855\n",
      "Epoch 164/250\n",
      "9168/9168 [==============================] - 1s 55us/step - loss: 2261.2335 - mse: 2261.2344 - mae: 32.9889 - val_loss: 2761.8159 - val_mse: 2761.8159 - val_mae: 35.4288\n",
      "Epoch 165/250\n",
      "9168/9168 [==============================] - 1s 56us/step - loss: 2262.4472 - mse: 2262.4470 - mae: 32.9614 - val_loss: 2726.1771 - val_mse: 2726.1772 - val_mae: 35.7163\n",
      "Epoch 166/250\n",
      "9168/9168 [==============================] - 0s 54us/step - loss: 2240.0445 - mse: 2240.0442 - mae: 32.8115 - val_loss: 2721.3649 - val_mse: 2721.3650 - val_mae: 35.5713\n",
      "Epoch 167/250\n",
      "9168/9168 [==============================] - 1s 55us/step - loss: 2254.5386 - mse: 2254.5378 - mae: 32.9145 - val_loss: 2682.7280 - val_mse: 2682.7280 - val_mae: 35.4531\n",
      "Epoch 168/250\n",
      "9168/9168 [==============================] - 1s 59us/step - loss: 2237.6835 - mse: 2237.6833 - mae: 32.8033 - val_loss: 2761.1657 - val_mse: 2761.1655 - val_mae: 35.5558\n",
      "Epoch 169/250\n",
      "9168/9168 [==============================] - 1s 55us/step - loss: 2263.4249 - mse: 2263.4243 - mae: 32.9610 - val_loss: 2679.4132 - val_mse: 2679.4131 - val_mae: 35.5886\n",
      "Epoch 170/250\n",
      "9168/9168 [==============================] - 1s 57us/step - loss: 2257.0988 - mse: 2257.0991 - mae: 32.9106 - val_loss: 2749.8393 - val_mse: 2749.8396 - val_mae: 36.2115\n",
      "Epoch 171/250\n",
      "9168/9168 [==============================] - 0s 47us/step - loss: 2233.3690 - mse: 2233.3696 - mae: 32.7735 - val_loss: 2679.3275 - val_mse: 2679.3267 - val_mae: 35.6550\n",
      "Epoch 172/250\n",
      "9168/9168 [==============================] - 0s 40us/step - loss: 2224.9554 - mse: 2224.9556 - mae: 32.7561 - val_loss: 2751.5086 - val_mse: 2751.5083 - val_mae: 35.3823\n",
      "Epoch 173/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9168/9168 [==============================] - 0s 53us/step - loss: 2238.3944 - mse: 2238.3943 - mae: 32.8173 - val_loss: 2751.7345 - val_mse: 2751.7346 - val_mae: 36.2424\n",
      "Epoch 174/250\n",
      "9168/9168 [==============================] - 0s 47us/step - loss: 2225.1398 - mse: 2225.1404 - mae: 32.7369 - val_loss: 2682.1276 - val_mse: 2682.1277 - val_mae: 35.6291\n",
      "Epoch 175/250\n",
      "9168/9168 [==============================] - 0s 36us/step - loss: 2224.9483 - mse: 2224.9478 - mae: 32.7175 - val_loss: 2746.6447 - val_mse: 2746.6450 - val_mae: 35.7569\n",
      "Epoch 176/250\n",
      "9168/9168 [==============================] - 1s 67us/step - loss: 2239.4403 - mse: 2239.4404 - mae: 32.8012 - val_loss: 2742.0375 - val_mse: 2742.0374 - val_mae: 35.5209\n",
      "Epoch 177/250\n",
      "9168/9168 [==============================] - 1s 55us/step - loss: 2225.3133 - mse: 2225.3127 - mae: 32.7366 - val_loss: 2704.9229 - val_mse: 2704.9229 - val_mae: 35.3284\n",
      "Epoch 178/250\n",
      "9168/9168 [==============================] - 1s 67us/step - loss: 2234.5268 - mse: 2234.5273 - mae: 32.8055 - val_loss: 2718.6506 - val_mse: 2718.6504 - val_mae: 35.1357\n",
      "Epoch 179/250\n",
      "9168/9168 [==============================] - 1s 67us/step - loss: 2219.5418 - mse: 2219.5417 - mae: 32.6691 - val_loss: 2736.5730 - val_mse: 2736.5732 - val_mae: 35.3467\n",
      "Epoch 180/250\n",
      "9168/9168 [==============================] - 1s 64us/step - loss: 2215.5657 - mse: 2215.5654 - mae: 32.6186 - val_loss: 2692.3306 - val_mse: 2692.3311 - val_mae: 35.1866\n",
      "Epoch 181/250\n",
      "9168/9168 [==============================] - 0s 53us/step - loss: 2202.9218 - mse: 2202.9214 - mae: 32.5892 - val_loss: 2685.8952 - val_mse: 2685.8955 - val_mae: 35.4168\n",
      "Epoch 182/250\n",
      "9168/9168 [==============================] - 0s 48us/step - loss: 2213.0839 - mse: 2213.0850 - mae: 32.5877 - val_loss: 2763.6901 - val_mse: 2763.6899 - val_mae: 35.7082\n",
      "Epoch 183/250\n",
      "9168/9168 [==============================] - 1s 78us/step - loss: 2200.3692 - mse: 2200.3684 - mae: 32.5494 - val_loss: 2696.5721 - val_mse: 2696.5720 - val_mae: 35.5349\n",
      "Epoch 184/250\n",
      "9168/9168 [==============================] - 1s 83us/step - loss: 2202.4408 - mse: 2202.4407 - mae: 32.5639 - val_loss: 2736.3785 - val_mse: 2736.3787 - val_mae: 35.5552\n",
      "Epoch 185/250\n",
      "9168/9168 [==============================] - 1s 87us/step - loss: 2195.4746 - mse: 2195.4744 - mae: 32.5354 - val_loss: 2729.1009 - val_mse: 2729.1006 - val_mae: 35.5349\n",
      "Epoch 186/250\n",
      "9168/9168 [==============================] - 1s 86us/step - loss: 2185.4253 - mse: 2185.4248 - mae: 32.4748 - val_loss: 2706.0282 - val_mse: 2706.0281 - val_mae: 35.2900\n",
      "Epoch 187/250\n",
      "9168/9168 [==============================] - 1s 80us/step - loss: 2194.4312 - mse: 2194.4307 - mae: 32.4837 - val_loss: 2768.7290 - val_mse: 2768.7292 - val_mae: 35.9565\n",
      "Epoch 188/250\n",
      "9168/9168 [==============================] - 1s 76us/step - loss: 2209.7040 - mse: 2209.7039 - mae: 32.6023 - val_loss: 2760.6325 - val_mse: 2760.6323 - val_mae: 35.3665\n",
      "Epoch 189/250\n",
      "9168/9168 [==============================] - 1s 78us/step - loss: 2180.8616 - mse: 2180.8613 - mae: 32.4465 - val_loss: 2665.3655 - val_mse: 2665.3652 - val_mae: 35.3147\n",
      "Epoch 190/250\n",
      "9168/9168 [==============================] - 1s 80us/step - loss: 2168.4770 - mse: 2168.4773 - mae: 32.3804 - val_loss: 2675.5276 - val_mse: 2675.5276 - val_mae: 35.3355\n",
      "Epoch 191/250\n",
      "9168/9168 [==============================] - 1s 77us/step - loss: 2172.8818 - mse: 2172.8818 - mae: 32.3719 - val_loss: 2691.5958 - val_mse: 2691.5964 - val_mae: 35.4400\n",
      "Epoch 192/250\n",
      "9168/9168 [==============================] - 1s 75us/step - loss: 2169.1450 - mse: 2169.1445 - mae: 32.3643 - val_loss: 2672.0326 - val_mse: 2672.0327 - val_mae: 35.3784\n",
      "Epoch 193/250\n",
      "9168/9168 [==============================] - 1s 82us/step - loss: 2168.1501 - mse: 2168.1501 - mae: 32.3474 - val_loss: 2708.7715 - val_mse: 2708.7717 - val_mae: 35.6560\n",
      "Epoch 194/250\n",
      "9168/9168 [==============================] - 1s 82us/step - loss: 2166.1413 - mse: 2166.1406 - mae: 32.3235 - val_loss: 2682.2181 - val_mse: 2682.2175 - val_mae: 35.2331\n",
      "Epoch 195/250\n",
      "9168/9168 [==============================] - 1s 74us/step - loss: 2162.9998 - mse: 2162.9995 - mae: 32.3437 - val_loss: 2846.2060 - val_mse: 2846.2065 - val_mae: 35.7090\n",
      "Epoch 196/250\n",
      "9168/9168 [==============================] - 1s 77us/step - loss: 2170.5610 - mse: 2170.5605 - mae: 32.3653 - val_loss: 2676.8056 - val_mse: 2676.8057 - val_mae: 35.3123\n",
      "Epoch 197/250\n",
      "9168/9168 [==============================] - 1s 82us/step - loss: 2161.7056 - mse: 2161.7058 - mae: 32.3134 - val_loss: 2661.9283 - val_mse: 2661.9287 - val_mae: 35.3794\n",
      "Epoch 198/250\n",
      "9168/9168 [==============================] - 1s 78us/step - loss: 2150.8447 - mse: 2150.8452 - mae: 32.2186 - val_loss: 2660.7999 - val_mse: 2660.7998 - val_mae: 35.2798\n",
      "Epoch 199/250\n",
      "9168/9168 [==============================] - 1s 72us/step - loss: 2156.8525 - mse: 2156.8521 - mae: 32.3077 - val_loss: 2649.8504 - val_mse: 2649.8501 - val_mae: 35.1811\n",
      "Epoch 200/250\n",
      "9168/9168 [==============================] - 1s 79us/step - loss: 2144.8353 - mse: 2144.8352 - mae: 32.1839 - val_loss: 2729.3970 - val_mse: 2729.3970 - val_mae: 35.5094\n",
      "Epoch 201/250\n",
      "9168/9168 [==============================] - 1s 80us/step - loss: 2146.3847 - mse: 2146.3848 - mae: 32.2460 - val_loss: 2699.6113 - val_mse: 2699.6113 - val_mae: 35.3312\n",
      "Epoch 202/250\n",
      "9168/9168 [==============================] - 1s 76us/step - loss: 2152.1216 - mse: 2152.1221 - mae: 32.1931 - val_loss: 2708.4884 - val_mse: 2708.4888 - val_mae: 35.3558\n",
      "Epoch 203/250\n",
      "9168/9168 [==============================] - 1s 79us/step - loss: 2136.3239 - mse: 2136.3240 - mae: 32.1510 - val_loss: 2681.6293 - val_mse: 2681.6294 - val_mae: 35.7058\n",
      "Epoch 204/250\n",
      "9168/9168 [==============================] - 1s 79us/step - loss: 2144.4568 - mse: 2144.4575 - mae: 32.2297 - val_loss: 2712.2355 - val_mse: 2712.2356 - val_mae: 35.1529\n",
      "Epoch 205/250\n",
      "9168/9168 [==============================] - 1s 82us/step - loss: 2144.5289 - mse: 2144.5293 - mae: 32.2095 - val_loss: 2645.2272 - val_mse: 2645.2273 - val_mae: 35.2213\n",
      "Epoch 206/250\n",
      "9168/9168 [==============================] - 1s 77us/step - loss: 2129.2860 - mse: 2129.2849 - mae: 32.1081 - val_loss: 2760.6890 - val_mse: 2760.6890 - val_mae: 36.0295\n",
      "Epoch 207/250\n",
      "9168/9168 [==============================] - 1s 77us/step - loss: 2141.5231 - mse: 2141.5229 - mae: 32.1606 - val_loss: 2675.1089 - val_mse: 2675.1089 - val_mae: 34.9758\n",
      "Epoch 208/250\n",
      "9168/9168 [==============================] - 1s 75us/step - loss: 2128.3464 - mse: 2128.3464 - mae: 32.1124 - val_loss: 2742.4195 - val_mse: 2742.4197 - val_mae: 35.7064\n",
      "Epoch 209/250\n",
      "9168/9168 [==============================] - 1s 78us/step - loss: 2116.8065 - mse: 2116.8064 - mae: 31.9716 - val_loss: 2691.0852 - val_mse: 2691.0852 - val_mae: 35.5463\n",
      "Epoch 210/250\n",
      "9168/9168 [==============================] - 1s 79us/step - loss: 2119.8273 - mse: 2119.8269 - mae: 32.0222 - val_loss: 2732.1398 - val_mse: 2732.1394 - val_mae: 35.5481\n",
      "Epoch 211/250\n",
      "9168/9168 [==============================] - 1s 77us/step - loss: 2131.4077 - mse: 2131.4077 - mae: 32.0889 - val_loss: 2651.0129 - val_mse: 2651.0127 - val_mae: 35.1272\n",
      "Epoch 212/250\n",
      "9168/9168 [==============================] - 1s 70us/step - loss: 2114.6094 - mse: 2114.6099 - mae: 32.0065 - val_loss: 2669.0461 - val_mse: 2669.0459 - val_mae: 35.1985\n",
      "Epoch 213/250\n",
      "9168/9168 [==============================] - 1s 74us/step - loss: 2115.3016 - mse: 2115.3013 - mae: 32.0192 - val_loss: 2701.4842 - val_mse: 2701.4844 - val_mae: 35.0366\n",
      "Epoch 214/250\n",
      "9168/9168 [==============================] - 1s 76us/step - loss: 2123.3652 - mse: 2123.3643 - mae: 32.0493 - val_loss: 2700.5351 - val_mse: 2700.5352 - val_mae: 35.2599\n",
      "Epoch 215/250\n",
      "9168/9168 [==============================] - 1s 81us/step - loss: 2106.1519 - mse: 2106.1519 - mae: 31.9383 - val_loss: 2693.6979 - val_mse: 2693.6975 - val_mae: 35.2017\n",
      "Epoch 216/250\n",
      "9168/9168 [==============================] - 1s 80us/step - loss: 2099.5704 - mse: 2099.5708 - mae: 31.9040 - val_loss: 2657.2807 - val_mse: 2657.2815 - val_mae: 35.2339\n",
      "Epoch 217/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9168/9168 [==============================] - 1s 82us/step - loss: 2093.6962 - mse: 2093.6958 - mae: 31.9016 - val_loss: 2670.9476 - val_mse: 2670.9478 - val_mae: 35.0936\n",
      "Epoch 218/250\n",
      "9168/9168 [==============================] - 1s 78us/step - loss: 2093.4631 - mse: 2093.4639 - mae: 31.8458 - val_loss: 2687.7582 - val_mse: 2687.7581 - val_mae: 35.3756\n",
      "Epoch 219/250\n",
      "9168/9168 [==============================] - 1s 76us/step - loss: 2110.2495 - mse: 2110.2495 - mae: 32.0144 - val_loss: 2820.4816 - val_mse: 2820.4814 - val_mae: 35.3900\n",
      "Epoch 220/250\n",
      "9168/9168 [==============================] - 1s 80us/step - loss: 2094.8810 - mse: 2094.8804 - mae: 31.8554 - val_loss: 2632.8920 - val_mse: 2632.8921 - val_mae: 35.3026\n",
      "Epoch 221/250\n",
      "9168/9168 [==============================] - 1s 77us/step - loss: 2110.4819 - mse: 2110.4817 - mae: 31.9713 - val_loss: 2676.3929 - val_mse: 2676.3931 - val_mae: 35.7487\n",
      "Epoch 222/250\n",
      "9168/9168 [==============================] - 1s 78us/step - loss: 2090.8763 - mse: 2090.8762 - mae: 31.8609 - val_loss: 2676.2795 - val_mse: 2676.2798 - val_mae: 35.3663\n",
      "Epoch 223/250\n",
      "9168/9168 [==============================] - 1s 78us/step - loss: 2094.1075 - mse: 2094.1072 - mae: 31.8859 - val_loss: 2706.9953 - val_mse: 2706.9951 - val_mae: 35.1139\n",
      "Epoch 224/250\n",
      "9168/9168 [==============================] - 1s 78us/step - loss: 2089.2839 - mse: 2089.2844 - mae: 31.8369 - val_loss: 2708.1209 - val_mse: 2708.1208 - val_mae: 35.0673\n",
      "Epoch 225/250\n",
      "9168/9168 [==============================] - 1s 77us/step - loss: 2075.6108 - mse: 2075.6106 - mae: 31.7798 - val_loss: 2646.1680 - val_mse: 2646.1675 - val_mae: 35.2233\n",
      "Epoch 226/250\n",
      "9168/9168 [==============================] - 1s 76us/step - loss: 2083.8342 - mse: 2083.8340 - mae: 31.7702 - val_loss: 2654.3352 - val_mse: 2654.3352 - val_mae: 35.4068\n",
      "Epoch 227/250\n",
      "9168/9168 [==============================] - 1s 77us/step - loss: 2071.0273 - mse: 2071.0269 - mae: 31.7244 - val_loss: 2660.1855 - val_mse: 2660.1853 - val_mae: 35.3501\n",
      "Epoch 228/250\n",
      "9168/9168 [==============================] - 1s 78us/step - loss: 2070.0365 - mse: 2070.0364 - mae: 31.7084 - val_loss: 2637.2618 - val_mse: 2637.2620 - val_mae: 35.1969\n",
      "Epoch 229/250\n",
      "9168/9168 [==============================] - 1s 81us/step - loss: 2101.6689 - mse: 2101.6689 - mae: 31.9237 - val_loss: 2666.6739 - val_mse: 2666.6738 - val_mae: 35.2695\n",
      "Epoch 230/250\n",
      "9168/9168 [==============================] - 1s 79us/step - loss: 2062.2878 - mse: 2062.2874 - mae: 31.6564 - val_loss: 2661.8443 - val_mse: 2661.8440 - val_mae: 35.2251\n",
      "Epoch 231/250\n",
      "9168/9168 [==============================] - 1s 78us/step - loss: 2056.6119 - mse: 2056.6121 - mae: 31.6520 - val_loss: 2632.1267 - val_mse: 2632.1272 - val_mae: 34.9368\n",
      "Epoch 232/250\n",
      "9168/9168 [==============================] - 1s 60us/step - loss: 2060.6933 - mse: 2060.6931 - mae: 31.6681 - val_loss: 2665.7049 - val_mse: 2665.7051 - val_mae: 34.9219\n",
      "Epoch 233/250\n",
      "9168/9168 [==============================] - 1s 76us/step - loss: 2075.5074 - mse: 2075.5078 - mae: 31.7258 - val_loss: 2774.3984 - val_mse: 2774.3982 - val_mae: 35.8843\n",
      "Epoch 234/250\n",
      "9168/9168 [==============================] - 1s 66us/step - loss: 2061.6766 - mse: 2061.6760 - mae: 31.6516 - val_loss: 2688.1396 - val_mse: 2688.1399 - val_mae: 35.3576\n",
      "Epoch 235/250\n",
      "9168/9168 [==============================] - 1s 75us/step - loss: 2043.8672 - mse: 2043.8667 - mae: 31.5629 - val_loss: 2728.4920 - val_mse: 2728.4929 - val_mae: 35.0703\n",
      "Epoch 236/250\n",
      "9168/9168 [==============================] - 1s 66us/step - loss: 2063.7349 - mse: 2063.7336 - mae: 31.6645 - val_loss: 2689.2728 - val_mse: 2689.2725 - val_mae: 35.8648\n",
      "Epoch 237/250\n",
      "9168/9168 [==============================] - 1s 69us/step - loss: 2067.8806 - mse: 2067.8813 - mae: 31.6940 - val_loss: 2669.7088 - val_mse: 2669.7085 - val_mae: 35.0901\n",
      "Epoch 238/250\n",
      "9168/9168 [==============================] - 1s 68us/step - loss: 2044.2531 - mse: 2044.2535 - mae: 31.5556 - val_loss: 2642.3442 - val_mse: 2642.3440 - val_mae: 34.9671\n",
      "Epoch 239/250\n",
      "9168/9168 [==============================] - 1s 74us/step - loss: 2048.8554 - mse: 2048.8555 - mae: 31.5692 - val_loss: 2646.3546 - val_mse: 2646.3542 - val_mae: 35.5340\n",
      "Epoch 240/250\n",
      "9168/9168 [==============================] - 1s 76us/step - loss: 2033.5889 - mse: 2033.5890 - mae: 31.4701 - val_loss: 2708.1295 - val_mse: 2708.1296 - val_mae: 35.5764\n",
      "Epoch 241/250\n",
      "9168/9168 [==============================] - 1s 73us/step - loss: 2043.7756 - mse: 2043.7753 - mae: 31.5630 - val_loss: 2659.7247 - val_mse: 2659.7246 - val_mae: 34.7417\n",
      "Epoch 242/250\n",
      "9168/9168 [==============================] - 1s 72us/step - loss: 2038.4603 - mse: 2038.4598 - mae: 31.5110 - val_loss: 2630.4361 - val_mse: 2630.4360 - val_mae: 34.8847\n",
      "Epoch 243/250\n",
      "9168/9168 [==============================] - 0s 49us/step - loss: 2031.1621 - mse: 2031.1627 - mae: 31.4786 - val_loss: 2647.7940 - val_mse: 2647.7947 - val_mae: 35.0932\n",
      "Epoch 244/250\n",
      "9168/9168 [==============================] - 0s 44us/step - loss: 2040.9213 - mse: 2040.9215 - mae: 31.5102 - val_loss: 2672.8619 - val_mse: 2672.8621 - val_mae: 34.8429\n",
      "Epoch 245/250\n",
      "9168/9168 [==============================] - 0s 48us/step - loss: 2037.2823 - mse: 2037.2821 - mae: 31.4969 - val_loss: 2757.3870 - val_mse: 2757.3872 - val_mae: 35.8138\n",
      "Epoch 246/250\n",
      "9168/9168 [==============================] - 1s 81us/step - loss: 2019.2692 - mse: 2019.2694 - mae: 31.3814 - val_loss: 2688.7032 - val_mse: 2688.7031 - val_mae: 35.7211\n",
      "Epoch 247/250\n",
      "9168/9168 [==============================] - 1s 73us/step - loss: 2028.0348 - mse: 2028.0343 - mae: 31.4395 - val_loss: 2634.8056 - val_mse: 2634.8054 - val_mae: 34.9033\n",
      "Epoch 248/250\n",
      "9168/9168 [==============================] - 1s 81us/step - loss: 2041.4199 - mse: 2041.4207 - mae: 31.5454 - val_loss: 2648.8922 - val_mse: 2648.8921 - val_mae: 35.0304\n",
      "Epoch 249/250\n",
      "9168/9168 [==============================] - 1s 81us/step - loss: 2013.7286 - mse: 2013.7281 - mae: 31.3359 - val_loss: 2634.4129 - val_mse: 2634.4131 - val_mae: 34.9442\n",
      "Epoch 250/250\n",
      "9168/9168 [==============================] - 1s 81us/step - loss: 2014.5597 - mse: 2014.5593 - mae: 31.3355 - val_loss: 2671.3361 - val_mse: 2671.3367 - val_mae: 35.3250\n",
      "51.594262604777384\n",
      "46.08715507964481\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "for j in range(24):\n",
    "    features.append('PM2.5_t-'+str(j))\n",
    "predVector = []\n",
    "for j in range(24):\n",
    "    predVector.append('PM2.5_t+'+str(j))\n",
    "X = df[features]\n",
    "y = df['PM2.5_t+23']\n",
    "scaler = StandardScaler()\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "scaler.fit(Xtrain)\n",
    "with tf.device('/CPU:0'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(50, input_dim=34, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(24, activation='linear'))\n",
    "    model.summary()\n",
    "    #Fit\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['mse','mae'])\n",
    "    history = model.fit(scaler.transform(Xtrain), ytrain, epochs=250, batch_size=50,  verbose=1, validation_split=0.2)\n",
    "    #Print Accuracy\n",
    "    testPred = model.predict(scaler.transform(Xtest))\n",
    "    trainPred = model.predict(scaler.transform(Xtrain))\n",
    "    print(mean_squared_error(testPred, ytest,squared=False))\n",
    "    print(mean_squared_error(trainPred, ytrain,squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'val_mse', 'val_mae', 'loss', 'mse', 'mae'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
